[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Why Statistics?",
    "section": "",
    "text": "Hello (An Introduction)!",
    "crumbs": [
      "List of R Code",
      "Hello (An Introduction)!"
    ]
  },
  {
    "objectID": "index.html#who-is-this-book-for",
    "href": "index.html#who-is-this-book-for",
    "title": "Why Statistics?",
    "section": "Who Is This Book For?",
    "text": "Who Is This Book For?\nThis book - Why Statistics? - is being written to support students‚Äô learning of the statistics, R programming skills, and research methods that are required of modern psychological scientists. Whew!\nI‚Äôve learned over the years that students approach statistics with a variety of intense emotions - many of them negative. For what it‚Äôs worth, I have been happy to see most students thrive in the class, and will do my best to support y‚Äôall and make this class (and book) a positive learning experienceTM.\nTo that end, please fill out this form (or talk to me in class) if you are confused or overwhelmed; this class work as well as it does because of past and present generations of students like you, and I hope that it will continue to improve thanks to present and future generations of students. If you want to use this text in your own class, reach out and I can send my lesson plans and assignments if they might be helpful.",
    "crumbs": [
      "List of R Code",
      "Hello (An Introduction)!"
    ]
  },
  {
    "objectID": "index.html#whats-in-this-book",
    "href": "index.html#whats-in-this-book",
    "title": "Why Statistics?",
    "section": "What‚Äôs In This Book?",
    "text": "What‚Äôs In This Book?\nThis book is organized into three sections. You can view these sections (and the chapters inside) by looking at the Table of Contents to the left.\n\nDescribing People : This first section focuses on how psychologists, we‚Äôll discuss both how and why psychologists seek to learn what people (or non-human animals) are like (Chapter 1), the different types of data that psychologists collect on people (Chapter 2), the fancy terms that psychologists use to describe how people differ (Chapter 3), the methods they use to try and turn complex psychological states like happiness, statistics anxiety, or love into numbers (Chapter 4), and the ways that they try to give context to these numbers (Chapter 5). We will also learn about some of the research methods required to develop your own research ideas, and fit these ideas in the past research that has been done (or not done!) on the topic.\nPredicting People : This section focuses on the ways that psychologists try and use statistics to make predictions (educated guesses) about what people are like. We‚Äôll discuss how psychologists use the information they learn about people to make predictions about a person based on some other information (Chapter 6), how they make these predictions based on the categorical group the person belongs to (Chapter 7), how they try to make a guess about all people even though they are just studying a few (Chapter 8), and how they try to (Chapter 9). We will also learn about some of the research methods required to critically evaluate some of the important assumptions about research, to better understand how much faith to place in the results of a psychological study.\nPeople Are Complex : The final section goes deeper into some of the more advanced statistics and methods that psychologists use to understand people in their full complexity. Specifically, we will discuss how psychologists use multiple bits of information to update their predictions about people (Chapter 10), how our predictions can change depending on other features of the person or situation (Chapter 11), and how researchers adapt their statistics to account for different types of data (Chapter 12). Finally, we‚Äôll conclude with some rambling thoughts about the whole endeavor of statistics and research in psychology, and reflect on the friends we made along the way on this journey (Chapter 13).\n\nThere are three parts to every chapter; when you click on a chapter, you‚Äôll see another table of contents appear that organizes these parts.\n\nStatistics : You‚Äôll learn how and why psychologists analyze data (i.e., use math) to learn about people (or non-human animals).\nR Programming : You‚Äôll learn how to use the programming language R to work with these data. We will go over examples, and there will be some helpful videos to watch.\nResearch Methods : You‚Äôll learn more about the decisions psychologists make when collect, analyzing, interpreting, and reporting data on people, and how these decisions can impact our understanding.",
    "crumbs": [
      "List of R Code",
      "Hello (An Introduction)!"
    ]
  },
  {
    "objectID": "index.html#goals-of-the-book",
    "href": "index.html#goals-of-the-book",
    "title": "Why Statistics?",
    "section": "Goals of the Book",
    "text": "Goals of the Book\nI hope that by the end of this book, you‚Äôll feel confident in your ability to do the kinds of authentic tasks that a psychological researcher might do :\n\nAnalyze Data. I‚Äôll provide you a dataset and / or statistical output, and you‚Äôll use your knowledge of statistics, R, and research methods to answer questions about the dataset. What do we learn from the data? Why should we care?\nConduct an Independent Study. You will identify a research question that you care about, and then design a study, collect and analyze the data, and write up a report to share with us what you learned about the question you had (and what other questions remain).\n\nWe will talk much more about these assignments throughout the semester, and you will be well prepared for them if you follow along with the readings, quizzes, and homework assignments each week.",
    "crumbs": [
      "List of R Code",
      "Hello (An Introduction)!"
    ]
  },
  {
    "objectID": "index.html#how-will-i-use-this-book-for-our-class",
    "href": "index.html#how-will-i-use-this-book-for-our-class",
    "title": "Why Statistics?",
    "section": "How Will I Use This Book for Our Class?",
    "text": "How Will I Use This Book for Our Class?\nThis semester, we‚Äôll be taking a flipped classroom approach to our learning.\n\nBefore lecture you‚Äôll read and watch some videos to be introduced to the content that we will then cover more deeply in lecture. You‚Äôll also take a short quiz (no time limit, open-note, and you can take as many times as you‚Äôd like) that will encourage you to do the readings, and let me know what topics are still confusing to students.\nDuring lecture we will start with a review of concepts you learned from the pre-readings, go over any common questions that students still have, and then use the remaining time to practice and discussing the skills and concepts. We will work on the homework assignment together\nAfter lecture you‚Äôll complete any homework that we didn‚Äôt finish in class, and then read for the next week‚Äôs lecture.\n\nThe flipped classroom approach requires y‚Äôall to do the readings and watch the videos before class, and requires me to write text and record videos that are engaging and helpful for the students in the class.\nPlease take a look at the syllabus (on our course page) for more information about assignments, grading, and other course policies. However, the TLDR is that this course is designed for YOU to learn, so let me know if something is not working (but know that you will also need to do the work and can expect some level of struggle, since that‚Äôs an important part of the learning process).",
    "crumbs": [
      "List of R Code",
      "Hello (An Introduction)!"
    ]
  },
  {
    "objectID": "index.html#who-is-writing-these-words-that-i-am-reading",
    "href": "index.html#who-is-writing-these-words-that-i-am-reading",
    "title": "Why Statistics?",
    "section": "Who Is Writing These Words That I Am Reading?",
    "text": "Who Is Writing These Words That I Am Reading?\nMy name is Arman Daniel Catterson, and I‚Äôm very very very lucky to be a professor in the Bay Area at Diablo Valley College (tenured) and at UC Berkeley (continuing lecturer), who has been teaching some version of this class since Summer 2015. Feel free to say ‚Äúhi‚Äù if you see me on campus :) or hit that ‚ÄúSUBSCRIBE‚Äù button on YouTube. Thank you for reading.",
    "crumbs": [
      "List of R Code",
      "Hello (An Introduction)!"
    ]
  },
  {
    "objectID": "chapters/1R_WhyStats.html",
    "href": "chapters/1R_WhyStats.html",
    "title": "Why Statistics?",
    "section": "",
    "text": "Part 1 : Why Statistics in Psychology?\nIn this week‚Äôs reading, you‚Äôll learn why students like you are required to learn the statistics, programming language, and research methods required of modern psychology.\nAs y‚Äôall know, this class is a requirement for students who want to be psychology majors. This is exciting for me (your professor), and probably some students too. However, over the years I have learned this can be frustrating and stressful for students who wonder why-the-flip they are required to take a math class when all they just want to learn about people (or other non-human animals), ya know!??!\nI agree that people (or non-human animals) are interesting. And we all have this interest in people (or non-human animals) because we are complex1. While people are similar in many ways, we also differ in radical ways; from superficial features like age and race, to more complex ways like our personality or emotions, to highly specific behaviors such as whether all students in the class are reading these words or not, or how bored or excited (or any emotional experience) students are while reading these words.\nI hope that you have an interest in people (or non-human animals), and that this class helps you learn how to think about those interests through a psychological, and statistical lens. You don‚Äôt always need this lens - and as we will discuss there can sometimes be dangers in viewing people through this lens - but it‚Äôs useful to be able to wear these ‚Äústats glasses‚Äù when needed.",
    "crumbs": [
      "List of R Code",
      "Describing People",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Why Statistics?</span>"
    ]
  },
  {
    "objectID": "chapters/1R_WhyStats.html#part-1-why-statistics-in-psychology",
    "href": "chapters/1R_WhyStats.html#part-1-why-statistics-in-psychology",
    "title": "Why Statistics?",
    "section": "",
    "text": "1¬†One of the reasons I love teaching in the Bay Area is y‚Äôall are hella complex.\n\nStatistics as a Language\nStatistics is a language that scientists use to describe this complexity. Indeed, historical records of the earliest written language reveal that people have used some form of symbols and numbers to represent - the Sumerian (modern day Iraq) stone tablet at the beginning of this chapter was used to represent where locusts and caterpillars had infested different plots of land.\nPsychology uses this language to better understand differences in people (or non-human animals), and other scientific disciplines focus on their own domains; physicists seek to understand differences (and similarities) in matter and energy, chemists seek to understand differences (and similarities) in elements and compounds, botanists seek to understand differences in plants, and economists seek to understand money.\nLike all languages, statistics has vocabulary - words that have shared meaning, and allow us to understand what we are talking about.\n\nVariables and Variation\nVariation is at the heart of statistics, and is the specific vocabulary that researchers use to describe the differences, change, and complexity that defines life.\n\n\n\n\n\n\n\nBetween-Person Variation describes how individuals differ from each other. Think about ways that you differ from others; not everybody wears glasses, has the same level of silliness / seriousness / desire to cause mischief / fascination with horses.\n\n\n\nWithin-Person Variation describes how one individual changes over time or across different situations. Think about the person you are today - are you exactly the same as you were yesterday? A year ago? You‚Äôve changed (varied) in ways both small (hunger, exhaustion, number of words you‚Äôve read for this class) and large (personality, love interests, identity, etc.)\n\n\n\nNo variation would describe a situation in which everyone is exactly the same. I can‚Äôt think of too many situations where there is no variation; let me know on Discord if you can think of one? And while there‚Äôs no theoretical limit to the amount of variation that there can be, one major task of this class will be to learn to quantify the amount of variation that we observe in our role as psychologists.\n\nüåûüåûüåû\nüåûüåûüåû\n\n\n\n\nA Variable is a label for some psychological phenomenon that has variation. I‚Äôm not sure where I first heard this, but psychologists often focus on what they call the ‚ÄúABCs‚Äù.¬†\n\nA is for Affect = the emotions that you feel.\nB is for Behavior = the actions that you do.\nC is for cognition = the thoughts you have.\n\nThese are not rigid categories, and psychologists often debate the definitions of these terms. But they can be useful ways to think about how to think about people, and help break down a complex phenomenon into more specific components.\n\n\nPractice : Variables and Variation\nFor example, think about how affect, behavior, and cognition might be relevant if I ask you to think about an upcoming exam (your first exam is in just a few weeks!)\n\n\n\n\n\n\n\n\nhighlight the cells below to see my ideas / check your understanding\n\n\nAffect Example\nthe feeling of anxiety or dread you have thinking about being assessed, or maybe a feeling of excitement about the opportunity to demonstrate your hard work / effort / knowledge!\n\n\nBehavior Example\nimmediately checking your calendar to see when exams are; or maybe avoiding your classes with a nice procrastination session on the ol‚Äô infinite scroll machine.\n\n\nCognition Example\nthinking about all the work you have to do; wondering why the professor would choose this example when he could have thought about the ways that affect, behavior, and cognition would be triggered when you see a puppy or kitten or something like that‚Ä¶\n\n\n\nHere‚Äôs a cat video I like for students needing a distraction from thinking about exams. I promise your exams in this class will be chill and I‚Äôll do my best to prepare y‚Äôall.\n\n\nWhat‚Äôs Language?\nStatisitcs, like spoken languages, is a combination of vocabulary, rules, and culture that people use to communicate with each other.\n\nVocabulary. Equations like ‚Äúthe mean‚Äù or ‚Äústandard deviation‚Äù or ‚Äúcronbach‚Äôs alpha‚Äù or ‚Äúp-value‚Äù are precise vocabulary terms that define some feature of variation. Some of these vocabulary words are easier to understand and remember than others, and like all languages, sometimes people disagree on the definition, and sometimes misuse these words.\nGrammar and Syntax. The way we organize words also matters when learning languages. Saying ‚Äúthe professor graded the students‚Äú has a very different meaning than ‚Äúthe students graded the professor‚Äù, even though these share the exact same words. Statistics (and research methods) also requires precision in the way we organize the ideas, terms, and processes. We‚Äôll learn more about this as we discuss the scientific method (a highly structured and organized approach to doing research), but also as we learn how to navigate doing data analysis.\nCultural Immersion. A good language class will also help students to understand the ways that the language is connected to people, places, and history2. In this class, we‚Äôll think about the ways we can immerse ourselves in the culture of statistics and research methods, from the cultural practices that inform which methods or tools to use, to the ways that the culture of statistics and research might needlessly create barriers for certain types of people or studies.\nPractice and Past Experiences. And yes, in order to gain fluency in a language, you need to practice! Attendance and regular engagement with this class will ensure that you are able to get the practice that you need. It‚Äôs also good to note that people differ in terms of their past experiences with computers and math, and are bringing those experiences (for better or worse) with them into this class.\n\n2¬†And usually food, though I‚Äôm not sure if there‚Äôs cultural food norms about statistics or research methods.\n\nCulture in Statistics : Me-Search\n\n\nHiya folks! Everyone‚Äôs favorite Open-Source Mickey Mouse here. I‚Äôll be popping up in the book from time to time to critically engage with the idea that statistics has a culture that is socially constructed by people like you!\n\nAll psychologists are interested in questions sparked by their own observations or experiences. This is sometimes called ‚Äúme-search‚Äù - the idea that a person‚Äôs research interests reflect their own experience.\nMe-search can be an important way for researchers to use their statistics and research methods training to address questions and issues that are relevant to their lives and communities.\nBelow are a few examples of research questions that are related to a person‚Äôs real-life experiences.\n\n\n\nReal Life Experience\nResearch Question\n\n\nA researcher develops vision problems due to his studies of light, and had to live in a completely darkened room, where he became completely isolated from most of reality.3\nDoes reality exist, or can it only be known through our perceptions?\n\n\nA black graduate student at the University of Chicago realizes that white people cross the street to avoid him, and finds himself whistling classical music to signal that he does not fit their negative stereotypes of a black male.4\nHow do people respond and react to others‚Äô negative stereotypes?\n\n\n\n3¬†This happened to Gustav Fechner, one of the early psychologists who pioneered the study of psychophysics, which tested theories that our perceptions do not always match reality (seen in many examples, such as the dress). FWIW I see it as blue and black.4¬†This anecdote (as reported here) inspired Claude Steele‚Äôs research on stereotype threat.Me-search also serves as a form of potential bias in research - not only will a researcher‚Äôs own biases and beliefs influence the way they conduct research, but the types of questions that are asked will be influenced by the types of people doing research.\nFor example, a survey of over 26,000 research articles in psychology documented just how rarely the topic of race is studied.\n\n\n\n\n\nAs we‚Äôll discuss throughout this class, life is complex and there is never one explanation for any phenomenon. However, it‚Äôs important to note that psychology as a field has historically been dominated by white authors (researchers who write scientific papers) and white editors (researchers who decide what papers get published or not).\n\n\n\n\n\nThese trends are important to reflect on, because they reveal a bias in who becomes a psychologist, and what types of questions these researchers are interested in pursuing.\nIt‚Äôs also a goal of this class to not only highlight the important contributions of non-white researchers and statisticians, but also make sure that all students in this diverse classroom feels empowered to use statistics, research methods, and R skills to ask (and answer) research questions that matter to them!5\n5¬†This is the purpose of your final project! We‚Äôll talk more about this throughout the class.\n\n\nPsychology as a REAL SCIENCE ‚Ñ¢\nPsychology uses statistics because, in part, it wants to establish itself as a real science, like physics and chemistry. You don‚Äôt have to take my word for it, just look at the definitions of some common sources of psychological knowledge - introductory textbooks:\n\n‚ÄúToday, we define psychology as the science of behavior and mental processes.‚Äù (Myers, 2011)\n‚Äú‚Ä¶the science of behavior and the mind.‚Äù (Grey, 2010)\n‚Äú‚Ä¶the scientific study of mind, brain, and behavior.‚Äù (Gazzaniga, 2010)\n‚Äú‚Ä¶We now define psychology as the science of behavior and mental processes.‚Äù (Myers & DeWall, 2018)\n\nI‚Äôm probably showing my age looking to textbooks, so let‚Äôs check in with ChatGPT6 to see whether the algorithmic summary of large piles of data suggests that psychology is, in fact, a ‚Äúreal science‚Äù :¬†\n6¬†See the syllabus for the course ChatGPT policy. You may also be interested to read on the ethical and environmental issues surrounding this emerging technology.\nPerhaps more authoritatively, the American Psychological Association (APA) confirms that psychology is, ‚Äúthe study of the mind and behavior‚Ä¶a diverse scientific discipline comprising several major branches of research.‚Äù (APA, 2024).\nThe consistent emphasis on science (and ‚Äúrigorous‚Äù methods!) in these definitions is an attempt to elevate psychology through language to the status of other ‚Äúhard‚Äù sciences, like physics and chemistry. But inclusion of the term ‚Äúscience‚Äù also seeks to differentiate psychology from its less scientific heritage and past, defend itself from accusations from other scientists / talking heads (if not some of your friends in STEM majors‚Ä¶or parents) that it is not actually REAL SCIENCE ‚Ñ¢.¬†\n\nWhat is Science? Prediction (and Error)\nOne of the most important goals of science is to form predictions, and then use these predictions in order to influence outcomes (a form of power).\nA prediction is an educated guess you have about the future. Educated means that the guess comes from some knowledge (either your experiences, beliefs, something you learned in a textbook, or the results of a scientific study). A prediction can have two outcomes :\n\nValid : your prediction is right\nError : your prediction is wrong.\n\nOf course, things are rarely as simple as ‚Äúright‚Äù or ‚Äúwrong‚Äù, and a large part of this class will be learning how to quantify exactly how much error there is in our scientific predictions.\nAs an example, let‚Äôs look to the stars. Astronomers have developed knowledge about celestial bodies - gravity, orbits, mass (I know very little about this). But I trust this science because astronomers are able to use it to make very valid predictions about giant space rocks and when they will come close to the earth. I can use this knowledge to plan a stargazing trip, or plan to watch all the rich people leave earth when the ‚Äúbig one‚Äù comes for the rest of us.\n\n\n\n\n\nYou can see when scientists predict Halley‚Äôs comet to come closest to earth here. While this is fairly accurate, I‚Äôve seen the exact prediction change over the years - there is still some error in this prediction.\n\n\nAs another example, on the day I‚Äôm writing these words, I do not predict that it will rain outside. I am making this prediction based on the following information:\n\nit did not rain yesterday\nit is not currently raining\nI live in Oakland, where it rarely rains in August.\nthe air doesn‚Äôt have that feeling of rain; that smell.\nI looked at the weather forecast app and it said we had a week of sunny weather ahead.\nNobody was carrying an umbrella today.\nthere are no clouds.\n\nBecause I predict it will not rain, I‚Äôm not too worried that the tarp is not covering my bike locked outside. Of course, my prediction could be wrong.\n\n\n It sometimes rains when sunny, as on Puffin Rock, where we‚Äôll be here come rain or shine.\nThink about some predictions that you have made. What knowledge informed the prediction? Did you use this prediction to influence your future behavior in some way? What kinds of predictions do psychologists make? We will chat more about these ideas in class :) but it‚Äôs a core focus on why psychologists use predictions.\nAnd so, here we are. In this document, in this required class for the psychology major, learning how to DO REAL SCIENCE. With statistics.\n\n\nLinear Models to Organize Our Predictions\nThe linear model a simple formula that helps researchers to make, and quantify, their predictions. We will talk much more about linear models in our class - they are one of the most important concepts we will cover and a foundation of modern statistics - but for now let‚Äôs just focus on the basics :\nA linear model takes the following form: 7\n7¬†I can already feel some of y‚Äôalls math anxiety rising across time and space. But remember, stats is just a language with some vocabulary we need to learn.\\[\nDV \\sim IV_1 + IV_2 + ... + IV_k + \\epsilon\n\\]\nBelow is a description of terms in the model :\n\nDV = dependent variable. This is the variable that you want to predict. It‚Äôs up to the researcher what variable is the dependent variable. More complicated models can have more than one DV, but for this class we will just focus on one DV at a time.\nIV = independent variable. This is a different variable that you think will help you make predictions. Again, it‚Äôs up to the researcher to choose what variables they will include to try and make predictions of the DV.\nk = any number. This is a way of saying that there can be many IVs. Life is complex, and so in order to predict one variable, we will need to use information from lots of other variables.\n~ = a squiggly line / tilde. I like to use the squiggly line to reinforce the idea that our model is uncertain and squiggly (this is not an exact prediction where an equal sign would be used). R also uses the tilde for defining a model, so it‚Äôs nice to start practicing stretching our pinkie finger to reach that upper left corner of the keyboard.\n\\(\\epsilon\\) = error = this term always concludes our written model, and accounts for the many other reasons why our predictions might be wrong. This could be because we haven‚Äôt included certain variables that are important to make predictions (either because we can‚Äôt measure them in our study, don‚Äôt think they are important enough to study, or don‚Äôt know that we should study them), or because we are measuring our variables with some amount of error8.\n\n8¬†We will learn more about measurement error in Chapter 4.For example, I could write out my prediction about rain as the following linear model :\n\nrain ~ clouds + umbrellas + weather app status + smell + air pressure + season + location + temperature + error\n\nWe read a model as : ‚Äúthe DV is a function of‚Ä¶‚Äù or sometimes ‚Äúthe DV depends on‚Ä¶‚Äù. So I‚Äôd read this model as ‚Äúrain is a function of clouds, umbrellas, the weather app status‚Ä¶etc.‚Äù And it‚Äôs good practice to leave the variables neutral (e.g., just ‚Äúclouds‚Äù, not ‚Äústorm clouds‚Äù).9\n9¬†Later we will talk about ways to account for the fact that more umbrellas increases the chance it will rain in our model.Error in this model would represent the fact that I haven‚Äôt accounted for some important variables that we know we should include (like humidity, coastal pressure systems, or palnetary waves), the fact that some of my variables were probably measured with error (was no one really carrying an umbrella???), and the fact that there are some things that we don‚Äôt know about rain, and what predicts it (this is what weather scientists are trying to learn more about.)\nOnce we add numbers and data to a linear model10, we can quickly see :\n10¬†We will get to this in Chapter 6!\nwhich variables allow us to make the best predictions, and which variables do not improve our predictions (e.g., is the number of umbrellas others are carrying a good predictor of whether it‚Äôs going to rain or not?)\nthe direction of the relationship between each IV and the DV (e.g., is it that more umbrellas = more rain, or more umbrellas = less rain?)\nthe amount of error in our predictions (e.g., how good, exactly, is this model at predicting the rain?)\n\n\nPractice Writing A Model\nImagine a researcher wants to better understand why people differ in happiness, and believes that playing music, drinking more water, and eating ice cream sandwiches are important factors to consider. How would you write this out as a linear model?\n\n\n\n\n\n\nCautionClick Here to See The Answer\n\n\n\n\n\nhappiness ~ music playing + water intake + ice cream consumption + error\nFor this model, I‚Äôve organized the focus of the research question on the left-hand side and listed the three other variables to the right, using neutral descriptions (e.g., ‚Äòwater intake‚Äô rather than ‚Äòdrinking more water‚Äô). I also included error at the end of my model to account for the fact that some people who play music, drink water, and eat ice cream are, in fact, not happy. I also didn‚Äôt add other varibles that might be related to happiness, since these were not part of the researcher‚Äôs statement.",
    "crumbs": [
      "List of R Code",
      "Describing People",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Why Statistics?</span>"
    ]
  },
  {
    "objectID": "chapters/1R_WhyStats.html#part-2-why-r",
    "href": "chapters/1R_WhyStats.html#part-2-why-r",
    "title": "Why Statistics?",
    "section": "Part 2 : Why R?",
    "text": "Part 2 : Why R?\nThis semester, we will also learn how to use the computer programming language R to work with data, conduct analyses, and make graphs. R can be intimidating to work with at first, and is more confusing than it needs to be sometimes (as you‚Äôll quickly find out‚Ä¶), but I promise you will learn!12 It‚Äôs totally okay (and expected) for you to feel frustrated at times; this is part of the learning process. So please embrace the ‚ÄúI HAVE NO IDEA WHAT I‚ÄôM DOING‚Äù dog meme energy (and look how happy the doggy looks!) as you embark on your R journey.\n\n12¬†In fact, that‚Äôs the point of this class.\n\n\n\n\n\n\n\n\nInstalling R\nUse this link to Download and Install the Programs R and RStudio Desktop\nNote : You must download both R and RStudio Desktop (these are two separate programs). Make sure to download the most recent version of R and RStudio to avoid issues in the future.\n\nR is the powerful, free, and somewhat intimidating computer program that we will use to analyze data in this class. This website is not super friendly - choose the operating system you have (Windows, MacOS, or Linux) and then download the ‚Äúlatest release‚Äù on the next page. If you have a chromebook or iPad / tablet, you will need to use posit.cloud.\nRStudio is an Integrated Development Environment (IDE) - basically a ‚Äúhome‚Äù for R to live in, with rooms and this program is not 100% necessary, but makes it a little easier to navigate R. Note that you will need to install R first in order for RStudio to work.\n\nHaving trouble getting these programs to work?\n\nHere‚Äôs one YouTube video someone made to show you how to download and install.\nTry posit.cloud. This is a web-based version of RStudio, and has a free option but limits your hours of work each month. There‚Äôs a paid option for $5/month that you can use if you sign up with your student e-mail address; former students also pointed out that you can always create a new ‚Äúfree‚Äù account if you run over the 15-hour limit.\nAsk for help! The professor, other students, or a tutor / your TA can help get everything working properly.\n\n\n\nNavigating R\nWatch the two videos below for a quick introduction to R - the program we will be using to analyze data.\n\nVIDEO : Navigating R\n\n\nwhat R looks like when you open it\nbasic math in the¬†console\nindexing and output\n\n\n\nVIDEO : Navigating RStudio\n\n\nwhat RStudio/Posit looks like; navigating the program\nbasic math in the console\nthe¬†source file¬†(makes life easier and saves your work!)",
    "crumbs": [
      "List of R Code",
      "Describing People",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Why Statistics?</span>"
    ]
  },
  {
    "objectID": "chapters/1R_WhyStats.html#part-3-how-science-the-scientific-method-in-five-easy-steps",
    "href": "chapters/1R_WhyStats.html#part-3-how-science-the-scientific-method-in-five-easy-steps",
    "title": "Why Statistics?",
    "section": "Part 3 : How Science? The Scientific Method in Five Easy Steps",
    "text": "Part 3 : How Science? The Scientific Method in Five Easy Steps\nThe Scientific Method is used to help science progress toward valid (accurate, ‚Äútrue‚Äù) predictions and avoid biases. This is the same scientific method you may have learned about in a previous science class or used for a science fair project11.\n11¬†¬†¬†Do elementary school students still do those science fair projects with the tri-fold posters? Adult scientists do the same kinds of science fairs, except they are called ‚Äúconferences‚Äù, involve a little more math, and use flat posters. I also don‚Äôt think anyone gets ribbons (status among scientists takes other less concrete forms). Some of the largest conferences include that held by the American Psychological Association or the Association of Psychological Science. The Western Psychological Association has a more local conference, and there are literally hundreds of other conferences based on research topic (for example, the Society of Personality and Social Psychology). If you are interested in learning more about conferences, ask your GSI what conferences they attend. And you even might be able to present your project in this class as a conference poster - no tri-fold needed.I like to organize the scientific method into five broad parts, described below.\n\nStep 1 : Identify a Question\nFirst, a researcher starts with a question about the variable that they want to predict or the psychological phenomenon they are interested in. In Part 1 of this chapter, we talked about how these questions are often informed by the researcher‚Äôs own observations or experiences. In this section, we‚Äôll discuss ways you can choose a topic to inspire a specific research question - something you will do as part of your final project in this class.\nWe‚Äôll talk about this more in class, but the basic idea is that you will identify a topic that you care about, develop this idea into a research question, design a study to answer some part of this question, collect and analyze the data from your study (using R!), and then write up the results as an academic paper.\nThe first step, is picking some topic or variable that you care about, and identifying the parts of your background or experiences that influence this topic. This can be a challenging first step, and it‚Äôs okay to struggle. Your topic will also change as we develop it, so don‚Äôt feel like it needs to be ‚Äúperfect‚Äù or ‚Äúlocked in‚Äù yet.\nLooking for project topic inspiration? A few ideas to consider :\n\nThink about why you become a psychology major. Was it because you were interested in people, find dreams fascinating, wonder if you could get good at detecting people‚Äôs lies? Think about these interests, then think about what variables might be relevant. For example, while you won‚Äôt have access to an fMRI machine (or the required statistics) in order to reconstruct images people see during their dreams, you might study people‚Äôs beliefs about the importance of their dreams, or the extent to which they have dreams.\nFocus on a problem you‚Äôve encountered or observed in real-life. Notice that something in the world could be better? How would you label that as a variable? For example, maybe you want to better understand people‚Äôs attitudes about capitalism or racism or housing costs.\nLook at faculty webpages, and see what they (and / or their graduate students) are studying. Does anything seem interesting to you? Who looks like they might be cool to work with? What aspects of their research question might you study with this project?\nChase the latest trends of today. Identify the new hot trend in our capitalist society these days, and then focus your project on that. For example, you could examine how people‚Äôs perceptions of HOTNEWTREND are influenced by some aspect of their identity, or how exposure to HOTNEWTREND influences people‚Äôs affect, behavior, or cognition (compared to another HOTTREND, or maybe an OLDTREND).\nYou don‚Äôt have to take my word for it. Here‚Äôs another guide that might offer some help, or ask for help in the class discord / office hours / lecture!\n\nOnce you‚Äôve chosen your topic, make sure that it‚Äôs the right level of focus, and something you can complete with the resources you have.\n\n\n\n\n\n\n\n\n\nInitial Topic\nProblem\nWays to Revise Without Sacrificing Your Vision\nExample Revised Version\n\n\n\n\nWhy do people do the things that they do?\nHow does social media influence mental health?\nGreat start, but these topics are too broad! Which things? Which people? What aspects of mental health?\nTry to focus by identifying a specific aspect of people‚Äôs affect, behavior, or cognition that you care about, or a specific group of people to study.\nWhy do people overshare personal details in social settings?\nHow does social media usage influence social anxiety in teenagers?\n\n\nDo sociopaths dream about kittens more than non-sociopaths?\nHow does inter-generational trauma change our neural wiring?\nInteresting topics! However, it would be really difficult to study with the resources that a student in this class would have. (e.g., it would be hard to find sociopaths to give surveys to; hard to rent out an fMRI to measure voxel activation.)\nThink about ways to adapt the question to account for people that you could study, and variables that you could more easily measure.\nHow do dreams influence people‚Äôs future behavior?\nWhat are some strategies that help people heal from inter-generational trauma?\n\n\n\n\n\n\n\n\n\nTipProf.¬†Cat Has An Idea\n\n\n\nHi folks, Prof.¬†Cat here. I‚Äôll be popping up throughout this book to guide you through the process of working on your final project. To do this, I‚Äôll work on an example along the way.\nChoosing a Topic and Reflecting on the Ways Your Background Influences (and maybe biases) You : I‚Äôm interested in the (negative) consequences of looking at images of war-torn cities on people‚Äôs attitudes about the inhabitants. Specifically, I‚Äôm wondering if seeing these images of destroyed cities - and the screaming or injured citizens who live in them - leads people to (unintentionally) dehumanize these people by seeing them only as powerless victims, and not inhabitants with culture, family structures, etc. This interest is inspired by a few different things - I‚Äôve seen a lot of horrifying images from Gaza in the last few years, but less emphasis or coverage about how these people are (and were) living before being bombed. I also have always been really critical of war and the US war machine as a kid (due to the influence of my parents, some books, Miyazaki films, a few teachers, and a minister). I‚Äôm also half-Iranian, so am biased to pay more attention to this part of the world (and the long history of conflict with the US), and probably am biased to see victims from this region in more humanizing ways because of the many cultural similarities (in language, music, dress, religion, love of tea, etc.)\n\n\n\n\n\n\n\nhi, says prof. cat\n\n\n\n\nStep 2 : Develop a Theory\nA scientific theory is one that is comprehensive, explanatory, and supported by evidence. For example, the theory of evolution is comprehensive (it relates to all nature, not just plants or animals or finches), it is explanatory (it‚Äôs why we and cats both narrow our eyes when we are scared and angry), and it is supported (by over 100 years of evidence12).¬†\n12¬†From fossil records to observations of fruit flies and plansMost scientists refrain from saying that a theory is ‚Äúproven‚Äù or ‚Äútrue‚Äù for two reasons:\n\nFirst, the scientific method is a process - our knowledge and ideas are continually updated. So it‚Äôs likely that the theory of evolution will be updated as we learn more about the complex ways genes replicate and interact with the environment. Saying that a theory is ‚Äúproven‚Äù is a common mistake - watch out for it!¬†\nSecond, most scientists (and psychologists) draw from Karl Popper‚Äôs philosophy of science, which adheres to a requirement for science called falsifiability - the ability to find evidence that rejects a theory (‚Äúability to falsify‚Äù). This means a theory is never ‚Äòproven‚Äô because scientists are continually looking for evidence to reject the theory. It also means that a belief that cannot be tested or rejected would be rejected as a scientific belief - you have to be able to test your belief in some observable way. Falsifiability is where I think science separates from religious belief - someone with faith doesn‚Äôt need evidence, and that‚Äôs okay! It just doesn‚Äôt make the belief scientific under science‚Äôs narrow definition.¬†\n\nHypotheses are specific predictions that researchers make about what they expect to see in the data if their theory is supported or is not supported by data.¬†\n\nThe alternative hypothesis (sometimes written HA) is the researcher‚Äôs own belief. It may seem strange to label your belief ‚Äúalternative‚Äù when this word is used for things that are supposed to be different (‚Äúalternative rock = it‚Äôs not your parent‚Äôs rock & roll!‚Äù), and we are so used to thinking that our beliefs are the default that to call them alternative seems wrong. This is an example of previous beliefs bias, and the decision to label our beliefs as the ‚Äúalternative‚Äù is an example of science trying to correct this bias. It‚Äôs a small and symbolic correction, but it‚Äôs better than nothing.\nThe null hypothesis (sometimes written H0) is the label given for whatever evidence would not support the researcher‚Äôs belief.\n\nEXAMPLE : Let‚Äôs say you believe that smoking cannabis hurts a person‚Äôs memory. What‚Äôs the alternative hypothesis? What‚Äôs the null?\n\nNull Hypothesis : People who smoke cannabis will perform BETTER or NO DIFFERENT on a memory test than people who do not smoke marijauna. 13\nAlternative Hypothesis : People who smoke cannabis will perform WORSE on a memory test than people who do not smoke marijauna.\n\n13¬†Many students forget to include the ‚ÄúNO DIFFERENT‚Äù in the null hypothesis example above. However remember that the null hypothesis is everything that your theory is not. So if your theory is that smoking cannabis HURTS memory, then finding no difference in the memory of pot smokers vs.¬†non-smokers would not support your theory, and is thus part of the null hypothesis. (Note : some of y‚Äôall may have missed this because you are remembering learning about ‚Äúdirectional‚Äù vs ‚Äúnon-directional‚Äù hypotheses - we will discuss this more in Lecture 8.)Practice : Identifying Models, Null, and Alternative Hypotheses. Here‚Äôs a quick practice quiz to review identifying models based on research questions and theories, and identifying the null and alternative hypotheses based on a given theory.\n\n\nStep 3 : Collect Data\nAs described above, scientific theories are supported by evidence called data. How researchers collect data is something we will discuss in more detail over the next few weeks. In general, researchers have to figure out how to measure the variables they are interested in studying (e.g., ‚Äúhow will I know if someone is happy or not?‚Äù), and then find participants (people or animals) to study (e.g., ‚Äúwhose happiness should I study‚Äù).¬†\nFor example, in our smoking cannabis example the decisions about how to measure memory (a music memory game or a word recognition task?) or how much cannabis (a little or a lot?) or who to study (Berkeley students or your grandparents?) in the study would likely influence the results.\nThe decisions that researchers make about how to define their measures and the participants they will study are very important, and will influence the results of the study (and thus our knowledge of the topic), so stay tuned for more discussion!\n\n\nStep 4 : Use Data to Test Theories\nFinally, researchers look to see if the data they collected supports or rejects their theory (and evaluate how strongly their theory is supported or rejected). We‚Äôll talk much more about using data to test theories this semester.\nIn our smoking cannabis example, the researcher would look to see which group did better on the memory test, and if the smokers did worse, then their theory would be supported (by this one study, at least).\n\n\nStep 5 : Repeat\nOnce a study is completed, researchers repeat the scientific method in two different ways.\nResearchers will sometimes repeat the exact same steps a second time - something called replication. Replication is critical to science.\nScientists also repeat the scientific method with a new question that is based on the results from their first study. Science is a process, and researchers are never done learning about how to better predict & control the world. There‚Äôs always more research to do. For example, after identifying a pattern between smoking cannabis and memory, a researcher might ask whether the type or dosage of cannabis intake would influence memory, whether abstinence could reverse the effects of memory impairment, or whether cannabis and coffee together might lead to a different result. Note that this is not an example of replication, since researchers are not testing the same question they had - but it‚Äôs repeating the scientific method for a new question inspired by the old. This is good, and part of scientific progress. But the problem is that the incentives for scientific publication focused almost entirely on ‚Äúnew‚Äù research, and not really making sure that the ‚Äúold‚Äù research was valid.\n\n\nIt‚Äôs Never Actually Easy\nScience is hard, and people are very complex, which makes psychological science very hard. Unfortunately, psychology is in a bit of a Replication Crisis. There are two parts to the crisis:\n\nResearchers tend not to seek to replicate their (or others‚Äô) results.\nSystematic attempts to replicate previous research suggests that the majority of results in psychology do not replicate14.\n\n14¬†You can go deeper into some of the drama surrounding the replication project here. lemme know what you think on the discord thread for this week.We will chat more about this in our next lecture. But I think that‚Äôs enough reading for now?",
    "crumbs": [
      "List of R Code",
      "Describing People",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Why Statistics?</span>"
    ]
  },
  {
    "objectID": "chapters/2R_Data.html",
    "href": "chapters/2R_Data.html",
    "title": "Data Data Data",
    "section": "",
    "text": "Part 1 : Defining Data\nHello students! We are back. In this week‚Äôs reading, you‚Äôll learn how to work with data in R - first how psychologists define data, then how to graph (and think critically about) different types of data with your HUMAN BRAIN, and finally how to work with data (and datasets) in R.\nResearchers seeking to bring a scientific approach to psychology love data, and aim to convert complex human thoughts, feelings, and behaviors into numbers. This is called quantitative data, and is the default approach almost all modern research psychologists take. For example:\nWhile we will learn more about the various ways psychologists collect data later this semester, for now it‚Äôs important to acknowledge that these numbers have error (called measurement error), a fair amount of work in psychology goes into learning how to reduce measurement error as much as possible, and the existence of measurement error is one form of error that will contributes to the ERROR term in our linear models.\nQuantitative data takes two forms that we will see in this class - numeric (sometimes called continuous data)¬† and categorical data (sometimes called ‚Äústring‚Äù data).",
    "crumbs": [
      "List of R Code",
      "Describing People",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data Data Data</span>"
    ]
  },
  {
    "objectID": "chapters/2R_Data.html#numeric-variables",
    "href": "chapters/2R_Data.html#numeric-variables",
    "title": "Data Data Data",
    "section": "Numeric Variables",
    "text": "Numeric Variables\n\nDefinition : Numeric Variable\nNumeric variables are when the value of the variable is a number (e.g., your Extraversion score is 62 on a scale from 0 to 100; or you said ‚Äúum‚Äù fifty times yesterday, or scrolled your phone five times since starting this reading.¬†\nContinuous variables are a special type of numeric variable, with the idea that values of the variable represent an ‚Äúinfinite‚Äù range of possibilities.\nWe often simplify complexity into discrete groups, but for most complex phenomena, I believe that it‚Äôs best to think of life as a spectrum. For example, while I look at my walls and say ‚Äúthey are blue‚Äù, a physicist who really understands color theory would be able to interpret the wavelength of light that is being reflected off these walls, and understands that that wavelength really just a point on an infinite spectrum (bound by a certain range).\n\nPsychologists working in R often use numeric data, and see this as a list (or vector) of numbers. For example, below is data on the narcissism (variable = NPI; a measure of how self-absorbed; self-interested) of a group of Berkeley Haas MBA students were1.\n1¬†These data come from the hormone_data.csv file, which should be updated to our course page. You‚Äôll learn more about how to access these data files later in this chapter.\nhaas &lt;- read.csv(\"./chapter_data/hormone_data.csv\", stringsAsFactors = T) \nhaas$NPI\n\n  [1] 3.43 4.40 3.13   NA 3.00   NA 3.05   NA   NA 3.63 3.95 3.08 3.83 3.23 3.80\n [16] 1.80 3.68 3.48 2.73 3.40 2.95 3.65 2.88 2.50 3.13 2.75 3.00 3.15 3.30 3.20\n [31] 2.43 3.13   NA 3.33 2.88 2.55 3.35 3.30 3.28 3.08 2.48 2.78 4.58 2.53 2.78\n [46] 2.88   NA 3.43 2.48 3.50 2.40 3.28 3.63 3.13 2.05 2.98 2.53 3.45 2.80 2.60\n [61] 2.05 2.60 2.73 3.25 2.78 2.93 3.08 3.33 4.05 3.65 3.15 3.33 3.98 4.48 3.00\n [76] 3.00 3.25 4.28 3.13 3.83 3.98 4.13 3.73 3.28 2.85 3.28 3.70 3.03 3.38 2.88\n [91] 3.20 3.40 3.70 3.50 2.58 2.90 3.00 3.35 2.63   NA   NA 4.05 3.43 4.28 3.00\n[106] 3.10 3.13 2.83 2.78 2.33 2.68 2.88 2.53 3.80 3.48 3.15 4.38 3.73 3.80 2.60\n[121] 3.38 3.13\n\n\nProfessor Interpretation I learn a lot just from this simple output! 2\n2¬†We‚Äôll go over how to load datasets in R in Part 2. For this part of the chapter, the focus is on understanding what R is doing, and why we are doing it.\nhaas &lt;- read.csv(‚Äú./chapter_data/hormone_data.csv‚Äù, stringsAsFactors = T‚Äù) is the R command that loads the dataset. Note that the path to the datafile - hormone_data.csv - is specific to the way I‚Äôve stored these data in my file system. You‚Äôll learn below how to change this to access your own dataset :)\nhaas$NPI is the R command that was used to generate the output below; a list of 122 individual Narcissism scores.\n\nI know there‚Äôs 122 individual scores because R is keeping count for me using indexing; the numbers in brackets. [1] shows that the first person in the dataset has a narcissism (NPI) score of 3.43; [118] 3.73 shows that this is the score for the 118th person in the dataset, and then I can count up to 122. There are much faster ways to do this, but I can do it this way too :)\nI also see there are a few missing data points in the responses - these are marked as NA. This could be people who didn‚Äôt complete the survey or were excluded for other reasons (e.g.¬†missing data).\n\n\n\n\nGraphing : The Histogram\nAlways always always graph your data. Graphs will help summarize, organize, and highlight important features of your data that would be impossible to see just by looking at numbers.3\n3¬†yes, dear student, it is true : a picture is worth‚Ä¶a thousand words.The Histogram is a common way researchers illustrate numeric variables. The histogram organizes data - you lose the individual values, but gain understanding in seeing how data are grouped together, which helps you observe patterns and get a quick summary of the variable.\nThere are two dimensions of a histogram :\n\nthe x-axis (the horizontal axis; what goes across) : displays the values of the variable as organized into groups (or ‚Äúbreaks‚Äù, in R).¬†\nthe y-axis (the vertical axis; what goes up and down) : displays the frequency (or count) of the individuals in the data who ‚Äúbelong‚Äù to that group.\n\nLet‚Äôs look at our MBA friends again, through the power of a histogram. As you look at a graph, it‚Äôs important to practice thinking about what you learn from the data. Let‚Äôs avoid fancy stats terminology for now; it‚Äôs not necessary for our purposes!!\n\nhist(haas$NPI, xlab = \"Narcissism Score\", col = 'black', bor = 'white', main = \"\")\n\n\n\n\n\n\n\n\n\nthe code : this code draws a histogram using the hist() function. I‚Äôve also added several arguments that change some of the default settings to give the graph some digital style.\n\nxlab gives the x-axis of the graph a nice label.\ncolchanges the color of the bars to black.\nbor changes the color of the lines surrounding the bars to white.\nmain changes the title. In this case, \"\" sets the title to be nothing, so there‚Äôs no title.\n\nthe graph : okay, what did R do!\n\nx-axis : this reports the grouped values of the individual narcissism scores.¬†\ny-axis : this reports how many people were in each group.\n\nprofessor interpretation with no fancy stats language needed.\n\nmost people (around 42?) had a narcissism score between 3 and 3.5\na few people were really high in narcissism‚Ä¶above a 4.5. I‚Äôm not sure from the graph exactly how many people were in this group, or what their score was.\na few people were really low in narcissism‚Ä¶below a 2. Again, I‚Äôm not sure from the graph exactly how many people were in this group, or what their score was, but I see their humble selves!\nI also notice that this is not a super large study - the frequencies on the y-axis are relatively low numbers.\n\n\n\n\nActivity : Think about Data!\nOkay, your turn. Below is a graph from the same MBA students, but this time measuring their testosterone levels. Look over the graph, THINK about what you see, and then expand the textbox below (click on the arrow on the right side of the green box) to see what I wrote.\n\nhist(haas$test, \n     xlab = \"Testosterone Level\", main = \"\", \n     col = 'black', bor = 'white')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipPROFESSOR SPOILERS : Expand this textbox when you are ready by clicking on the arrow ‚Äî&gt;\n\n\n\n\n\n\nthe graph\n\nx-axis : this reports the grouped values of the individual testosterone scores, measured in some kind of density (pg/ML)\ny-axis : this reports how many people were in each group.\n\nwhat I see and observe with no stats language :\n\nmost people (around 25+12+20 = 77) had a testosterone level between 50 and 100.\nthere were no scores below zero (which makes sense) and one person who had a very high level of testosterone. I‚Äôm not a hormone researcher, but the non-negative values seems good, and I might want to make sure that there wasn‚Äôt some data entry error for the extreme score.\nAgain, I notice that this is not a super large study‚Ä¶.and these relatively low numbers seem similar to the narcissism data, which makes sense since they came from the same study.",
    "crumbs": [
      "List of R Code",
      "Describing People",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data Data Data</span>"
    ]
  },
  {
    "objectID": "chapters/2R_Data.html#categorical-variable",
    "href": "chapters/2R_Data.html#categorical-variable",
    "title": "Data Data Data",
    "section": "Categorical Variable",
    "text": "Categorical Variable\n\nDefinition : Categorical Variable\nA categorical variable is when the values of the variable represent different groups (or categories). Categories are often useful and simple ways to group individuals together. For example, when I see a color, I don‚Äôt ever describe it in terms of its color hex code or specific wavelength - I just call it by the simple primary color that I got from the crayola box‚Ä¶.maybe the 24 color version if I‚Äôm feeling fancy.\nWhich of the shades below would you call blue?4\n4¬†they are all one color - the human color. just kidding from the top left it‚Äôs 2, 3, 4, and 7. but I‚Äôm blue green color blind so you should argue with me in the comments.\n\n\n\n\nThe broad label for the variable is called the factor, and the specific groups of data are called levels. So in the color example, the category of color would be the factor, and the different groups of color would be the levels.\nAs another example, researchers can measure gender with categories such as female, male, transgender, and other. Identify the factor and levels in this example.\n\n\n\n\n\n\nTipWhat are the factors and levels in the example above?\n\n\n\n\n\n\nFactor : would be the variable of gender. Generally there‚Äôs one factor label for each variable.\nLevel : would be the categories female, male, transgender, and other.\n\n\n\n\n\n\nCulture in Statistics : Gender Identity\n\n\n\nHi folks! It‚Äôs me again, Open-Source Mickey Mouse to talk with you about the idea that statistics has a culture that is socially constructed by people like you!\nOne domain where this is particularly relevant is in the area of gender identity. While many people identify as ‚Äúmale‚Äù or ‚Äúfemale‚Äù, some people don‚Äôt fit into these categories. (Seems pretty simple to me to let folks exist as they want! But I‚Äôm just a poor open-source servant freed from my corporate overlords.) Yet as folks in power have recently forced this narrow binary view of gender identity onto everyone, it becomes even more critical to engage with these ideas and try to define a science that can capture the complexity of human life in ways that let people be their full selves.\nUnfortunately, most psychological researchers still hold on to Male / Female binaries in the way they measure gender or sex, yet there are many reasons - both scientific and humanistic - to give people more range to express important aspects of their identity.\n\n\n\nfacebook‚Äôs attempt at measuring more complex categories.\n\n\nIndeed, categories almost always oversimplify the complexity of life, yet are often used by people (and researchers) because they can sometimes be useful and simple shortcuts for us to understand the world.\nIf you are simply interested in doing a superficial survey of a variable like race, ethnicity, or gender, then I think categorical data can be a fine -if often unscientific - approach, and would recommend giving all people the chance to express their identity in some way. For example, here‚Äôs an article describing research on the way that exclusionary categories can negatively impact science, and offering clear and easy recommendations for reseachers to broaden science (e.g., reporting non-binary participants).\nHowever, if you are interested in really digging into a variable, then a continuous approach is almost always best, since it allows for more flexibility in capturing complexity in variation. We‚Äôll discuss more on how to do this in a few weeks when we learn about measuring continuous variables with likert scales.\n5\n5¬†a continuous approach to measuring gender. hi if u still reading, let me know if you have any thoughts on this section of the chapter.\n\nGraphing : The Categorical Plot\nThe histogram only describes a graph for numeric data, since it organizes numbers into groups (it kind of turns complex variation into more simple categories). When the variable is categorical, people call it a plot ü§∑.\nThis graph looks very similar to our histogram :¬†\n\nthe x-axis (the horizontal axis; what goes across) : displays the levels of the factor variable.\nthe y-axis (the vertical axis; what goes up and down) : displays the frequency (or count) of the individuals in the data who ‚Äúbelong‚Äù to each group.\n\nAlright, back to our good MBA friends. This is a graph of the categorical variable ‚Äúsex‚Äù. Again, look over the graph, THINK about what you see (no stats terminology; what do you learn!), and then highlight my text to see what I wrote about.\n\nplot(haas$sex, col = 'black', bor = 'white', xlab = \"Sex\")\n\n\n\n\n\n\n\n\n\nThe Code : this code draws a histogram using the plot() function. Note that I‚Äôve asked R to plot the variable haas$sex. I‚Äôve also added several arguments that change some of the default settings to give the graph some digital style.\n\nxlab gives the x-axis of the graph a nice label.\ncolchanges the color of the bars to black.\nbor changes the color of the lines surrounding the bars to white.\n\nThe Graph :\n\nx-axis : this reports levels (female; male) of the categorical factor variable Sex.¬†\ny-axis : this reports how many people were in each group.\n\nWhat I see and observe with no stats language :\n\nIt appears that the researchers only measured sex as a f/m binary (or that no participants reported a category other than female or male).\nthere were more males than females in this dataset. This matches my perception / stereotype of what a typical MBA program might look like; however I looked into it and Haas reports a larger percentage of female enrollments in the MBA program; so our data may not serve as a representative sample of the true population.6\n\n\n\n6¬†Will learn about these ideas much later this semester! here‚Äôs a link to learn more about women in Haas. capitalism will eat us all eventually I guess, and good to challenge our stereotypes / perceptions!",
    "crumbs": [
      "List of R Code",
      "Describing People",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data Data Data</span>"
    ]
  },
  {
    "objectID": "chapters/2R_Data.html#defining-variables-in-r",
    "href": "chapters/2R_Data.html#defining-variables-in-r",
    "title": "Data Data Data",
    "section": "Defining Variables in R",
    "text": "Defining Variables in R\nBelow are some videos, and R code, that review how to define a variable in R. Yeah! I‚Äôm not going to share the Rscripts I‚Äôm using for these videos, since I think there‚Äôs value in typing this out yourself to get that muscle memory in üí™ü§ò but let me know if you disagree / there‚Äôs a reason to provide them to y‚Äôall!\n\nVideo : Defining Numeric Variables in R\n\n\nobjects - assign function for numerical data\nc() : combining data together.\nlength() : the number of objects\n\n\n\nVideo : Graphing Numeric Variables in R\n\n\nhist() : a graph\nchanging arguments : xlab, ylab, main\n\n\n\nVideo : Defining Categorical Variables in R\n\n\ncategorical data (‚Äústring‚Äù)\nas.factor() : to convert a string to a categorical variable\nlevels() : to see the levels of your categorical variable.\n\n\n\nVideo : Graphing categorical variables in R\n\n\nplot()\nchanging arguments : col, bor, main; xlab; ylab",
    "crumbs": [
      "List of R Code",
      "Describing People",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data Data Data</span>"
    ]
  },
  {
    "objectID": "chapters/2R_Data.html#the-dataframe",
    "href": "chapters/2R_Data.html#the-dataframe",
    "title": "Data Data Data",
    "section": "The Dataframe",
    "text": "The Dataframe\n\nDefinition : Rows and Columns\nAs a researcher, you‚Äôll be interested in understanding not only one variable at a time, but will be interested in a dataset - multiple variables about an individual that are organized - in order to see how variables are related to each other (remember : this is a function of the linear model).\nThe datasets in our class will be stored on Dropbox; you can find a link to this on our course page, under the Course Materials module (see below for the image that you‚Äôre looking for).\n\n\n\n\n\nYou‚Äôll learn how to load these datasets later in this lecture. For now, what is a dataset?\nA dataset is really a dataframe - a two-dimensional way to organize data - and takes the following structure in this class.\n\nthe rows define the individual in the dataset.¬†rows go across horizontally, like a rowboat going across a lake.\nthe columns define the variables in the dataset. ¬†go up and down vertically; like what might support a bridge.\n\nLook at the example below - again from our budding MBAs in the Haas program. What do you observe about the rows and columns? What does this tell you about the dataset?\n\n\nIn the example dataframe above - I see 5 rows (representing 5 individuals) and five columns (representing five variables like the person‚Äôs age, their sex, their testosterone levels, political ideology, and their NPI (Narcissism) score.¬† or their Race, etc.) Note that the names of the variables do not count as a row, since these are not individuals in the dataset, and you would need to know more about the dataset to know that ideo = political ideology, or test = testosterone. I also see that R is helpfully telling me that this is only a snapshot of the entire dataset - the whole dataframe has 122 entries (rows), meaning that there‚Äôs 122 MBA students in this dataset, and 5 total columns (which we can all see here.)\nAs a researcher, you have access to an entire dataset (either collected by you or another researcher) that organizes multiple variables for each individual. This semester, we‚Äôll work with a variety of datasets on different psychological topics - not just haas students.\n\n\nDefinition : Indexing\nThe dataset gives us access to all the individual rows and columns at once, we will often want to focus on one specific variable (or individual) at a time. Indexing refers to a flexible method of selecting a specific set of data from a larger collection. Previously‚Äô we‚Äôve seen indexing when asking R to produce a large set of numbers; for example asking it to count from 1 to 100.\n\n1:100\n\n  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n [91]  91  92  93  94  95  96  97  98  99 100\n\n\nThe indexing shows up as brackets next to the actual data, and is way for R to index that the [1]st data entry is the number 1, the [24]th entry is the number 24, and so on.\nWhen I ask R to show me the dataset haas, the output can be overwhelming. Below is what R shows you when you ask to see the haas dataset. And this is a relatively small dataset with just 122 individuals (rows) and five variables (columns).\n\nhaas\n\n    age    sex   test ideo  NPI\n1    NA   male     NA   NA 3.43\n2    NA   male 143.89   NA 4.40\n3    NA   male     NA   NA 3.13\n4    30   male     NA    4   NA\n5    28   male     NA    4 3.00\n6    33 female     NA    5   NA\n7    NA   male     NA   NA 3.05\n8    NA   male     NA   NA   NA\n9    NA   male  77.95   NA   NA\n10   NA   male     NA   NA 3.63\n11   26   male 126.29    3 3.95\n12   31   male  59.48    5 3.08\n13   27   male  89.45    4 3.83\n14   27   male  82.80    3 3.23\n15   28   male  97.39    4 3.80\n16   30   male  54.80    4 1.80\n17   28   male  46.07    4 3.68\n18   24   male     NA    4 3.48\n19   32   male  87.40    3 2.73\n20   25   male  85.01    4 3.40\n21   27   male     NA    3 2.95\n22   25   male     NA    4 3.65\n23   24   male 102.86    3 2.88\n24   28   male  73.60    3 2.50\n25   27   male     NA    2 3.13\n26   29   male  90.68    4 2.75\n27   28   male  44.70    3 3.00\n28   29   male     NA    4 3.15\n29   30   male  77.61    4 3.30\n30   26   male  57.20    5 3.20\n31   31   male  49.92    4 2.43\n32   28   male     NA    3 3.13\n33   32   male     NA    5   NA\n34   27   male  85.01    2 3.33\n35   32   male  74.86    2 2.88\n36   27   male 125.89    3 2.55\n37   29   male  77.07    4 3.35\n38   30   male     NA    4 3.30\n39   26   male  30.54    4 3.28\n40   27   male  73.76    3 3.08\n41   32   male  65.61    3 2.48\n42   29   male  51.23    4 2.78\n43   28   male  85.17    3 4.58\n44   28   male     NA    3 2.53\n45   29   male  57.14    2 2.78\n46   30   male  65.31    4 2.88\n47   28   male  53.07    4   NA\n48   28   male 104.65    3 3.43\n49   31   male  90.32    3 2.48\n50   NA female  24.71   NA 3.50\n51   26 female  20.99    3 2.40\n52   27 female     NA    4 3.28\n53   28 female     NA    3 3.63\n54   28 female   5.51    5 3.13\n55   27 female  40.49    4 2.05\n56   25 female  35.05    2 2.98\n57   26 female  57.71    4 2.53\n58   26 female  27.36    4 3.45\n59   27 female  59.37    3 2.80\n60   26 female  21.63    3 2.60\n61   29 female     NA    3 2.05\n62   25 female     NA    7 2.60\n63   37 female  39.50    4 2.73\n64   28 female  42.35    4 3.25\n65   24 female  32.42    3 2.78\n66   29   male  97.63    4 2.93\n67   28   male 131.51    3 3.08\n68   26   male     NA    4 3.33\n69   27   male     NA    5 4.05\n70   29   male     NA    2 3.65\n71   27   male 140.53    2 3.15\n72   26   male     NA    3 3.33\n73   28   male     NA    3 3.98\n74   22   male  90.88    5 4.48\n75   29   male 148.24    5 3.00\n76   29   male 132.24    4 3.00\n77   27   male  82.43    4 3.25\n78   26   male  73.43    4 4.28\n79   35   male     NA    2 3.13\n80   27   male 100.49    4 3.83\n81   25   male  94.31    4 3.98\n82   27   male  72.53    2 4.13\n83   30   male 133.35    4 3.73\n84   27   male  59.77    4 3.28\n85   30   male  91.83    4 2.85\n86   29   male  82.13    3 3.28\n87   30   male 172.15    3 3.70\n88   29   male 228.17    2 3.03\n89   27   male 133.70    3 3.38\n90   27   male  89.24    4 2.88\n91   28   male  88.62    3 3.20\n92   28   male  86.83    4 3.40\n93   25   male 138.65    4 3.70\n94   33   male  59.75    4 3.50\n95   28   male  46.30    3 2.58\n96   31   male 107.02    3 2.90\n97   24   male  60.16    2 3.00\n98   26   male     NA    4 3.35\n99   27   male 107.71    3 2.63\n100  23   male  99.64    2   NA\n101  32   male 131.51    3   NA\n102  29   male  91.94    4 4.05\n103  25   male  53.67    3 3.43\n104  25   male     NA    4 4.28\n105  27 female     NA    4 3.00\n106  27 female  59.24    2 3.10\n107  30 female  28.03    5 3.13\n108  28 female  33.38    5 2.83\n109  26 female  53.31    4 2.78\n110  28 female  27.53    4 2.33\n111  29 female  16.89    4 2.68\n112  25 female  51.53    4 2.88\n113  26 female  37.15    4 2.53\n114  27 female  50.55    2 3.80\n115  28 female     NA    4 3.48\n116  28 female  41.35    5 3.15\n117  29 female  64.66    5 4.38\n118  24 female  37.95    2 3.73\n119  27 female  54.26    4 3.80\n120  27 female     NA    4 2.60\n121  27 female 113.41    4 3.38\n122  29 female  41.35    3 3.13\n\n\nI am overwhelmed with data. So it will be important to find ways to target the data that we want. There are several ways we can do this!\nBecause a dataset has two different dimensions, we have to use two coordinates to index the dataset - one coordinate for the row(s) that we want to focus on, and one coordinate for the column(s) that we want to focus on.\n\n\nIndexing an Entire Dataset\nBecause a dataset has two different dimensions, we have to use two coordinates to index the dataset - one coordinate for the row(s) that we want to focus on, and one coordinate for the column(s) that we want to focus on.\n\ndata # this reports the entire dataset. In the example to the right, I‚Äôve typed in haas (since this is the name of the dataset in this example) and see the dataset reported.\ndata[i, j] # this code returns a specific row (i), and a specific column (j). The convention is to use the letter i for a row first, then j for a column [you can remember this order as RC Car, or R is Cool]. For example\n\nhaas[2,3]\n\n[1] 143.89\n\n\nShows me that R has highlighted the second row and third column of the dataset - the second person‚Äôs testosterone level is 143.89 units.\ndata[ , j] # if you leave a blank for the rows, then R will return all of the rows, and whatever column you specify. This can be good for looking at a specific variable for all individuals. For example, the following code returns all of the testosterone data (the third column).\n\n```{r}\nhaas[,3]\n```\n\ndata[i, ] # ¬†if you leave a blank for the column, then you would see all of the columns for a specific row. This can be good for looking at a specific individual‚Äôs entire dataset; such as all of participant 2‚Äôs data below.\n\nhaas[2,]\n\n  age  sex   test ideo NPI\n2  NA male 143.89   NA 4.4\n\n\nhaas[i:i, c(j, j)] # you can adapt this code to give a range of values too. for example, if I want to see rows 4-10 and columns 1 and 3, I would run the following code.\n\nhaas[4:10, c(1,3)]\n\n   age  test\n4   30    NA\n5   28    NA\n6   33    NA\n7   NA    NA\n8   NA    NA\n9   NA 77.95\n10  NA    NA\n\n\n\n\n\nIndexing a Single Variable\n\ndata$variable # You can also use the $ (dollar sign) in R to reference a single variable from a dataset. This is very useful, because you can use the name of the variable instead of the numerical index. So, for example, if I want to highlight the testosterone levels of the haas dataset, I would run the following :\n\nhaas$test\n\n  [1]     NA 143.89     NA     NA     NA     NA     NA     NA  77.95     NA\n [11] 126.29  59.48  89.45  82.80  97.39  54.80  46.07     NA  87.40  85.01\n [21]     NA     NA 102.86  73.60     NA  90.68  44.70     NA  77.61  57.20\n [31]  49.92     NA     NA  85.01  74.86 125.89  77.07     NA  30.54  73.76\n [41]  65.61  51.23  85.17     NA  57.14  65.31  53.07 104.65  90.32  24.71\n [51]  20.99     NA     NA   5.51  40.49  35.05  57.71  27.36  59.37  21.63\n [61]     NA     NA  39.50  42.35  32.42  97.63 131.51     NA     NA     NA\n [71] 140.53     NA     NA  90.88 148.24 132.24  82.43  73.43     NA 100.49\n [81]  94.31  72.53 133.35  59.77  91.83  82.13 172.15 228.17 133.70  89.24\n [91]  88.62  86.83 138.65  59.75  46.30 107.02  60.16     NA 107.71  99.64\n[101] 131.51  91.94  53.67     NA     NA  59.24  28.03  33.38  53.31  27.53\n[111]  16.89  51.53  37.15  50.55     NA  41.35  64.66  37.95  54.26     NA\n[121] 113.41  41.35\n\n\ndata$variable[i] # We can then use indexing to narrow this down. Because a variable only has one dimension (it‚Äôs just a collection of individuals; not rows and individuals) I only need to use one coordinate to index the specific individual(s) I want to find.\n\n\nhaas$test[2]\n\n[1] 143.89\n\n\n\ndata$variable[i:i] # Can be used to find a range of individuals. These numbers need to be sequential for this code to work.\n\nhaas$test[1:3]\n\n[1]     NA 143.89     NA\n\n\ndata$variable[c(i,i,i)] # If you want to find individuals who are not next to each other, you need to use the c (combine) function to combine multiple coordinates together. You can have as many coordinates here as you want. For example :¬†\n\nhaas$test[c(2,3,14:18, 116:118)]\n\n [1] 143.89     NA  82.80  97.39  54.80  46.07     NA  41.35  64.66  37.95\n\n\n\n\n\nTest Yourself : Indexing!\nOkay, practice time. Use the haas dataset and your knowledge of indexing to identify what R would show if you typed in the following commands (no R required)\n\nhaas$age[47]\nhaas$NPI[1:3]\nhaas[51,2]\nhaas[60, ]\nhaas[ , 6]\n\n\n\n\n\n\n\nTipAnswer Key : Indexing!\n\n\n\n\n\n\n28\n3.43 4.40 3.13\nfemale\n26 female 21.63 3 2.6\nyou would get an error; there is no 6th column.\n\n\n\n\n\n\n\nIn R : How to Navigate Datasets\nLet‚Äôs get some more practice with some actual data. Before we learn how to import data in R, we can work with a super exciting dataset that is already part of the R program - a dataset on the weights of chickens (chkwt).\nWatch the two videos below to see how I navigate this dataset; here‚Äôs a link to the RScript that I use in the videos.¬†\n\nVideo : Checking Datasets in R\n\n\nlength() : counts the number of objects (variables) in a dataset (or any object)\nnrow() : counts the number of rows (participants) in a dataset\n\nhead() : looks at the first six rows of a dataset\n\n\nVideo : Navigating Datasets with Indexing\n\n\nUse this video to answer the check-in questions below.\n\n\n\nCheck-In : Navigating Variables and Datasets with Indexing\nUse the fake dataset below and your knowledge of navigating datasets with indexing to identify what answer R would give if you gave it the following code. (Note : no R is needed to complete this problem).\n\ndata\n\n  StudentID favDrink age\n1         1     boba  20\n2         2     boba  19\n3         3     boba  54\n4         4   coffee  22\n5         5    water  38",
    "crumbs": [
      "List of R Code",
      "Describing People",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data Data Data</span>"
    ]
  },
  {
    "objectID": "chapters/2R_Data.html#the-.csv-file",
    "href": "chapters/2R_Data.html#the-.csv-file",
    "title": "Data Data Data",
    "section": "The .csv File",
    "text": "The .csv File\n\nDefinition : The .csv File\nAs a researcher, you will work with data that you (or other researcher friends) have collected. The .csv file (csv stands for comma separated variables) is one of the most common formats for storing data that you will encounter. You‚Äôve probably encountered this before, and likely have opened this file type in a program like excel or google sheets. But in this class, we‚Äôll learn to load these files directly into R. Using R has two advantages :\n\nR is way more powerful than excel or Google Sheets.\nR allows us to document all of our steps. This semester, we‚Äôll learn how to make changes to the dataset (e.g., changing the names of a variable; removing bad data; transforming data). It‚Äôs important to be completely transparent about these changes, and doing these changes in R (with an R Script!) will ensure that we document our steps for our future selves / other researchers.\n\n\n\nVideos : How to Load Datasets in R\nThere are two different ways to load a data file into R : one way (‚Äúthe point and click method‚Äù) involves clicking some boxes, like most of y‚Äôall are used to doing. The second way involves typing in an R command. Below are videos that highlight each method.\n\n\n\n\n\n\nImportant\n\n\n\nRegardless of which method you use, there are three things you want to check every time you load data : change the name of the dataset (to make it something simple to type and memorable); check the headers (make sure the variables have names, since sometimes), and set stringAsFactors = TRUE (which will automatically convert all your string variables into categorical factor variables, which is almost always what you will want to do in this class.\nThese instructions are highlighted because students often forget. So note the importance! They are important steps!!!\n\n\n\nVideo : Importing Data with the ‚ÄúPoint and Click‚Äù Method and Console Method\n\n\nthe ‚Äúpoint and click‚Äù method of loading data\nthe console / Rscript method of loading data\n\n\n\nVideo : For Posit Users : working with projects and loading data in the cloud.\n\n\nposit.cloud users! You will need an extra step that folks using posit.cloud have to use to upload datasets to the cloud (and then import them).",
    "crumbs": [
      "List of R Code",
      "Describing People",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data Data Data</span>"
    ]
  },
  {
    "objectID": "chapters/2R_Data.html#practice-quiz-2",
    "href": "chapters/2R_Data.html#practice-quiz-2",
    "title": "Data Data Data",
    "section": "Practice Quiz 2",
    "text": "Practice Quiz 2\nOkay, this was a LOT. Like drinking water from a fire hose. I promise this will get easier, and it just requires practice. Let‚Äôs see how clear the professor‚Äôs instructions were with this‚Ä¶practice quiz!\nCLICK THIS LINK to take the Practice Quiz. Use your knowledge of navigating and loading datasets to answer¬†\n\nHere‚Äôs an Rscript with the practice quiz questions. Submit your answers to the link above for credit (this is graded based on completion).\nHere‚Äôs a video key with the practice quiz answers. Please attempt the quiz on your own; if you get stuck, refer to the relevant videos above. And feel free to reach out on Discord if something‚Äôs confusing or I got something wrong. Appreciate y‚Äôalls engagement!",
    "crumbs": [
      "List of R Code",
      "Describing People",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data Data Data</span>"
    ]
  },
  {
    "objectID": "chapters/3R_Description.html",
    "href": "chapters/3R_Description.html",
    "title": "Description",
    "section": "",
    "text": "Part 1 : Descriptive Statistics\nHello! This week, you‚Äôll learn how to use statistics to describe data in terms of centrality and complexity (and review how the mean and standard deviation can be thought of as prediction and error).\nWe use statistics as a language to describe how individuals are both similar to, and different from, each other.",
    "crumbs": [
      "List of R Code",
      "Describing People",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Description</span>"
    ]
  },
  {
    "objectID": "chapters/3R_Description.html#centrality",
    "href": "chapters/3R_Description.html#centrality",
    "title": "Description",
    "section": "Centrality",
    "text": "Centrality\nStatistics like the mean, median, and mode focus on what is most central to a distribution of data. These measures of centrality reduce the complexity of individual scores, but in doing so emphasize some of the core aspects of the variable. You can think of centrality like a summary of the data. While a lot of information is lost in summary (e.g., Lord of the Rings is much more about some Hobbits trying to destroy an evil ring), a summary often gets the key points across in few words (e.g., Lord of the Rings does spend a lot of time describing Hobbits trying to destroy an evil ring.)\n\nThe Mean\n\nDefinition.\nThe mean (also known as the average) is one of the most important statistics, and the foundation for much of what we will do in this class.\nYou probably learned this equation as something like, ‚Äúadd up all the numbers and divide by the total number of numbers‚Äù. This is technically correct, but scientists like to be more specific and formal, and so statistics uses a more specific language.\nThe statistical equation for the mean is below; it may look confusing, but it is really just a fancy version of the same definition of the mean you know and love. Specifically, the formula defines the mean as ‚Äúequal to the sum of all individual x-values (starting with the first individual and ending with the last individual in the dataset), and divided by the sample size.‚Äù\n\n\n\n\n\n\n\nThe Equation\nBreakdown of Terminology\n\n\n\n\n\\[\n\\Large \\bar{y} = \\frac{\\sum_{i=1}^n y_i}{n}\n\\]\n\n\\(\\bar{y}\\) = ‚Äúy bar‚Äù = the mean of y\n\\(\\Sigma\\) = Sigma = the ‚Äúsum‚Äù of numbers (starting from the number on the bottom all the way through the number on the top).\n\\(i\\) = index = an individual\n\\(n\\) = sample size = the number of individuals in the dataset\n\\(y\\) = a variable\n\n\n\n\nSo, when reading this formula, you would ‚Äúsay‚Äù : ‚Äúy bar (the mean) is equal to the sum of all individuals of y (\\(y_i\\)) starting with the first (\\(i=1\\)) and going all the way through the total number of individuals (\\(n\\)). And then you take that sum, and divide by the total number of individuals (\\(n\\)).\nSee?! Simple!\n\n\nWhy It Matters.\nOne important characteristic of the mean is that it is the value of a distribution that is closest to all the other scores. This means that the mean serves as our ‚Äúbest guess‚Äù (a prediction) about the value of what any random individual scores in this distribution. This is why the mean is also called the ‚Äòexpected value‚Äô.\nYou‚Äôve internalized this in many ways - if you know the average temperature for summer in the Bay Area is a high of 70 degrees and a low of 56, then you can predict on any given day that you might need a sweater in the morning and evening, but will be hot in the afternoon.\nHOWEVER - the mean does not perfectly describe all scores in the distribution (because people are complex - we are not the same). We‚Äôll talk more about these ‚Äúerrors‚Äù in prediction when we talk about the standard deviation (a measure of complexity) below.\n\n\n\nThe Median\n\nThe Definition\nThe median is the value that is in the very middle of a distribution of scores. This means that 50% of scores in the distribution are higher than the median value, and 50% of the scores are lower than the median value.¬† If there‚Äôs an odd set of numbers, the median is the middle number. If there‚Äôs an even set of numbers, the median is the average of the two middle numbers.¬†\nSo imagine two sets of numbers : what number is in the very middle?\n\n2, 3, 3, 5, 10, 14, 19\n2, 2, 3, 3, 5, 10, 14, 19\n\n\n\n\n\n\n\nActivity : What‚Äôs the median?\n\n\n\n\n\n\n2, 3, 3, 5, 10, 14, 19 = the median value is 5, since this number is in the middle. note that when calculating the median, the values of the data should be organized from smallest to larges.\n2, 2, 3, 3, 5, 10, 14, 19 = the median value is 4; since there is no ‚Äútrue‚Äù middle, 4 is the value that is in the middle of the two nearest values 3 and 5.\n\n\n\n\n\n\nWhy It Matters\nBecause the median is defined entirely by the middle of a distribution, it is less influenced by extreme data (outliers) than the mean. Below are two graphs of height. The one on the left is for a collection of people, and the histogram on the right is a graph of heights for a collection of people and some big friendly giants. The median of this distribution is illustrated as a vertical blue line, and the mean of this distribution is illustrated as a vertical red line.\n\n\n\n\n\n\n\n\nThe Mode\n\nThe Definition\nThe mode is the most common number in a set of numbers. If two numbers are equally common in a distribution, then the distribution is said to be bi-modal (and more than two most common numbers = multimodal)\nSo if your distribution was : 2, 3, 3, 5, 10, 14, 19, 20, 20, then the mode would be‚Ä¶1\n1¬†the mode would be 3 and 20, since these are the most frequent\n\nWhy It Matters\nThe mode is rarely reported in research articles - I sometimes see it in within-person studies, or in situations where researchers are measuring psychophysiological measures (like heart rate or vagal tone) where the peak of the distribution might indicate something important.\nConceptually, it can be interesting to note the presence of a mode, and to think about why a bi-modal (or multi-modal) distribution occurs.¬†\nFor example, look at the following illustration : why might this bimodal distribution occur?\n\n\n\n\n\n\n\nReasons for this bimodal distribution.\n\n\n\n\n\n\n\n\n\n\n\n\nThe more conventional answer is that this bimodal distribution represents overlap between male and females in a dataset.\n\nAnother possibility is that this graph illustrates a python who has swallowed an elephant.2\n\n\n\n\n\n\n\n\n2¬†see Antoine de Saint-Exup√©ry‚Äôs (1943). The Little Prince.",
    "crumbs": [
      "List of R Code",
      "Describing People",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Description</span>"
    ]
  },
  {
    "objectID": "chapters/3R_Description.html#complexity",
    "href": "chapters/3R_Description.html#complexity",
    "title": "Description",
    "section": "Complexity",
    "text": "Complexity\nComplexity refers to the ways that data differ from each other - really the focus of variation. And yet, as scientists, we are trying to understand patterns in that variation, and so have developed a language to talk about some of the systematic ways that individuals differ.\n\nRange.\n\nDefinition.\nOne way people differ is by the extreme ends; this is called the range. Most people refer to the range as the lowest and highest value; though sometimes the range is defined as the distance between the highest and lowest value.\n\n\nWhy It Matters.\nThe extreme low and high of your variable give you a sense about the limits of variation. How to interpret these limits really depends on the variable you are measuring; for a variable like reaction time in some cognitive task, I would expect the low end to be zero (or something near zero, but not negative) and the high end influenced by how long I would expect the task to take (usually anywhere from less than a second for a quick reaction to a few minutes.) If the high end of the range was something like 30-minutes, I would think that something went wrong.\nAs a researcher (and teacher) I often use the range as a quick way to ensure the data are correct - that is, to confirm that the lowest and highest scores are possible values given the way the variable was measured. For example, I check the range when looking at test scores to make sure no students got a negative score or score above 100% (both impossible in my class.)\n\n\n\nOutliers.\n\nThe Definition\nOutliers are extreme values that are so different from the rest of the data that you remove them from the dataset because you worry that they might cause problems for your data. Outliers exist because of errors in data entry (for example, the person who lists their age as 1009 instead of 19) or because they represent individuals who are qualitatively different from the rest of your data (for example, the participant who is a billionaire and lists that as their income.3)\n3¬†No shame billionaires! You are just very different from the rest of us. Let us know if I can hang out on your yacht? You belong to an understudied group and I have some research ideas. kthxbye.How extreme is too extreme? Some folks like to come up with rules for making this decision based on the number of standard deviations away from the mean or some other metric. I appreciate those efforts, but personally believe it‚Äôs better to judge outliers based on the qualitative decisions above (and use past research as a guide). Whatever you do, make sure to (1) justify your decisions, (2) report these decisions in your code and analyses, and (3) make any decisions about removing outliers as early as possible and before you start making predictions.\n\n\nWhy It Matters.\nThe presence of outliers in your data has the potential to influence your results in ways that will bias your results. For example, if someone wandered off in the middle of a cognitive task, their outliers for reaction time might make you think people took way longer to complete the task than people really needed to take. If I included everyone who didn‚Äôt take the exam (and got a zero) in taking the average of the exam score, I might think that students did worse on the exam than they really did, when I should have excluded these zeros to get a better representation of how people did on the exam.\nIt‚Äôs therefore super critical to a) identify any possible outliers in the data, and b) remove them from data analysis, and c) be 100% transparent that you have removed data. You will learn to do this using R in Part 4 below.\n\n\n\nSkew.\n\nDefinition.\nSkew describes asymmetry in the shape of the distribution. A graph that is symmetrical has no skew.\n\n\n\n\n\nIt‚Äôs easy to confuse the different types of skew, so I like to think of skew as a type of pokemon (‚Äúpikaskew‚Äù), where the direction of skewness is defined by its tail, since Pokemon (at least the ones I can think of) have tails. This is my trick; feel free to use it (you are welcome) or develop your own!¬†\nskew ‚Üí pikachu ‚Üí ‚Äì&gt; tail\n\n\nWhy It Matters.\nSkew can help us to think about why people\n\nNegative Skew (also called ‚ÄúLeft Skew‚Äù) is when the distribution ‚Äútail‚Äù sticks out on the negative (low) end of the measure. A negatively skewed distribution means that there‚Äôs a greater probability of high scores than low scores, and can be explained by ceiling effects (a measure does not differentiate between high scorers - like an exam that almost everybody aces because you were all prepared.)\nPositive Skew (also called ‚ÄúRight Skew‚Äù) is when the distribution ‚Äútail‚Äù sticks out on the positive (high) end of the measure. A positively skewed distribution means that there‚Äôs a greater probability of low scores than positive scores, and can be explained by floor effects (a measure does not differentiate between low scorers).\nNo Skew (also called the ‚ÄúNormal Distribution‚Äù) is when the distribution is symmetrical.\n\n\n\n\nKurtosis\nKurtosis describes the size of the tails, relative to the middle of the distribution. I think of kurtosis as the pointiness of the distribution. Mesokurtic is a distribution that looks ‚Äúnormal‚Äù, leptokurtic is a distribution that is skinny in the middle (with many individual scores in the tail of the distribution), and platykurtic is a distribution wide in the middle with few observations in the tails.\n\n\n\n\n\nLike skew, there are ways to quantify kurtosis, but we won‚Äôt cover this in the class. I almost never see kurtosis reported as a statistic in journals.",
    "crumbs": [
      "List of R Code",
      "Describing People",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Description</span>"
    ]
  },
  {
    "objectID": "chapters/3R_Description.html#the-standard-deviation-a-mix-of-centrality-and-complexity",
    "href": "chapters/3R_Description.html#the-standard-deviation-a-mix-of-centrality-and-complexity",
    "title": "Description",
    "section": "The Standard Deviation : A Mix of Centrality and Complexity",
    "text": "The Standard Deviation : A Mix of Centrality and Complexity\n\nThe Definition\nStandard Deviation (sd) is a way to quantify the ‚Äòaverage‚Äô variation in your dataset. More specifically, it is the average distance between all individual data points in the distribution, and the mean of the distribution. These distances (between an individual score and our prediction) are called residuals.\nThe standard deviation is always positive, since it describes the average distance of individual scores from the mean (residuals), and not whether those residuals are above or below the mean. A standard deviation of zero means there is no variation - all scores in the distribution are the same. The larger the standard deviation, the more variation there is in the data. There‚Äôs no limit to how large the standard deviation might be.\nUnderstanding what the standard deviation is, and how it‚Äôs calculated, is a critical skill. In the videos below I walk through how to calculate standard deviation by ‚Äúhand‚Äù. You will never actually use this approach in real-life, but I find it very helpful to fully understand what a standard deviation is, and how (and why) we use residual scores in statistics. With the chickenweights dataset, of course.\n\nVideo : The Mean as Prediction\nWith no other knowledge about an individual, the mean is our best prediction about what an individual is like.\n\n\n\nVideo : Residuals as Error\nResiduals refer to the fact that the mean is not a perfect prediction for every person; people will differ from our predictions. These differences between each individual‚Äôs actual score and our predicted value (in this case, the mean) are called residuals. The residuals will always be actual score minus prediction - I remember this because as researchers we care about actual people first.\n\n\n\nVideo : The Sum of (Squared) Residual Errors\nThe sum of squared errors (SS) is a way to quantify the total residuals when using the mean to make predictions. As you‚Äôll see in the video, we must first square the residual differences in order to remove the direction (since we care about the total error, not whether the individual was above or below our prediction.). And then we sum these squared differences to quantify the total (squared error).\n\n\n\nVideo : The (Unsquared) Average of These Squared Residuals = Standard Deviation\nThe standard deviation is the squared root of these averaged squared differences. Or, in less technical terms, the standard deviation is the average of how much people differ from the mean - a measure of the average variation.\n\n\n\n\nWhy This Matters\nGreat question! The standard deviation does two things :\n\nThe standard deviation quantifies how much the ‚Äúaverage‚Äù person differs from the average of the individuals in a group. This is a great statistic to have, since it quantifies how much individuals differ from each other on average (between-person variation)\nThe standard deviation serves as a baseline for our predictions. The mean is a GREAT starting place for our predictions, but soon we will want to make more sophisticated predictions. We will do this with our good friend the linear model, and will know whether we can improve our predictions by decreasing the amount of residual error.",
    "crumbs": [
      "List of R Code",
      "Describing People",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Description</span>"
    ]
  },
  {
    "objectID": "chapters/3R_Description.html#video-describing-happiness-world-happiness-report",
    "href": "chapters/3R_Description.html#video-describing-happiness-world-happiness-report",
    "title": "Description",
    "section": "Video : Describing Happiness (World Happiness Report)",
    "text": "Video : Describing Happiness (World Happiness Report)\nIn this video, I walk through loading the World Happiness Report (in our Chapter Dataset folder on the course page), and describing and graphing the happiness variable.",
    "crumbs": [
      "List of R Code",
      "Describing People",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Description</span>"
    ]
  },
  {
    "objectID": "chapters/3R_Description.html#video-standard-deviation-of-happiness-world-happiness-report",
    "href": "chapters/3R_Description.html#video-standard-deviation-of-happiness-world-happiness-report",
    "title": "Description",
    "section": "Video : Standard Deviation of Happiness (World Happiness Report)",
    "text": "Video : Standard Deviation of Happiness (World Happiness Report)\nIn this video, I review how to calculate the standard deviation, and what this statistic tells you about the variable happiness.",
    "crumbs": [
      "List of R Code",
      "Describing People",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Description</span>"
    ]
  },
  {
    "objectID": "chapters/3R_Description.html#r-code-descriptive-statistics",
    "href": "chapters/3R_Description.html#r-code-descriptive-statistics",
    "title": "Description",
    "section": "R Code : Descriptive Statistics",
    "text": "R Code : Descriptive Statistics\nBelow is a list of code we‚Äôll use to calculate descriptive statistics in R.\n\n\n\n\n\n\n\nR Command\nWhat It Does\n\n\nsummary(dat)\nReports descriptive statistics for all variables in the dataset.\n\n\nsummary(dat$variable)\nReports descriptive statistics for a continuous variable.\nReports frequency for a categorical variable.\n\n\nmean(dat$variable, na.rm = T)\nReports the mean (average) of a variable; you must include the na.rm = T argument if there is missing data (otherwise R will return NA as the result).\n\n\nmedian(dat$variable, na.rm = T)\nReports the median (middle point) of a variable.\n\n\nrange(dat$variable, na.rm = T)\nReports the lower limit and upper limit of the variable.\n\n\nsd(dat$variable, na.rm = T)\nReports the standard deviation of the variable.\n\n\nhist(dat$variable)\nabline(v =mean(dat$variable))\nDraws a line on a plot or histogram at specified values (e.g., this draws a vertical line at the mean of dat$variable. You can replace v with h to draw a horizontal line. We will use abline() later in the semester in a different way.\n\n\npar(mfrow = c(i, j))\nSplits your graphics window into i rows and j columns (replace i and j with numbers)",
    "crumbs": [
      "List of R Code",
      "Describing People",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Description</span>"
    ]
  },
  {
    "objectID": "chapters/3R_Description.html#practice-quiz-with-r-practice-quiz-3-describing-variables",
    "href": "chapters/3R_Description.html#practice-quiz-with-r-practice-quiz-3-describing-variables",
    "title": "Description",
    "section": "Practice Quiz with R : Practice Quiz 3 (Describing Variables)",
    "text": "Practice Quiz with R : Practice Quiz 3 (Describing Variables)\nInstructions\nUse R and the twitter_emotion_data.csv to answer the following questions in the check-in above.\n\nHere‚Äôs a link to the dataset (load this into R) and a link to the Codebook (that describes the dataset)\nLink to a video key; but good to try this on your own!\n\nQuestions\n\nLoad the data and check to make sure the data loaded correctly. How many individuals (tweets; in this dataset, each row = one tweet) are in the dataset?\nGraph the variable retweet_count. How would you describe the shape of this variable?\nWhat do you learn about the variable retweet_count from this graph?\nWhat is the mean of the variable retweet_count?\nWhat is the median of the variable retweet_count?\nWhat is the standard deviation of the variable retweet_count? [note : you do not need to do this by hand]?\nWhat is the lowest value (range) of the variable retweet_count?\nWhat is the highest value (range) of the variable retweet_count?\nNow, work with the categorical variable Orientation. How many Liberal (tweets) are in the dataset?\nHow many Conservative (tweets) are in the dataset?",
    "crumbs": [
      "List of R Code",
      "Describing People",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Description</span>"
    ]
  },
  {
    "objectID": "chapters/3R_Description.html#video-removing-outliers",
    "href": "chapters/3R_Description.html#video-removing-outliers",
    "title": "Description",
    "section": "Video : Removing Outliers",
    "text": "Video : Removing Outliers\nThe video below describes how to remove outliers, using a datset built into R.\n\nWe will practice removing outliers in lecture! It will get easier.",
    "crumbs": [
      "List of R Code",
      "Describing People",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Description</span>"
    ]
  },
  {
    "objectID": "chapters/3R_Description.html#the-publication-process",
    "href": "chapters/3R_Description.html#the-publication-process",
    "title": "Description",
    "section": "The Publication Process",
    "text": "The Publication Process\nResearchers spread their scientific knowledge by publishing research papers. You can read more about how researchers publish research HERE. However, the TLDR is something like this :\n\nResearcher has an idea! They assemble a team of others interested in the idea, and do a literature review in order to gain background knowledge on the topic.\nResearcher designs a study, collects data, analyzes the data, and writes up the data as a report. All the steps of the scientific method. This is what you will do for the final project!\nResearcher submits the report to a scientific journal. A journal is a collection of articles that are usually united by some common theme. In general, the shorter the name of the journal, the more prestigious4. For example, the ‚ÄúJournal of Research in Personality‚Äù is considered less prestigious than the ‚ÄúJournal of Personality and Social Psychology‚Äù, which is less prestigious than the journal called ‚ÄúPsychological Science‚Äù, which is considered less prestigious than the journal called ‚ÄúScience‚Äù.¬†\nAn editor decides whether to review or reject the article. Editors make an initial decision - based on a summary of the study and a letter that the author writes to the editor - whether the research article might be a good fit for the journal. If so, they pass it along to the next step. If not, they send a ‚ÄúThanks, but‚Ä¶.‚Äù rejection letter.\nThe editor sends the article to peer-reviewers. Peer reviewers are other researchers who have some related research interests or skills in the topic of the paper. They will look over the article. The editor may know these people, or they get asked by another peer-reviewer who didn‚Äôt want to do it but nominated someone else to step into the role. Peer-reviewers work on a completely voluntary basis - it‚Äôs seen as required service, and there‚Äôs a bit of professional reputation to maintain in doing this work. Yes, there are problems with unpaid labor in academia. We can chat about that if you‚Äôd like; ask questions / raise it as an issue on Discord.\nThe editor makes a decision on the paper based on the feedback from the peer-reviewers. The editor summarizes the feedback, and either accepts the paper, accepts as long as the person makes necessary revisions, asks the researcher to ‚ÄúRevise and Resubmit‚Äù (this is called an R&R - probably the most common outcome, and does not guarantee that the paper will be accepted if the revisions are made, but will get sent out to peer-reviewers again), or rejects the paper. In any case, the author will see the comments made by the peer-reviewers and the editor.\nThe (accepted) paper goes to a proofreader and is published. Hooray! This process probably takes anywhere from 6 months (insanely fast) to 2 years (or more, depending on the number of revisions that are required).\n\n4¬†Prestige is a very subjective concept in science. However, scientists have found many ways to quantify it, as described in the sections below!",
    "crumbs": [
      "List of R Code",
      "Describing People",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Description</span>"
    ]
  },
  {
    "objectID": "chapters/3R_Description.html#publish-or-perish",
    "href": "chapters/3R_Description.html#publish-or-perish",
    "title": "Description",
    "section": "Publish! Or Perish‚Ä¶?",
    "text": "Publish! Or Perish‚Ä¶?\nAsk any grad student or professor - the publication process is stressful, unpredictable, slow, and threatening. Grad students are required to publish papers in order to have a chance at an academic job as a researcher (and even extremely productive and thoughtful graduate students are not guaranteed an academic job), professors are required to publish papers in order to get a chance of getting tenure (and even extremely productive and thoughtful researchers are not guaranteed tenure).\nThis creates incredible pressure on researchers to get results; pressure that often can interfere with people‚Äôs ability to do GOOD science. We‚Äôll talk about this more throughout the semester; bring questions to class / Discord!",
    "crumbs": [
      "List of R Code",
      "Describing People",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Description</span>"
    ]
  },
  {
    "objectID": "chapters/3R_Description.html#types-of-research-articles",
    "href": "chapters/3R_Description.html#types-of-research-articles",
    "title": "Description",
    "section": "Types of Research Articles",
    "text": "Types of Research Articles\nWe‚Äôll learn how to read and dissect scientific articles this semester, but first it will be important to learn how to identify There are a few different types of articles that researchers write :\n\nOriginal Reports : An original report is where the researcher(s) write about the results of a novel study they did to test some theory. This means the researchers did something ‚Äúnew‚Äù - usually they collected and analyzed new data to test a theory, or analyzed existing data in a new way. Here‚Äôs an example of an original report on the topic of emotion regulation.\nReplication : A replication is a type of study where a researcher repeats the steps they (or another) researcher did, and sees if they get the same result. As we discussed, psychology (and many other fields) is in a replication crisis. This type of article was not very common before the 2010s, but is more common now. Still, faculty tend to be biased toward producing original reports - a school like Berkeley or Stanford would not hire a researcher just for doing replications. Here‚Äôs an example of a replication on the topic of emotion regulation. Note this is not really a direct replication, since they replicated in a different population.\nMeta-Analyses : A meta-analysis is where researchers take other people‚Äôs data, collect it, and analyze it in order to see broad trends across an entire field. For example, a meta-analysis might take all the research on whether there‚Äôs a relationship between playing violent video games and violent behavior, and analyze this existing research in terms of common themes, such as the type of video games the researchers studied, the measures of violence, and the results. Meta-Analyses can be a great way to look at a broad trend, but they rely on the assumption that the individual studies they summarize are, in fact, valid themselves. That is, if there are systematic biases in the way researchers study a topic, the Meta Analysis won‚Äôt solve or even identify those problems. Here‚Äôs an example of a meta-analysis done on the topic of emotion regulation.\nReview Article : Review articles summarize existing research without doing any additional data analysis (in contrast to a Meta-Analysis, in which there is data analysis of past research). This is the closest thing to a paper you might write in an English class - the authors take past research, summarize it in terms of common themes, and maybe highlight limitations, or new directions the field might take. Review articles are a great way to get a broad overview of a topic, since they summarize and organize past research, and will often highlight next steps that researchers should take. Here‚Äôs an example of a review article done on the topic of emotion regulation.",
    "crumbs": [
      "List of R Code",
      "Describing People",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Description</span>"
    ]
  },
  {
    "objectID": "chapters/3R_Description.html#the-peer-review-process",
    "href": "chapters/3R_Description.html#the-peer-review-process",
    "title": "Description",
    "section": "The Peer Review Process",
    "text": "The Peer Review Process\nIn order to publish their results, researchers have their peers review their work, and provide comments or suggestions. The peer-review process ideally serves two purposes :¬†\n\nPeer Reviewers Help Improve the Research. The peers doing the reviewing are supposed to have expertise in the topic of the paper. This allows them to suggest ways to improve the paper. These suggestions can run from the simple (such as recommending other researchers to reference in the introduction or additional analyses to run) to very involved (suggestions for additional studies to run, different methods to use, or different people to study).\nPeer Reviewers Provide a Vote of Confidence in the Ideas and Analyses of the Paper. The editor of a journal will look to the peer reviewers for evidence that the research is ‚Äúhigh quality‚Äù and / or novel enough to be published. There‚Äôs a fair amount of bias here - some reviewers think the research is good but not considered a ‚Äúgood fit‚Äù for the journal. But the ‚Äúpeer-reviewed‚Äù label of a journal gives at least one layer of confidence that some other people like this research.\n\nOne important aspect of the peer-review process is that the peer-reviewers are anonymous to the author, and sometimes the author is anonymous to the peer-reviewers. Ideally, this helps prevent previous beliefs bias (since some researchers may have positive or negative impressions of each other) and social influence bias (since a famous researcher at a fancy school may have their research more trusted than someone with less prestige to their name or institution). However, psychological fields are often small enough that people tend to know who‚Äôs doing what research, and there are other cues that can tip peer-reviewers off about who the author of a study is (for example, the author of a study will likely reference their own work, since their new study builds off their old study.)\n\n\n\n\n\n\nAcademic Publishers are Predatory Capitalists.\n\n\n\n\n\n\n\nYou know how the music business can hurt artists and interrupt the free flow of groovy music??? Well, academic publishers are no different, and can make profits in the billions of dollars. They do this by charging exorbitant fees for accessing peer-reviewed articles, and not paying the researchers who publish papers any money. That‚Äôs right; researchers get a 0% commission of any sales of their academic articles. It is a horrible and corrupt system that deserves to die.\nBut you don‚Äôt have to take my word for it! You can read more about the problem with publishers and a (controversial) solution - sci-hub - here. Here‚Äôs an [OPTIONAL] longer articlethat goes into the history of academic publishing, and why it‚Äôs so corrupt. Here‚Äôs a videothat covers some of the same info. Here‚Äôs an even longer article that goes into sci-hub and ‚Äúscientific communism‚Äù. Feel free to skim or skip all of this! But lemme know if you read / found something you thought was interesting :)",
    "crumbs": [
      "List of R Code",
      "Describing People",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Description</span>"
    ]
  },
  {
    "objectID": "chapters/3R_Description.html#finding-research-articles",
    "href": "chapters/3R_Description.html#finding-research-articles",
    "title": "Description",
    "section": "Finding Research Articles",
    "text": "Finding Research Articles\nNext week we will find research articles related to YOUR research interests. I like to use Google Scholar for this - I find the search features powerful and Google will comb the internet for free versions of research articles (so I don‚Äôt have to be on campus or have proxy access to UC Berkeley‚Äôs library or ‚Äústeal‚Äù articles from publishers using sci-hub).\nFor example, searching for one of my dissertation chapters shows this page.\n\n\n\nHow to Find Articles\nSome tips for finding an article related to your topic :¬†\n\nUse the right ‚Äújargon‚Äù. As part of the operationalization process, scientists use specialized terms. What you might call ‚Äúholding it in‚Äù researchers call ‚Äúexpressive suppression‚Äù; a ‚Äújerk‚Äù would be someone ‚Äúlow in agreeableness‚Äù; that feeling of ‚Äúbeing hella stressed before an exam but also low key stepping up because of that stress‚Äù is the ‚Äúpsychophysiological distinction between challenge and threat‚Äù. As you search for research related to your topic, take note of how researchers are describing related phenomena, and adjust your terms as needed. This is part of building your schema for the topic.\nLook at past research on the topic. If you‚Äôve found a relevant article on your topic, it‚Äôs likely that the article has referenced other research that is also relevant. Read or scan through the introduction (or the references section) and see if there‚Äôs something that looks related to your interests.\nLook at future research on the topic. If you found a relevant article on your topic, it‚Äôs likely that other researchers have also read that article, and used it in their future research. Google Scholar has a ‚ÄúCited By‚Äù button []that you can use in order to see more recent articles that have referenced the article you found. This is a particularly useful way to find more recent research if you found a ‚Äúclassic‚Äù in the field, or check for replications or controversies.\nOld Research is Okay! But look for new research too. Many students wonder if an ‚Äúold‚Äù study is still relevant. Some papers are ‚Äúclassics‚Äù in the field, and great to read. But it‚Äôs likely that our field‚Äôs understanding of self-esteem has changed from 1970. If you are hoping to build your schema on a topic, finding a review article from the last 10 years (or 5; or 2!) would be a good place to start.\n\n\n\nHow to Evaluate an Article\nIt can often be overwhelming for students to sift through the masses of research on a topic and know what‚Äôs most important and relevant.¬†\nReading the article and using your critical thinking skills / psychological training is the best approach. To do this, it is recommended to go to Graduate School, where you canspend multiple years reading as much as you want on a beautiful college campus, taking classes where you can deeply engage with the research and ideas, immerse yourself in meaningful intellectual conversations had by other graduate students and kindly professors - the gleam of knowledge and excitement of supporting the next generation of researcher in their eye. Oh, that is not interesting to you / grad schools are flooded with applications / you can‚Äôt get an office hour appointment with your professor who actually / you don‚Äôt have the privilege of spending 5-8 years getting paid near-poverty wages to be a poor scholar?¬†\nWell, below are a few other ideas to make superficial , all depending on our good friend social influence bias :\n\nThe Citation Count. Google scholar allows you to see how many other research articles have referenced the article that you found. While this can be a nice way to see how influential a paper is, a paper could be referenced a lot for reasons other than its validity, and maybe no one has read the most amazing paper in the world.\nThe Impact Factor of the Journal. Journals have different ‚Äúprestige factors‚Äù, and the impact factor is one way to quantify this prestige. Impact factor is usually defined as the number of times the average article in a journal has been referenced by other researchers in a year. So an impact factor of 2 means that each article in that journal is referenced by two other articles in a year. The impact factor is something you have to look up - journals usually track this. I wouldn‚Äôt spend too much time worrying about it, but it‚Äôs a quick way to get a very superficial sense of the journal‚Äôs reputation. For example, let‚Äôs see how the impact factor relates to our rule of ‚Äúbroader journal name = more prestige‚Äù.\n\n\n\n\n\n\n\n\nJournal\nImpact Factor According to Google in 2024\n\n\nJournal of Research in Personality\n2.6\n\n\nJournal of Personality and Social Psychology\n6.4\n\n\nPsychological Science\n10.1 [couldn‚Äôt find a recent stat on this tho]\n\n\nScience\n44.7 [this escalated quickly]\n\n\n\n\nThe Researcher and Institution. Another way to evaluate an article is by evaluating the author. Does it look like this research has produced other cool research, or do you find their work boring and problematic for some reason? Is this researcher well known and respected in the field? Do they seem to have a happy photo with all their graduate students on their lab website, or is their lab website 10 years old and just has one sad looking graduate student asking for help with their eyes? Has the researcher continued to produce interesting, reliable research? Or were they the subject of a replication scandal?\n\n\n\nVideo Examples : Using Google Scholar to Find Research\n\nUsing Google Scholar\n\n\nHow to find articles, use the right jargon, and do some very superficial evaluations of the article‚Äôs quality.\n\n\n\nExporting APA Citations\n\n\nThe ‚Äú‚Äù button on Google Scholar makes life so much easier!! No more memorizing APA format!! Hooray!!!",
    "crumbs": [
      "List of R Code",
      "Describing People",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Description</span>"
    ]
  },
  {
    "objectID": "chapters/4R_Scales.html",
    "href": "chapters/4R_Scales.html",
    "title": "Measures",
    "section": "",
    "text": "Part 1 : Likert Scales\nThis week, you‚Äôll learn about how (and why) a likert scale can be used to define continuous variation, and how to create a likert scale in R.",
    "crumbs": [
      "List of R Code",
      "Describing People",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Measures</span>"
    ]
  },
  {
    "objectID": "chapters/4R_Scales.html#definition-theory",
    "href": "chapters/4R_Scales.html#definition-theory",
    "title": "Measures",
    "section": "Definition & Theory",
    "text": "Definition & Theory\nA likert scale is a common survey method psychologists use to measure continuous variation. Likert scales can be used for self-report surveys (where an individual answers questions about themselves), or given to observers (where an individual answers questions about another person - either someone they know, or someone they are actively observing).¬†For example, to the right is an example likert scale - the Rosenberg Self-Esteem Scale1. Here‚Äôs a link to take the survey and get feedback.\n1¬†Rosenberg, M. (1965). Rosenberg Self-Esteem Scale (RSES) [Database record]. APA PsycTests. Here‚Äôs a link to take the survey and get feedback.\n\n\n\n\nBelow are some common terms we will use when describing likert scale. (There‚Äôs a video that goes over these terms with another example below too.)\n\n\n\n\n\n\n\n\nTerm\nDefinition\nUsage / Example\n\n\nScale\nThe variable that you want to measure as a continuous variable.\nSelf-esteem is often measured with the Rosenberg Self-Esteem Scale (1965)\n\n\nItem(s)\nThe specific question(s) in the scale. Each item measures some aspect of the variable the researcher is interested in.\nThe Rosenberg Self-Esteem Scale (RSE; 1965) is a ten item scale, which means it has ten questions about self-esteem.\n\n\nResponse Scale\nHow people answer the scale items. People give a number rating on a fixed range of options with labels. Many 5-point scales include the following labels (1 = Strongly Disagree; 2 = Disagree; 3 = Neutral; 4 = Agree; 5 = Strongly Agree.)\nThe RSE was originally written to use a 4-point rating scale from 0 (Strongly Disagree) to 3 (Strongly Agree). When Professor includes the RSE in his studies, he might change the response scale to go from 0-4 to so it has an odd-number of answers to allow people to say they are ‚Äúneutral‚Äù.\n\n\nPositively-\nKeyed Item\nAn item that measures the high end of the scale, where answering ‚Äúyes‚Äù to the question means you are high on this variable.\n‚ÄúOn the whole, I am satisfied with my life‚Äù is a positively-keyed item, because answering 4 (Strongly Agree) means the person says they are high in self-esteem.\n\n\nNegatively-Keyed Item\nAn item that measures the low end of the scale, where answering ‚Äúyes‚Äù to the question means you are low on the variable.\n‚ÄúI certainly feel useless at times‚Äù is a negatively-keyed item, because answering 4 (Strongly Agree) means the person says they are low in self-esteem.\n\n\nReverse Scoring\nHow researchers ‚Äúflip‚Äù the negatively-keyed items to be positively-keyed. To calculate how to reverse-score an item, you can add the lower and upper limit of the full range. So to reverse-score a response scale that goes from 0 to 4 = 0 + 4 = subtract the negatively keyed items from 4. A response scale that goes from 1 to 15 = 1 + 15 = subtract the negatively keyed items from 16. And so on.\nThis is confusing. We will practice in lecture some more, okay?\nTo reverse score the negatively-keyed item, you would subtract the person‚Äôs 4 from 4 (since the scale range is 0 to 4, so 0+4 = 4). A response of 4 (Strongly Agree) to the question I certainly feel useless at times means that for this question, the person‚Äôs self-esteem would be rated as a 0.\nPOP QUIZ : a survey has a response scale that goes from 1-7. What number would you subtract from in order to reverse score a negatively-keyed item?2\n\n\n\n2¬†You would subtract from 8, since 1+7 = 8. So a 7 for a negatively-keyed item on the 1-7 scale would be turned into a 1, since 8-7 = 1Advantages of Likert Scales\n\nThey are easy to administer as a self-report or observational survey, and provide structured data.\nThe principle of aggregation describes a phenomenon where combining multiple items into one scale will provide a more reliable and continuous measure. Remember, the normal distribution is a theoretical distribution that exists when there are multiple explanations for one variable that occur randomly in a population. The multiple items in a scale are one way to try and measure the multiple random explanations for variation. And indeed, as you‚Äôll see in the R demonstration, when you combine multiple categorical items into one variable, the distribution of the variable looks more normal than any individual item.\nResearchers can assess the reliability of the multiple items that were included in the measure. For example, if we are reliably measuring extraversion, then there should be. Cronbach‚Äôs (alpha) is a statistic that estimates the internal consistency (reliability) of a scale, and is based on (a) the similarity of people‚Äôs responses to the items in a scale (the more similar, the higher the reliability) and (b) the number of items in a scale (sales with many items will tend to have higher cronbach than will likert scales with just a few items.) There‚Äôs no official ‚Äúrule‚Äù for what‚Äôs considered good or bad alpha, but below are some guidelines:\n\nŒ± &gt; .8 = GREAT! Your scale is reliable\nŒ± = .5 - .7 = OKAY! Your scale has low reliability, so something may be wrong with your measure.\nŒ± &lt; .5 = UH OH‚Ä¶your scale has really low reliability. Below are a few possible reasons:\n\nyour scale only has a few items: remember, that Œ± is influenced by the number of items in your scale; so scales with a few items will almost certainly have a low alpha reliability.\nwere the items in scale incorrectly coded (reverse scored items)?\nis the scale measuring different things (no consistency)? It could be that your scale isn‚Äôt very precise, and is measuring different variables.",
    "crumbs": [
      "List of R Code",
      "Describing People",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Measures</span>"
    ]
  },
  {
    "objectID": "chapters/4R_Scales.html#video-example-the-extraversion-scale",
    "href": "chapters/4R_Scales.html#video-example-the-extraversion-scale",
    "title": "Measures",
    "section": "Video Example : The Extraversion Scale",
    "text": "Video Example : The Extraversion Scale\nHere‚Äôs another example of a likert scale - Extraversion items adapted from Big Five Inventory 2 (Soto & John, 2017).\n\n\n\n\nWatch This Video.\n\n\n\nCheck-In\nPLEASE COMPLETE THIS CHECK-IN TO PRACTICE REVERSE-SCORING.",
    "crumbs": [
      "List of R Code",
      "Describing People",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Measures</span>"
    ]
  },
  {
    "objectID": "chapters/4R_Scales.html#i-like-to-read",
    "href": "chapters/4R_Scales.html#i-like-to-read",
    "title": "Measures",
    "section": "I Like To Read!",
    "text": "I Like To Read!\nIn this demonstration, we‚Äôll use the class dataset to create a scale to measure differences in EXTRAVERSION. To do this, we will need to complete the following steps:\n\n1. Import and Check the Data.\nFirst, we‚Äôll need to load the data, and check to make sure it is imported correctly. You can access these data here - personality measures of extraversion (how social people say they are) and conscientiousness (how organized people say they are).\n\nec &lt;- read.csv(\"../datasets/ec_data.csv\") # Note: make sure to change the *path* in the `read.csv()` function to point R to the correct spot to import the ec dataset from YOUR computer.\n\n\n\n2. Create a data.frame that isolates the items in the scale.\nNow, I‚Äôll just create a smaller dataframe of the variables that I want to work with for the extraversion scale. From the codebook, I see that the extraversion scale is made up of the items e1-e6r, with the r indicating items that are negatively-keyed.\n\nextra.df &lt;- data.frame(ec$e1, ec$e2, ec$e3, # the three positively keyed items\n                    ec$e4r, ec$e5r, ec$e6r) # the three negatively keyed items\nhead(extra.df) # this code checks my work and make sure my newly created dataframe in fact contain the positively and negatively keyed items\n\n  ec.e1 ec.e2 ec.e3 ec.e4r ec.e5r ec.e6r\n1     2     3     2      4      4      4\n2     2     4     3      3      3      4\n3     3     4     2      4      5      4\n4     2     3     2      3      4      3\n5     5     5     5      1      1      1\n6     1     3     3      3      4      4\n\n\n\n\n3. Correctly reverse-score the negatively-keyed items in the scale.\nNow, I need to reverse-score the negatively keyed items. Since the scale ranged from 1 to 5, I need to subtract the negatively keyed items from 6 to reverse the scoring (so 6 - 1 = 5, and 6 - 5 = 1.)\nNote that you can calculate how to reverse-score an item by adding the lower and upper limit of the full range. So to reverse-score a response scale that goes from 0 to 4 = 0 + 4 = subtract the negatively keyed items from 4. A response scale that goes from 1 to 15 = 1 + 15 = subtract the negatively keyed items from 16. And so on.\nI can again use the head() function to check my work and confirm that I successfully reverse scored the variables. Note that you can reverse-score the variables in one step; I don‚Äôt really do this twice when creating a scale :)\n\nextra.df &lt;- data.frame(ec$e1, ec$e2, ec$e3, # the three positively keyed items\n                    6-ec$e4r, 6-ec$e5r, 6-ec$e6r) # the three negatively keyed items\nhead(extra.df) # this code checks my work and make sure my newly created dataframe in fact contain the positively and negatively keyed items. Note that there seems to be more consistency in the scores for each individual - people who are low in e1-e3 are now also low in e4r - e6r.\n\n  ec.e1 ec.e2 ec.e3 X6...ec.e4r X6...ec.e5r X6...ec.e6r\n1     2     3     2           2           2           2\n2     2     4     3           3           3           2\n3     3     4     2           2           1           2\n4     2     3     2           3           2           3\n5     5     5     5           5           5           5\n6     1     3     3           3           2           2\n\n\n\n\n4. Evaluate the reliability of the items in this variable.\nI‚Äôll examine the internal reliability of the scale. Internal reliability measures how consistent people‚Äôs responses were for each of the items of the scale. I‚Äôm hoping for a high value.\n\nlibrary(psych) # make sure you install.packages(\"psych\") first!\nalpha(extra.df)\n\n\nReliability analysis   \nCall: alpha(x = extra.df)\n\n  raw_alpha std.alpha G6(smc) average_r S/N  ase mean   sd median_r\n      0.87      0.87    0.88      0.52 6.6 0.03  3.1 0.88     0.49\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.80  0.87  0.92\nDuhachek  0.81  0.87  0.92\n\n Reliability if an item is dropped:\n            raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r\nec.e1            0.88      0.88    0.89      0.59 7.2    0.028 0.017  0.58\nec.e2            0.85      0.85    0.85      0.53 5.6    0.035 0.029  0.51\nec.e3            0.82      0.82    0.83      0.48 4.6    0.040 0.025  0.45\nX6...ec.e4r      0.85      0.85    0.86      0.53 5.7    0.035 0.032  0.51\nX6...ec.e5r      0.80      0.81    0.81      0.46 4.3    0.044 0.020  0.46\nX6...ec.e6r      0.86      0.86    0.86      0.55 6.1    0.032 0.020  0.53\n\n Item statistics \n             n raw.r std.r r.cor r.drop mean   sd\nec.e1       50  0.63  0.63  0.52   0.47  2.9 1.18\nec.e2       50  0.75  0.77  0.72   0.65  3.6 0.91\nec.e3       50  0.87  0.88  0.87   0.79  3.3 1.12\nX6...ec.e4r 49  0.77  0.76  0.69   0.64  3.3 1.22\nX6...ec.e5r 49  0.91  0.91  0.92   0.86  3.0 1.16\nX6...ec.e6r 50  0.73  0.72  0.67   0.59  2.7 1.21\n\nNon missing response frequency for each item\n               1    2    3    4    5 miss\nec.e1       0.14 0.26 0.28 0.24 0.08 0.00\nec.e2       0.04 0.06 0.30 0.50 0.10 0.00\nec.e3       0.06 0.20 0.30 0.30 0.14 0.00\nX6...ec.e4r 0.04 0.31 0.20 0.24 0.20 0.02\nX6...ec.e5r 0.08 0.27 0.35 0.16 0.14 0.02\nX6...ec.e6r 0.16 0.34 0.20 0.22 0.08 0.00\n\n\n\n\nThere‚Äôs a LOT going on in this code, but I‚Äôm looking at the number underneath raw_alpha, which shows me that this scale is reliable. This is good, and would be something that I would report when describing the measure of Extraversion.\nThe rest of the code output gives you other statistics on the scale (e.g., the mean or standard deviation), and shows you what the reliability of the scale would be if you removed one of the items from the scale. This can be useful for diagnosing whether there was an issue with your code (e.g., did you forget to reverse-score one item?) or with the scale (e.g., is one of your items actually measuring something other than extraversion?) In this case, the reliability doesn‚Äôt change much if we remove any of the items.\n\n\n5. Use the rowmeans() function to create a new variable that is the average of each person‚Äôs items.\nNow, I need to create one variable that is combines all the items into one number. To do this, I could either add up each person‚Äôs 6 extraversion scores, or take the average. The convention is usually to take the average for personality variables - not sure why‚Ä¶a cultural difference.\nSo we will use the rowMeans() function to do this. There are two methods for this.\n\nMethod #1 (Default - very conservative approach) : only calculate an average score if people answered every item in the dataset. This completely removes a person from the dataset, even if they answered 5/6 of the questions.\n\nthis calculates the average of these items for each row (individual) in the dataset. This is the measure of extraversion for each person. Notice that there is some missing data - the default for this code is that a person‚Äôs data will be totally removed if they didn‚Äôt answer all the questions in the scale. This is a very conservative way to handle missing data.\n\n    rowMeans(extra.df) \n\n [1] 2.166667 2.833333 2.333333 2.500000 5.000000 2.333333 3.333333 3.666667\n [9] 2.833333 2.333333 2.000000 3.500000 4.166667 2.333333 4.500000 3.166667\n[17]       NA 3.666667 3.000000 3.666667       NA 3.500000 3.666667 4.000000\n[25] 1.666667 4.833333 3.666667 3.833333 3.166667 3.000000 2.500000 2.833333\n[33] 1.000000 2.833333 3.333333 3.000000 3.166667 2.166667 2.333333 3.166667\n[41] 1.833333 3.000000 3.166667 4.333333 4.166667 3.500000 3.500000 2.166667\n[49] 4.833333 1.666667\n\n\n\nMethod #2 (more liberal approach) : this removes missing data from specific items, so the average score will be calculated even if the person didn‚Äôt answer some of the items.¬†\n\n\nrowMeans(extra.df, na.rm = T) #     adding na.rm = T as an argument. \n\n [1] 2.166667 2.833333 2.333333 2.500000 5.000000 2.333333 3.333333 3.666667\n [9] 2.833333 2.333333 2.000000 3.500000 4.166667 2.333333 4.500000 3.166667\n[17] 3.800000 3.666667 3.000000 3.666667 2.800000 3.500000 3.666667 4.000000\n[25] 1.666667 4.833333 3.666667 3.833333 3.166667 3.000000 2.500000 2.833333\n[33] 1.000000 2.833333 3.333333 3.000000 3.166667 2.166667 2.333333 3.166667\n[41] 1.833333 3.000000 3.166667 4.333333 4.166667 3.500000 3.500000 2.166667\n[49] 4.833333 1.666667\n\n\n\nIMPORTANT In order to save the output as a variable, you will need to assign the output of rowMeans() to a new object, ideally one that is saved as part of the original dataset.\n\n\nec$EXTRAVERSION &lt;- rowMeans(extra.df, na.rm = T) # this saves the scales to the dataset as a new object (which I'm calling EXTRAVERSION). I like to name variables that I create in ALL CAPS. Note that I've chosen to be less conservative in how I handle missing data.\n\n\n\n6. Graph this variable and interpret the graph (what do you learn)?\nOkay, we have a scale! And this scale should measure continuous variation! Let‚Äôs graph it. I‚Äôm looking for a graph that ranges from 1 to 5 (since that was the limit of my response scale), and something that looks mostly normally distributed.\n\nhist(ec$EXTRAVERSION, main = \"\", col = 'black', bor = 'white')\n\n\n\n\n\n\n\n\nThis looks good. When I look at this graph I see the following things:\n\nthe range of the scale goes from 1 to 5. This is good, because the response scale ranged from 1 to 5. An incorrect range would suggest that I made a mistake when analyzing the data.\nthe graph is mostly normally distributed. This is also good, and expected for a personality variable like Extraversion. If there was some extreme skew, I might again think that I made a mistake when creating the scale, or wonder if there was some non-random variable that was influencing students‚Äô extraversion scores (for example, the entire cohort of students took an improv class during orientation).\nthere is some slight positive / right skew - there are a few students who are maxed out on extraversion. What‚Äôs up extraverts [waves furiously]. Not sure why that is, but some deviation from normality is always expected! Ooh, one theory is that this survey was given out at the end of a three-hour lecture, so maybe students who were more extraverted were more likely to stick around in a large classroom setting, and thus more likely to be included in the survey. This is an example of sampling bias.",
    "crumbs": [
      "List of R Code",
      "Describing People",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Measures</span>"
    ]
  },
  {
    "objectID": "chapters/4R_Scales.html#i-like-to-watch-videos-creating-a-likert-scale",
    "href": "chapters/4R_Scales.html#i-like-to-watch-videos-creating-a-likert-scale",
    "title": "Measures",
    "section": "I Like To Watch Videos : Creating a Likert Scale",
    "text": "I Like To Watch Videos : Creating a Likert Scale\n\n\nHere‚Äôs a link to the Rscript I used in the video\nNOTE : skip / ignore the stuff on using the scale in a model - realized that is leftover from when I used to talk about likert scales after linear models and prof has not gotten a chance to re-record the video‚Ä¶we will talk about linear models in a a few weeks. Let me know if you have ideas about how to improve the video for when I do re-record!",
    "crumbs": [
      "List of R Code",
      "Describing People",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Measures</span>"
    ]
  },
  {
    "objectID": "chapters/4R_Scales.html#self-reports",
    "href": "chapters/4R_Scales.html#self-reports",
    "title": "Measures",
    "section": "Self-Reports",
    "text": "Self-Reports\nOne of the simplest ways to collect data on an individual is just to ask them what they are like, and have the person report on themselves (a self-report). There are two different approaches to getting self-reports - survey methods and qualitative interviews.\n\nQualitative Interviews\nOne way to get individuals to tell you what they are like is through a structured interview where researchers ask open-ended questions. One such example of this is the McAdams Life Narrative3. In this structured interview, a trained research assistant asks a set of broad questions to participants over the course of 1 to 3 hours. The research assistant is advised to ‚Äúfeel free to skip some of these questions if they seem redundant or irrelevant, and should follow up with other questions as needed ‚Äú but also to ‚Äúnot adopt an advisory or judgmental role, but should instead serve as an empathic and encouraging guide and an affirming sounding board.‚Äù\n3¬†McAdams talks about his work in this popular press interview and writes about it in this scientific journal review article.4¬†Here‚Äôs a link to the full narrative instructions if you want to do the whole thing; it‚Äôs a great way to know someone.Below is an excerpt from the first part of the interview - if you are comfortable, please share your chapters on the Chapter 4 Discord thread!4\n\n\n\nSurvey Methods\nQualitative interviews are not very common in psychological research, because they take a lot of time to conduct, and then more time to convert people‚Äôs open-ended responses into data (a form of behavioral coding, described in more detail below).\nInstead, the majority of self-reports come from surveys. Read about these below.\n\n\n\nDefinition\nA questionnaire where individuals answer specific questions about themselves on a structured rating scale.\n\n\nExample\n‚ÄúOn a scale from 1 (Strongly Disagree) to 5 (Strongly Agree), how satisfied with your life are you right now?‚Äù\n\n\nBenefits\n\nEasy to collect : It only takes a few minutes for people to answer a survey, and the data come in a clean and organized format that requires little effort to analyze. In Part 2 below, you‚Äôll learn how to clean and organize the results of a likert scale.\nSelf-Knowledge Validity : People know things about themselves, often this knowledge is based in ‚Äúreality‚Äù, and sometimes a self-report is the only way to get this knowledge. For example, only you know what was your happiest moment in life, and you probably have an accurate awareness about how anxious you are about the final project in this class.\n\n\n\nLimitations\n\nSelf-Enhancement / Self-Diminishment Bias : People are often motivated to either enhance or diminish their accomplishments when asked. For example, no student has ever come up to me at the end of the semester and told me, ‚ÄúProfessor, just so you know - I cheated on the exam.‚Äù Even though they may know they cheated, they don‚Äôt want to admit that because our society has norms or guidelines about when cheating is appropriate5. Other times, a person may not want to highlight their accomplishments (self-diminishment) because they don‚Äôt want to seem like they are bragging.\nSelf-Insight Bias : Sometimes, people also really don‚Äôt know what they are like. A person may not know, for example, whether they snore when they sleep (a behavior), how anxious they really are in a situation (an affect), or their patterns of unconscious bias (a cognition).\n\n\n\n\n5¬†Indeed, our society decides what forms of cheating are acceptable.Self-reports have a bad reputation in psychology, particularly because of the ability for people to engage in self-enhancement/diminishment or self-insight bias. However, they are a very powerful, and very commonly used method of assessment, even for studies where researchers are able to observe other types of data.¬†\nFor example, despite all the behavioral data that powerful technology companies like Facebook/Instagram/Meta, TikTok, Twitter/X, etc. collect, you‚Äôve probably seen them ask you to answer some survey. They care about you6, and know that asking you questions about yourself is an important way to show that level of care.\n6¬†‚Ä¶and your clicking on advertisements; hey, it‚Äôs hard to distinguish the two really‚Ä¶Below is one example from some survey when I used to be on Facebook. If they changed their graphic design since I was last on, it‚Äôs probably because of some survey feedback they received.",
    "crumbs": [
      "List of R Code",
      "Describing People",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Measures</span>"
    ]
  },
  {
    "objectID": "chapters/4R_Scales.html#observations",
    "href": "chapters/4R_Scales.html#observations",
    "title": "Measures",
    "section": "Observations",
    "text": "Observations\nOften, self-reports are insufficient to capture what a person is like, or researchers are studying individuals who cannot give self-reports, such as infants, people with disabilities, or non-human animals.7\n7¬†If I was a billionaire, I‚Äôd fund a team of psychologists to train monkeys to answer surveys. this is maybe why I am not a billionaire. that and the whole ‚Äúintergenerational wealth‚Äù thing / chosen teaching career / lack of a desire to crush others and extract as much wealth from them‚Ä¶hard to know which factor is at play. Life is complex! Let me know if you are a billionaire and want to fund some other ideas / subscribe to my newsletter.\n\nRead about some common forms of observation methods below.\n\n\n\nDefinition\nObservational methods refer to ways in which another individual generates data on the target person of interest.\n\n\nExamples\nInformant Reports. Informant reports are a special form of surveys, where researchers ask friends, family, or strangers to answer survey questions about another person. This is technically observational data, since the people answering the surveys are basing their judgments on their observations of the individual.\n |\nBehavioral Data. When the variables of interest are physical, then researchers can use measurement tools to directly observe the behavior. For example, researchers wanting to measure stress might measure cortisol by taking samples of saliva from the cheek; researchers wanting to understand the brain look to voxel activation with fMRI, or cortical neuron activation with EEG.\n |\nBehavioral Coding. Sometimes, it‚Äôs easier to have research assistants observe the physical behaviors of interest. For example, y‚Äôall served as behavioral coders when you counted the number of interruptions (a behavior!) Other times, research assistants will observe real-life interactions and observe variables such as time spent talking, distance between participants, or provide ratings of how much emotion or anxiety the person seemed to be expressing (using a rating scale). The ‚Äústrange situation‚Äù task (where a parent leaves the room and researchers observe what a child does) is another example.8\n\n\n\nBenefits\n\n‚ÄúExternal‚Äù. Researchers like observational methods because they, by definition, require some outward expression of the underlying psychological processes. Professor could go on a rant here and won‚Äôt, but psychology has been increasingly fixated on behavior since Watson kicked off the behaviorism movement in the 1920s, and these data are often prioritized, since our field loves predicting people‚Äôs actual behavior (so they can exert power over it).\nLess influenced by self-report biases. Observers are considered to be less biased than individuals, who may be particularly prone to self-enhancement, self-diminishment. And observers may be able to fill in the gaps left behind by self-insight bias. For example, a person‚Äôs close friends may know more about the individual‚Äôs reputation than they themselves do.\nMore reliable (multiple sources). While there is only one self to provide a self-report, there can be multiple observers. Indeed, observational methods almost always leverage this power and require multiple swabs of spit to get a reliable estimate of cortisol, or have multiple research assistants rate the same behavior to check and make sure they are each getting a similar answer.\n\n\n\nLimitations\n\nTime-Intensive. It can be incredibly costly to collect observational data. Yale charges over $600/hour for use of their fMRI machine (Berkeley doesn‚Äôt list their prices), and conducting a study to measure a naturally occurring behavior could take years between designing the study, recruiting and training the research assistants to observe the behavior, taking the time to collect the data, and then doing the behavioral coding necessary to convert the observations into numbers. Whew. Much easier to just give someone a survey and have them complete it.\nThe Hawthorne Effect. The act of observation can often change behavior. Couples may be less likely to fight if they know they are being monitored by a psychologist, and even light changes its behavior depending on the way it is observed. Researchers can take steps to address this change in behavior - and sometimes the power of the phenomenon is so large that it doesn‚Äôt matter if a researcher is there. In couples studies, for example, researchers can get couples to fight by having each person write out a list of things that cause conflict in the relationship, and then having the people trade lists and talk about it. Often however, individuals habituate to the presence of observation - do you think about the fact that your online interactions are being harvested by tech companies every day, or just kinda get used to it?\nNot all aspects of psychology are observable. A person‚Äôs subjective experience matters, and sometimes a self-report is the only way you can get this data. If you look like you are smiling and have the neurotransmitter levels that suggest you are happy, but feel miserable, are you happy?\n\n\n\n\n8¬†You can compare this child‚Äôs reaction to this child‚Äôs reaction. What do you observe? How would you quantify these observations and turn them into cold, hard data?\n\nTLDR : there are lots of different methods to measure what individuals are like, and for your final project you probably will want to give people self-report surveys to keep things simple for this very first study!",
    "crumbs": [
      "List of R Code",
      "Describing People",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Measures</span>"
    ]
  },
  {
    "objectID": "chapters/5R_GoodScience.html",
    "href": "chapters/5R_GoodScience.html",
    "title": "‚ÄúNormal‚Äù Data",
    "section": "",
    "text": "Part 1. The ‚ÄúNormal‚Äù Distribution.\nThe image above depicts Grounded in the Stars, a 12-foot sculpture of a young woman that stood in New York City‚Äôs Times Square from April 29 through June 14, 2025.\nThere‚Äôs a lot that I love about this image, from the artistry of rendering braids and clothes so naturally in sculpted bronze, to the way the character stares off in the distance as if unconcerned with the gaze of others, to the way the sculpture proudly claims space in a society that does not currently and historically create inclusive spaces for black women. Indeed, the sculpture is surrounded by other massive images1 whose presence in the space may go unnoticed because it is so routine to see images of white men in public places.\nThe artist‚Äôs website emphasizes the way the sculpture challenges pre-existing ideas about who is represented in our society, and references Michelangelo‚Äôs David as inspiration, but clarifies that this sculpture is ‚ÄúA fictionalized character constructed from images, observations, and open calls spanning between Los Angeles and London‚Äù who ‚Äúcarries familiar qualities, from her stance and countenance to her everyday clothing.‚Äù\nThis statue, and the fact that it was an amalgamation of real people, also reminded me of the sculptures of ‚ÄúNormmann‚Äù and Norma.\nLike the unnamed woman in Times Square who was the subject of Grounded in the Stars, Normmann and Norma‚Äôs measurements were based on composites of other people; in Norma‚Äôs case ‚Äúthe averaged measurements of ‚Äònative white‚Äô American women recorded and standardized by the Bureau of Home Economics in 1940, in an attempt to devise the first standardized system of sizing for ready-made clothes.‚Äù2\nUnlike the subject of Grounded in the Stars, Norma and Normmann were based on different demographic data. More important, these characters were labeled and advertised as ‚ÄúThe Typical American‚Äù. A type of person who reflected and reinforced the values that white America wanted to promote.\nIndeed, these statues achieved their purpose in various ways; Norma and Normman were first displayed in the American Museum of Natural History during the International Congresses of Eugenics in 1945. Furthermore, when the statues were gifted to the Cleveland Health Museum, the museum held a contest to find the woman who most typically represented Norma3, offering prize money in ‚Äúa search for Norma, the typical American woman‚Äù. While a winner was selected, the search failed in the original efforts to find someone who perfectly matched this average, as the measurements of the winner ‚Äúdid not coincide with those of the statue. . . . after assessment of the measurements of 3,863 women who entered the search, Norma remained a hypothetical individual.‚Äù4\nHere, we see a few course themes that we will discuss in this chapter, and in this week‚Äôs lecture :\nBefore we get into the critique in our class, let‚Äôs use this space to go over some of the basic definitions.",
    "crumbs": [
      "List of R Code",
      "Describing People",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>\"Normal\" Data</span>"
    ]
  },
  {
    "objectID": "chapters/5R_GoodScience.html#definition.",
    "href": "chapters/5R_GoodScience.html#definition.",
    "title": "‚ÄúNormal‚Äù Data",
    "section": "Definition.",
    "text": "Definition.\nThe ‚ÄúNormal‚Äù (or Gaussian) distribution is a common shape for what distributions of data look like. The shape is often called a ‚Äúbell curve‚Äù, because it looks like the curve of a bell. Ding.\nWhile many distributions appear normal, the ‚ÄúNormal Distribution‚Äù ‚Ñ¢ refers to a distribution that describes an expected probability of a range of scores.\n\n\n\n\n\nThe Normal Distribution is called ‚Äúnormal‚Äù, in part, because researchers expect to see this type of distribution for variables where two conditions are met:\n\nThere are multiple explanations for why the variation occurs. This happens very frequently across all science, since life is complex and there are many reasons why individuals differ.\nThese multiple explanations occur independently. ‚ÄúIndependence‚Äù means that one explanation for variation is unrelated to the presence of another explanation for variation. For example, your height is somewhat influenced by your genetics (you are likely similar height to your family members) and your diet (i.e., whether you got enough nutrients in childhood or not), but your diet is likely unrelated to your genetics. Non-independence might mean that there is some shared experience among individuals that is influencing their variation in predictable ways. For example, children who were exposed to all the horrors of war experienced multiple factors that negatively impacted their height - malnutrition, sleep disturbances,\n\nIndeed, many of the variables that psychologists measure do appear ‚ÄúNormal‚Äù. For example, let‚Äôs look at one example ‚Äúnormal‚Äù distributions - the ‚Äúself-esteem‚Äù variable we saw in Lecture 2. Note that it‚Äôs not perfectly normal5, but it‚Äôs pretty close and representative of the kind of ‚Äúreal world‚Äù data that you might encounter.\n5¬†good not to hold data to unrealistic body images as well as people.\nhist(d$SELFES, col = 'black', bor = 'white', \n     main = \"Histogram of Self-Esteem\", \n     xlab = \"Self-Esteem Score\", breaks = 15)\n\n\n\n\n\n\n\n\n‚ÄúIndependent‚Äù explanations for why variation in self-esteem occurs. Self-esteem is complex, and can be influenced by variables such as: genetics, parental environment, home environment, income, neurotransmitters, whether your crush told you they like you too the day you took the self-esteem survey, etc. These variables are considered independent because one does not influence the other, and they differ across participants in the study. That is, someone who had a happy parental environment may not necessarily have a high income, rich people have their crushes ignore them too, etc.\n‚ÄúNon-Random‚Äù Explanations. Careful observers will note that self-esteem appears slightly shifted above the mid-point of the scale (which goes from 1-4, so 2.5 would be the mid-point), and that there‚Äôs some slight negative skew (meaning more individuals are on the higher end of the distribution). It‚Äôs unclear why there‚Äôs this shift in the data, but below are a few possible reasons :\n\nThere‚Äôs some shared cultural experiences among participants - our society values self-esteem, and people might be biased to self-enhance / self-present a higher self-esteem. This is a non-independent influence, since many participants might experience this in our culture that puts pressure on people to have a high self-esteem.\nThe sample was biased to include people with higher levels of self-esteem, or maybe people with lower levels of self-esteem were less likely to complete the survey (which hurts their self-esteem because they haven‚Äôt yet fully internalized that it‚Äôs okay to be imperfect in a society that demands perfection.) In any case\nThe survey was administered on a day when everyone in the world had a really good hair day, so there‚Äôs a non-independent factor that‚Äôs shifting many people‚Äôs self-esteem up.\n\n\nExample 2 : Thinking About Distributions\nBelow is another example of a distribution - this one is not random, but is skewed (pop quiz : is it positively or negatively skewed? See here for answer6.)\n6¬†It is negatively skewed, since the tail is on the negative side of the distribution.\n\n\n\n\n\n\n\n\n\n\nTip‚ÄúRandom‚Äù Reasons for Variation\n\n\n\n\n\nLots of things can influence a student‚Äôs score on an exam, such as how much students were motivated to study, how much time they had to study, what was going on in their lives, whether they had a study buddy in the class, whether they were sick or not on the day of the exam, their ‚Äútest taking‚Äù skills and strategies and anxiety levels, etc.\n\n\n\n\n\n\n\n\n\nTip‚ÄúNon-Random‚Äù Reasons for Variation.\n\n\n\n\n\nThese data are not normally distributed because the students were all part of the same college, in the same classroom, taught by the same professor, at the same time. The professor did his best to prepare these students, and wrote a test that would be based on the kinds of practice they had gone over in lecture. These variables are considered ‚Äúnon random‚Äù because they were shared by all students. The data are skewed because these non-random shared experiences helped students do well on the exam.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarningCulture in Statistics : The Normal Curve\n\n\n\nThat‚Äôs right folks, it‚Äôs time for another chat with your friend Open-Source Mickey Mouse! This time, we‚Äôre gonna chat about the idea that people differ from some average. The idea that you could quantify people as ‚Äúaverage‚Äù is fairly new - it‚Äôs hard to pinpoint exactly, but a scientist named Quetelet first extended the statistical methods derived from astronomy to be applied to humans in the 1860s7. Quetelet thought the average was an ideal state since it reflected the center of all possible individuals. However, a few years later Francis Galton used Quetelet‚Äôs ideas to try and ‚Äúrank‚Äù individuals according to some hierarchy of excellence. For Galton, the average was not an ideal state, but rather something to overcome in a desire for greater and greater excellence. Galton was also a racist and father of the eugenics movement, who used distorted statistics as a tool to justify his own pre-existing racist beliefs that white people were superior to everyone else. Yuck!\nI think it‚Äôs worth the time and space to hear this from Galton himself.\nTo conclude, the range of mental power between‚ÄîI will not say the highest Caucasian and the lowest savage‚Äîbut between the greatest and least of English intellects, is enormous. ‚Ä¶ I propose in this chapter to range men according to their natural abilities, putting them into classes separated by equal degrees of merit, and to show the relative number of individuals included in the several classes‚Ä¶..The method I shall employ for discovering all this, is an application of the very curious theoretical law of ‚Äúdeviation from an average.‚Äù First, I will explain the law, and then I will show that the production of natural intellectual gifts comes justly within its scope. - Galton, Hereditary Genius (1869). Linked here.\nThe point of bringing up some of the racist origins of statistics is two-fold :\n\nMany people like to argue that ‚Äúgood‚Äù statistics is objective, and that scientists are free of bias. So it‚Äôs important to acknowledge that the inventors of many statistics that we use had clear biases and did not behave objectively. Rather than try to be free of bias, I think it‚Äôs important to acknowledge where and when people are biased, and address those biases up explicitly.\nStatistics is a tool, and that tool can be used poorly and dangerously in ways that can cause a lot of pain. Just because something has numbers doens‚Äôt mean it‚Äôs correct. For example, Galton‚Äôs work showing ‚Äústatistically‚Äù that white people were superior in ability to other groups didn‚Äôt account for other factors like socioeconomic status. And for what it‚Äôs worth, modern scientists overwhelmingly agree that ‚Äúrace‚Äù is a social construct. There are genetic differences that explain different skin tones, however there is more genetic variation in skin tone within a similar ‚Äúrace‚Äù than between different ‚Äúraces‚Äù. Read more about this here.\n\nAlright, that‚Äôs all for now! Let me know what you think and see you next time!\n\n\n7¬†see Rose‚Äôs END OF AVERAGE (2016) or, for a more critical approach, Chapman‚Äôs (2023) EMPIRE OF NORMALITY.",
    "crumbs": [
      "List of R Code",
      "Describing People",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>\"Normal\" Data</span>"
    ]
  },
  {
    "objectID": "chapters/5R_GoodScience.html#why-this-matters.",
    "href": "chapters/5R_GoodScience.html#why-this-matters.",
    "title": "‚ÄúNormal‚Äù Data",
    "section": "Why This Matters.",
    "text": "Why This Matters.\nThe normal distribution is foundational to the statistics we will learn in this class. (And in an advanced statistics class, you‚Äôll learn how to adapt techniques if you want to understand non-normal distributions.)\nThis does not mean that every variable needs to be normally distributed; we‚Äôll work with skewed variables and categorical variables (which have their own distributions) in lots of ways. However, the ‚Äúnormal distribution‚Äù is an important reference point that we can use to evaluate variables. For example, exam scores are negatively skewed because they differ from the normal distribution.",
    "crumbs": [
      "List of R Code",
      "Describing People",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>\"Normal\" Data</span>"
    ]
  },
  {
    "objectID": "chapters/5R_GoodScience.html#some-conceptual-understanding",
    "href": "chapters/5R_GoodScience.html#some-conceptual-understanding",
    "title": "‚ÄúNormal‚Äù Data",
    "section": "Some Conceptual Understanding",
    "text": "Some Conceptual Understanding\n\nZ-Score : Definition\nThe z-score is a linear transformation8 that calculates the distance of an individual score (\\({y_i}\\)) from the mean (\\(\\hat{y}\\)) in units of standard deviation \\({\\sigma_y}\\).\n8¬†A linear transformation means that the order and spacing of the data are unchanged - the person who has the highest self-esteem when measured on a 0 to 4 scale will still have the highest z-score, and be the same distance away from the next highest individual.\\[\n\\huge z = \\frac{y_i - \\hat{y}}{\\sigma_y}\n\\]\nEach individual score in the dataset can be z-scored, and this statistic tells you how far above or below the individual is from others (the mean), relative to the average difference from the means (which is what the standard deviation describes.)\nZ-scoring changes the units of the variable from the original unit of measurement, to units of standard deviation.\n\nA z-score of 1 means that the individual is 1 standard deviation above the mean (about as different as average).\nA z-score of 0 means that the individual is EXACTLY the mean.\nA z-score of -4.112 means that the individual is over 4 standard deviations below the mean; which is VERY below average.\nA z-score of .01 means the person is just a tiiiiny bit above the mean.\n\n\n\nZ-Score : in R\nYou can see first few ‚ÄúRaw‚Äù and ‚ÄúZ-Scores‚Äù from the self-esteem dataset below.\n\n\n  Raw.Scores   Z.Scores\n1        3.0  0.5300156\n2        3.3  0.9593493\n3        2.4 -0.3286517\n4        2.7  0.1006820\n5        4.0  1.9611279\n6        3.6  1.3886830\n\n\nThere are two ways of calculating the z-scores:\n\nUsing the scale() function. This function calculates the mean and standard deviation of an object, and then uses these statistics to calculate the z-score for the data.\n\nd$SELFES_Z &lt;- scale(d$SELFES)\nd$SELFES_Z[1:5]\n\n[1]  0.5300156  0.9593493 -0.3286517  0.1006820  1.9611279\n\n\n\nNote that the [1:5] index is not needed for z-scoring, and is just helping me only show the first five elements.\n\nYou can also manually calculate a z-score. But don‚Äôt do this by hand. I mean you could, but you could also do this with a pencil and that‚Äôs not needed anymore.\n\nzSELF &lt;- (d$SELFES - mean(d$SELFES, na.rm = T)) / # distance from the mean, divided by....\n          sd(d$SELFES, na.rm = T) # the standard deviation\nzSELF[1:5] # just showing the first 10 z-scored variables.\n\n[1]  0.5300156  0.9593493 -0.3286517  0.1006820  1.9611279\n\n\n\n\n\nZ-Score : Who Cares?\nA z-score can be useful for two reasons :\n\nIt removes the units of measurement. Many variables in psychology are measured with likert scales, or other made up numbers that don‚Äôt have a shared meaning, but are instead meant to rank or sort individuals along some spectrum. For example, I don‚Äôt really know what to do with the knowledge that a person 3 in the dataset has a self-esteem score of 2.4 . But knowing that this person‚Äôs z-score is -0.3286517 tells me that their self-esteem is lower than average, but only a little lower than the average.\nIt emphasizes the individual‚Äôs rank within the distribution. If you know a job at Amazon dot com is offering 5187 BEZOS BUCKS, you may not know whether this is a high or low paying job among other jobs at Amazon dot com. But if you know the salary of this job has a z-score of .34, you know that this job is higher than average, but only a little higher.",
    "crumbs": [
      "List of R Code",
      "Describing People",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>\"Normal\" Data</span>"
    ]
  },
  {
    "objectID": "chapters/5R_GoodScience.html#example-using-z-scores-to-make-decisions-about-outliers",
    "href": "chapters/5R_GoodScience.html#example-using-z-scores-to-make-decisions-about-outliers",
    "title": "‚ÄúNormal‚Äù Data",
    "section": "Example : Using Z-Scores to Make Decisions About Outliers",
    "text": "Example : Using Z-Scores to Make Decisions About Outliers\nIf we look at the variable age from the same self-esteem dataset, we see some issues.\n\nhist(d$age)\n\n\n\n\n\n\n\n\nThis looks wrong. R says this is a histogram, but it doesn‚Äôt look like the histograms we‚Äôve seen before for a few reasons :\n\nWhy does the x-axis go so far to the right? There are extreme outliers in the variable age - most likley due to errors in data entry - that are way out to the right on the x-axis.\nWhat are those numbers on the x-axis!? These outliers are so extreme, that R is reporting the values of age using exponents. 5e+08 means 5 * 10^8 = 5 * 10 * 10 * 10 * 10 * 10 * 10 * 10 * 10 = 500,000,000 = 500 million. Whew!\nBut where are these outliers on the graph? It doesn‚Äôt look like there‚Äôs anything there? The dataset is so large (tens of thousands of people appear to have ages that are closer to zero years old than 500 million years old) that the individual outliers are not registering on the graph. But they exist, and we can use R to find them.\n\n\nd$age[d$age &gt; 100 & !is.na(d$age)]\n\n [1]       1975       1993        229        366       1354      90210\n [7]      80230        120        590        442        234        258\n[13]        333        980       1000      45678        333        156\n[19]        134        152        474        414        334        630\n[25]        169       1997 2147483647        123       7300        118\n[31]        972       9000     100000        662        117\n\n\nIn this code, I‚Äôm using indexing [] to ask R to find two things :\n\nd$age &gt; 120 asks R to find the rows from the dataset d that contain ages that are greater than 100 years old.\n!is.na(d$age) asks R to find the rows where age is NOT and NA values (the ! when used in coding means ‚Äúnot‚Äù). You don‚Äôt strictly need this code, but it made the output easier to read by removing all the NA values that I don‚Äôt care about right now.\nthe & sign in between these commands asks R to find individuals where both conditions are met. You can also use the | bar9 to ask R to find individuals where EITHER condition is met.\n\n9¬†the bar can be typed by hitting shift + the key above your return or enter key on the keyboard.I see that some of the ages were entered in as the year of birth; others look like maybe errors in data entry or jokes (e.g., 90210) and someone entered in an extreme value of 2147483647 that is contributing to our strange graph, and throwing off the mean (but not the median, since it, as we all remember from Chapter 3, is less sensitive to outliers.)\n\nmean(d$age, na.rm = T)\n\n[1] 45333.77\n\nmedian(d$age, na.rm = T)\n\n[1] 22\n\n\n\nSide Quest : Removing Outliers Without Z-Scoring\nSo let‚Äôs remove these outliers, and check to see that R did this correctly.\n\nd$age[d$age &gt; 100 & !is.na(d$age)] # finds the outliers\n\n [1]       1975       1993        229        366       1354      90210\n [7]      80230        120        590        442        234        258\n[13]        333        980       1000      45678        333        156\n[19]        134        152        474        414        334        630\n[25]        169       1997 2147483647        123       7300        118\n[31]        972       9000     100000        662        117\n\nd$age[d$age &gt; 100 & !is.na(d$age)] &lt;- NA # replaces the outliers with NA\nhist(d$age) # my graph.\n\n\n\n\n\n\n\n\nThis looks better, but now I see that there are some suspiciously young ages.\nSo I‚Äôll adapt my code to look for individuals who are less than 18 years old.\n\nmin(d$age, na.rm = T)\n\n[1] 1\n\nyoungfolk &lt;- d$age[d$age &lt; 18 & !is.na(d$age)] \nyoungfolk[1:100] # just the first 100 folks less than 18\n\n  [1] 15 16 14 15 16 14 16 13 17 12 14 14 14 17 16 16 16 13 17 16 17 16 16 17 17\n [26] 15 17 16 13 14 15  1 17 17 17 17 16 12 17 17  6 12 15 17 17 14 16 16 17 11\n [51] 14 14 11 17 17 16  4 14 15 17 16 16 15 16 15 15 16 17 14 17 16 17 14 15 15\n [76] 17 14 15 14 17 16 15 16 16 16 17 17 14 16 16 17 17 16 15 16 16 16 16 16 17\n\n\nI see a lot of teenagers, and some young kids, and even an individual who is reporting their age as 1. And while it could be interesting to look at the narcissism levels of teenagers (and Freud wrote a brilliant if slightly deranged chapter on baby narcissism in Civilization and Its Discontents), I‚Äôm going to avoid any CPHS violations (which treats minors as vulnerable populations) and invoking the wrath of the THE YOUTH who ARE OUR FUTURE, and go ahead and remove these people from the dataset too.\n\nd$age[d$age &lt; 18 & !is.na(d$age)] &lt;- NA\n\nAnd now I have much clearer view of the variable age, and a more representative mean and standard deviation of age.\n\nhist(d$age, xlim = c(15,100), breaks = 30, main = \"Histogram of Age (Outliers Removed)\", xlab = \"Age (in years)\")\n\n\n\n\n\n\n\nmean(d$age, na.rm = T)\n\n[1] 30.07251\n\nsd(d$age, na.rm = T)\n\n[1] 12.25539\n\n\n\nOkay, back to using z-scoring.\nLooking at the graph, I‚Äôm not sure that 100 was the best cut-off for age. There are only a few people between the ages of 80-100, and I‚Äôm wondering whether they should actually be included in the dataset.\n\nd$age[d$age &gt; 80 & !is.na(d$age)]\n\n [1]  85  99  92 100  95  83  85  85  89  92  81  99  87 100  82  81  85  98  90\n[20]  90  99  86  84  84  82\n\n\nHere‚Äôs where a z-score could be useful. Are these elders radically different from our distribution? Let‚Äôs see how far they are away from the mean of age in units of standard deviation!10\n10¬†In the code below, I‚Äôm calculating the z-score of age, and defining this as ageZ. But I‚Äôm using the original variable - age - to define my rules inside of the bracket.\nd$ageZ &lt;- scale(d$age)\nd$ageZ[d$age &gt; 80 & !is.na(d$age)]\n\n [1] 4.481906 5.624261 5.053084 5.705858 5.297874 4.318713 4.481906 4.481906\n [9] 4.808293 5.053084 4.155519 5.624261 4.645100 5.705858 4.237116 4.155519\n[17] 4.481906 5.542664 4.889890 4.889890 5.624261 4.563503 4.400309 4.400309\n[25] 4.237116\n\n\nI see that all of these ages are very more than four standard deviations away from the average - this is very different from average. In a ‚Äúnormal‚Äù distribution, the vast majority of other data would be expected to fall below these z-scores. I can look this up with the pnorm() function, which calculates the probability of a score falling below a given z-score (what‚Äôs called the ‚Äúlower tail‚Äù of the distribution. You can also use this function to find the upper tail by changing one of the arguments.)\n\npnorm(d$ageZ[d$age &gt; 80 & !is.na(d$age)]) * 100\n\n [1]  99.99963 100.00000  99.99998 100.00000  99.99999  99.99922  99.99963\n [8]  99.99963  99.99992  99.99998  99.99838 100.00000  99.99983 100.00000\n[15]  99.99887  99.99838  99.99963 100.00000  99.99995  99.99995 100.00000\n[22]  99.99975  99.99946  99.99946  99.99887\n\npnorm(d$ageZ[d$age &gt; 80 & !is.na(d$age)])\n\n [1] 0.9999963 1.0000000 0.9999998 1.0000000 0.9999999 0.9999922 0.9999963\n [8] 0.9999963 0.9999992 0.9999998 0.9999838 1.0000000 0.9999983 1.0000000\n[15] 0.9999887 0.9999838 0.9999963 1.0000000 0.9999995 0.9999995 1.0000000\n[22] 0.9999975 0.9999946 0.9999946 0.9999887\n\n\nSo, if 99.99963% of other scores are expected to fall below an age that is 4.48 standard deviations above the mean (which is the z-score for an 85 year old person in this dataset), then it seems like the 85 year old person is radically different from the rest, and could be excluded.\nSome people like to define ‚Äúrules‚Äù for excluding data; things like 3x the standard deviation, or 4x the standard deviation. I‚Äôm not a huge fan of these rules, since I think they ignore important context for variables, and are hard to always apply (for example, the teenagers in the dataset are within 3 standard deviations, but I still think they could and should be excluded.) It‚Äôs best to try and see what the standards in a particular field are, try to make decisions about outlier removal before you collect the data (something called pre-registration, which we will learn about later), document these changes (in your R code), and then be prepared to make different decisions when someone reviewing your research (like an advisor or journal editor) asks you to do something different.",
    "crumbs": [
      "List of R Code",
      "Describing People",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>\"Normal\" Data</span>"
    ]
  },
  {
    "objectID": "chapters/5R_GoodScience.html#video-example-war-data",
    "href": "chapters/5R_GoodScience.html#video-example-war-data",
    "title": "‚ÄúNormal‚Äù Data",
    "section": "Video Example : War Data",
    "text": "Video Example : War Data\nIn the video below, I introduce the dataset, do some data cleaning, and discuss the mean and median.\n\nIn the video below, I work through calculating the standard deviation, why the outliers are causing problems with the descriptive statistics, and then professor goes on a little tangent about a few different methods on how to address these problems. Note : we won‚Äôt really cover log-transformations in this chapter (yet! maybe I‚Äôll add it at some point but there‚Äôs already a lot going on.) But let me know if you have questions and I can try to explain point y‚Äôall to some other resources.\n\nHere‚Äôs a link to the RScript I use in the videos.\n\nAnother Video : Z-Scores\nHere‚Äôs a video I recorded a few years ago on z-scores. Plan to re-record something, so let me know what you like about this video, and / or what other videos on this topic (or any topic in the class) you would like to see.",
    "crumbs": [
      "List of R Code",
      "Describing People",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>\"Normal\" Data</span>"
    ]
  },
  {
    "objectID": "chapters/5R_GoodScience.html#good-data-validity-and-reliability",
    "href": "chapters/5R_GoodScience.html#good-data-validity-and-reliability",
    "title": "‚ÄúNormal‚Äù Data",
    "section": "Good Data : Validity and Reliability",
    "text": "Good Data : Validity and Reliability\nScientists evaluate the quality of their measures in terms of validity (‚Äútruth‚Äù or accuracy) and reliability (repeatability or precision). Learn more about these two concepts - and their specific forms - in the two videos below.\n\nSpecific Forms of Validity\n\nFace Validity: asks us to evaluate whether our measure or result look like what it should look like. This is a superficial (and somewhat subjective) judgment. But it is often a powerful and quick way to assess. For example, if I measure my height and it tells me 100 feet, I know something is wrong because there is no way I‚Äôm that tall. Or, when looking at a self-esteem measure, I would want to see items that look like self-esteem questions (‚ÄúI feel good about myself‚Äù.) If the self-esteem measure had other questions in it that didn‚Äôt really seem like they were measuring self-esteem (‚ÄúI like to look at myself in the mirror‚Äù) I would have questions about the face-validity of the measure. This seems super obvious, but it‚Äôs an important check - do the measures used actually look like what they should?\nConvergent validity: asks us to evaluate whether our measure similar to related concepts. When two things converge, they come together, and we want our measure to be similar to things that it should be similar to. For example, a measure of body height should be related to a measure of shoe size or tibia length. A measure of self-esteem should be similar to a measure of self-efficacy or satisfaction with life, since both are about how the person is subjectively seeing themselves. They shouldn‚Äôt be exactly the same thing, but we‚Äôd expect to see a pattern in the data. (We‚Äôll talk more about how to quantify these patterns when we learn more about linear models.)\nDiscriminant validity: asks about whether our measure is different from unrelated concepts. When two things diverge, they are different from one another. And we WANT our measure to be different from things that we expect them to be different from. For example, a measure of height should be different from a measure of reading speed or how organized a person is. We would expect self-esteem to be different from how social a person is (though maybe there‚Äôs some relationship since our society values sociability, and people who are social might get more positive messages from others, bolstering their self-esteem.) This is the hardest concept for students to get, but it‚Äôs a really important test of the validity of a measure. I not only want my measure to be related to concepts it should be related to, but also different from concepts it should be different from.\n\n\n\nReliability\n\nTest-retest reliability: asks us to evaluate whether we get the same result if we take multiple measures separated by time. If I think of self-esteem as a stable trait, I should expect to see some similarity in a person‚Äôs self-esteem at one time point and then the next day. Of course, there will be some change - self-esteem (and other personality variables) can be influenced by the situation and environment. But they shouldn‚Äôt be radically different if we have a good measure of this core aspect of the self.\nInter-rater / Inter-judge reliability: asks us to evalute whether multiple observers (or tools) make similar measurements. If I have two rulers made by the same company, I would expect them to give me similar answers for how tall I am. Similarly, two different observers who are reliable should make similar jugments about a person‚Äôs self-esteem, or the number of interruptions they count. If our measure is not reliable, then we might get different answers from the different people (or tools) making the measurement.\nInter-item reliability: When we learned about likert scales, we learned about Cronbach‚Äôs Alpha - a method of assessing how much the different items in a likert scale were related to each other. This is a form of reliability - specific only to likert scales where we have different questions that are all measuring the same thing. If the scale is reliable, we expect to get similar answers across the different items. For example, someone who says ‚ÄúI feel good about myself‚Äù should also say ‚ÄúI feel that I have a number of good qualities.‚Äù\n\n\n\nExample : Validity and Reliability\nThink about a scale. How would you evaluate the validity (face, convergent, discriminant) and reliability (test-retest / interjudge) of a bathroom scale? Think about this on your own, then look over the video key / guide below.\n\n\n\n\n\n\nTipExpand To See Answers\n\n\n\n\n\nWatch the video below to go over some possible answers, or just look over the table.\n\n\n\n\n\n\n\n\nface : does our measure or result look like what it should look like?\nhigh : I have a sense of what my weight should be (e.g, if it says 10 or 1000 i know either the units are wrong or scale is broken.)\n\n\nconvergent : is our measure similar to related concepts?\nhigh : my weight according to the scale is (somewhat) related to how much I stress eat, how little I exercise, my parents‚Äô weight, etc.\n\n\ndiscriminant : is our measure different from unrelated concepts?\nhigh : my weight is unrelated to intelligence, how much I love R, whether I wear sandals with or without socks, etc.\n\n\ntest-retest : do we get the same result if we take multiple measures?\nhigh : If I step on the scale and get a number, I should be able to step off the scale, step on again, and get the same number.\n\n\ninterrater reliability : would another observer make the same measurements?\nhigh : a different scale (same model and technology) should give me the same result as my scale.\n\n\ninter-item reliability : would one item in the likert scale be related to others?\nnot relevant. a bathroom scale is not a likert scale.",
    "crumbs": [
      "List of R Code",
      "Describing People",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>\"Normal\" Data</span>"
    ]
  },
  {
    "objectID": "chapters/5R_GoodScience.html#bad-data-phrenology-in-terms-of-validity-and-reliability",
    "href": "chapters/5R_GoodScience.html#bad-data-phrenology-in-terms-of-validity-and-reliability",
    "title": "‚ÄúNormal‚Äù Data",
    "section": "Bad Data : Phrenology in terms of Validity and Reliability",
    "text": "Bad Data : Phrenology in terms of Validity and Reliability\nWatch the video below to review these terms, in the context of phrenology - an example of scientific racism.\n\n\n\n\n\n\nPhrenology is no longer considered a valid or reliable science, yet its presence still lingers in psychology, and is often taught as history without reference to its racist origins and consequences11. And as we have (and will continue to discuss) there are still many ways in which racism (and sexism, classism, and ableism) occur and affect psychology (and other sciences too).\n11¬†For examples, in the common Intro Psych textbook written by Myers & DeWall (2018)For example, more modern intelligence testing is often criticized for prioritizing White European values and language in the way it assesses supposedly ‚Äúobjective‚Äù knowledge. In an important test of this claim, the psychologist Robert Williams (pictured to the right) designed an IQ test that was as reliable as the default IQ test, but was ‚Äúbiased‚Äù to prioritize and value Black culture.\n\n\n\n\n\n\n\nAs seen in the table, Black students scored higher on this IQ test than White students - a point he (and others) use to emphasize the inherent biases in intelligence testing. Dr.¬†Williams also came to define the concept of Ebonics, and demonstrate that African American English is as much a complete language as ‚ÄúStandard‚Äù English.¬†\n\n\n\n\n\n\nCheck-In : Reliability and Validity.\nTest your understanding of reliability and validity with the check-in above.\nBelow is a video to review the check-in answers, since these terms can be tricky :)\n\n\n\nWould You Like to Learn More? [Optional Readings]\n\nHere‚Äôs a textbook chapter on the same topics. Note these authors use three terms to describe what I broadly call ‚Äúconvergent validity‚Äù. Internal consistency (a form of reliability) is measured with ‚Äúalpha reliability‚Äù (we will learn about this next week).\nDr.¬†Williams talks about his research here and here‚Äôs an episode of the TV show Good Times that Dr.¬†Williams consulted on. Here‚Äôs a link to his full study. Note that Dr.¬†Williams gave his intelligence test a name I don‚Äôt feel comfortable using because it is sexist :(. Times change, and it‚Äôs good to call out outdated language and update our terms accordingly :)\nLearn more about the racist history of how phrenology was produced and consumed and an article that conducted more recent researchto test phrenology‚Äôs theories.",
    "crumbs": [
      "List of R Code",
      "Describing People",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>\"Normal\" Data</span>"
    ]
  },
  {
    "objectID": "chapters/6R_LinearModels.html",
    "href": "chapters/6R_LinearModels.html",
    "title": "The Linear Model",
    "section": "",
    "text": "Part 1 : Making Predictions\nThis week, you‚Äôll learn about how we can define a linear model to make predictions about one variable (the DV) from another (the IV). You‚Äôll then learn about R2 (read : ‚ÄúR-squared‚Äù) - professor‚Äôs favorite statistic - and how this quantifies the amount of error in a model. You‚Äôll then learn how our good friend the z-score can be used to calculate a correlation coefficient. Wow!\nAt the beginning of the semester, we talked about how the goal of psychological science is to make predictions about people, while recognizing that our predictions will not perfectly match what actually happens (which we call error).\nConceptually, we can think of a person‚Äôs actual score on y (some variable) as the sum of our prediction and the error in our prediction (with the goal to minimize error as much as possible).\nAs an equation, we would write this as: actual.score = our.prediction + error\nGreat! But the real question is WHAT should define our prediction?",
    "crumbs": [
      "List of R Code",
      "Predicting People",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>The Linear Model</span>"
    ]
  },
  {
    "objectID": "chapters/6R_LinearModels.html#recap-the-mean-and-linear-model-as-prediction",
    "href": "chapters/6R_LinearModels.html#recap-the-mean-and-linear-model-as-prediction",
    "title": "The Linear Model",
    "section": "RECAP : The Mean (and Linear Model) as Prediction",
    "text": "RECAP : The Mean (and Linear Model) as Prediction\nPreviously, we learned how the mean was a simple way to make a prediction about individuals.\nFor example, if you wanted to know whether it would rain today, you might look at the average rainfall for today‚Äôs date, and use that average for your prediction.\nLet‚Äôs review this idea, and set up the linear model, by working with a classic dataset in teaching statistics - the Prestige dataset. These data are contained in the car package, and are used in one of the classic R textbooks - John Fox‚Äô Applied Regression Analysis and Generalized Linear Models (2nd Edition).\n\n\nCode\n# install.packages(\"car\") # this installs a package - you only need to do this ONCE. Remove the # to run this code.\nlibrary(car) # loading the library - make sure you installed it first!\n\n\nLoading required package: carData\n\n\nCode\npresto &lt;- Prestige # creating a copy of the dataset so you don't mess something up :)\nnames(presto) # tadaa!!\n\n\n[1] \"education\" \"income\"    \"women\"     \"prestige\"  \"census\"    \"type\"     \n\n\nYou can read more about these variables by typing in ?Prestige to access the help page for the dataset. For this lecture, we‚Äôll be working with the variable prestige, which is a measure of how prestigious certain jobs were, in Canada, in the 1960s1. Note that individuals in this dataset are not people, but people‚Äôs attitudes about types of jobs.\n1¬†Super exciting! But remember this is like driving around a safe, boring parking lot!! We‚Äôll hit the highway soon enough.The graph to the right illustrates variation in this variable, as well as how the mean of prestige is the value that is closest to all the scores in the distribution.\nThe graph below 2 is defined by the following equation :\n2¬†NOTE: Eagle eyed students may note something odd about the way the data are arranged on this graph - it looks like there‚Äôs some quadratic (curved) pattern in the way the data are arranged. Because the x-axis for this graph (the index) is the row in which each data, all this quadratic pattern means is that more prestigious jobs tended to be listed earlier in the dataset than less prestigious jobs.\\(\\huge y_i = \\hat{Y} + \\epsilon_i\\)\n\n\nCode\nplot(presto$prestige)\nabline(h = mean(presto$prestige), lwd = 5, col = 'red',\n       xlab = \"Index (Individual Row Number)\",\n       ylab = \"Individual Prestige Score\")\n\n\n\n\n\n\n\n\n\n\n\nHere‚Äôs a guide to what‚Äôs in the equation and on the graph!\n\\(\\Large y_i\\) = the DV = the individual‚Äôs actual score we are trying to predict (remember \\(_i\\) = index; a specific individual.)\n\non the graph: each individual dot (on the y-axis; the x-axis just describes when people submitted the survey.\n\n\\(\\Large \\hat{Y}\\) = our prediction (the mean).\n\non the graph: the solid red line\n\n\\(\\Large \\epsilon\\) = residual error = distance between the predicted values of y and the individual‚Äôs actual value of y\n\non the graph: the distance between each dot and the line.\n\nThe equation is fancy way of saying that an individual‚Äôs actual scores = our prediction (the mean) + error.\nThe mean was a good starting place to make predictions for two reasons :\n\nBy definition, the mean is the value that is closest to all the scores. In other words, the mean minimizes the error in our predictions.\nThe mean is static (it does not change), meaning we can make the same prediction for every person. This is a good place to start, but obviously there‚Äôs a lot of error in our predictions. In fact, using our knowledge of residual errors, we can calculate this total (squared) error when we use the mean to make a prediction.\n\n\nCode\nresidual &lt;- presto$prestige - mean(presto$prestige)\nsum(residual^2)\n\n\n[1] 29895.43\n\n\n\nThis value is the total sum of the squared residuals (also called the sum of the squared errors or often abbreviated SST when focused on the residuals of the mean.\nTLDR : we are interested in a variable (prestige), we can make pretty good predictions about prestige based on the mean (since by definition the mean is closest to all the scores in the data), and we are able to quantify how good our prediction is by calculating the sum of squared errors.\nNext, we‚Äôll try to improve upon our predictions of prestige.",
    "crumbs": [
      "List of R Code",
      "Predicting People",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>The Linear Model</span>"
    ]
  },
  {
    "objectID": "chapters/6R_LinearModels.html#the-linear-model-its-just-a-line-with-a-slope",
    "href": "chapters/6R_LinearModels.html#the-linear-model-its-just-a-line-with-a-slope",
    "title": "The Linear Model",
    "section": "The Linear Model : It‚Äôs Just a Line (With a Slope)",
    "text": "The Linear Model : It‚Äôs Just a Line (With a Slope)\n\nBasic Concept.\nThe linear model is a simple, yet flexible, way to make predictions about one variable (the DV or y). There are many types of models, but most follow the same basic principles that we will review in this document. The mean is actually a linear model in its simplest form : y ~ 1 + error, with 1 serving as a constant value (the mean!).\nThe mean is a great starting place, but it‚Äôs limited because it‚Äôs static - we have the same prediction for everyone in the dataset, when we know that people differ. So we will want to come up with predictions that change, depending on what we learn about another person. This is something I refer to as the principle of covariation. It‚Äôs a simple idea - if there‚Äôs a pattern in how two things vary, then we can use information about one variable to make a prediction about what will happen for the other variable. This is what we talked about earlier in the semester when we discussed the idea of prediction - we‚Äôre just formalizing this idea with numbers now.\nSo, as a conceptual example, there is variation in rain (some days it rains and some days it does not) and there is variation in clouds (bright skies some days; clouds other days). The covariation happens as I notice that on days when it rains (variable = rain), it also tends to be cloudy (variable = clouds). These two variables vary together (they ‚Äúco-vary‚Äù). Of course, this pattern isn‚Äôt always true - there‚Äôs error.\nI could write this as a model, where my predictions about whether it will rain will be influenced by whether I see clouds in the sky : rain = clouds + error\nI would use this model to make an adjustment to my prediction when it rains - if there are clouds, I might think it‚Äôs more likely that it will rain. That is, the presence of clouds changes my prediction. This change is a critical idea, and will be important to quanity - how large is the change? Does it improve my predictions?\nAs another example, think about how you would write an equation to model the idea that on days when it‚Äôs raining, I tend to hear raindrops (and on days when it‚Äôs not raining, I don‚Äôt tend to hear raindrops). I might write this model as rain = rain sounds + error\nThese two models look similar, but are not equal. If I had to guess, I‚Äôd say that hearing rain is a better way to make predictions of rain than looking at the clouds, meaning there would be less error in my predictions. So not only do models use information about which variables you can use to make predictions about another, but they also tell you how much you should update your predictions about one variable from another.\n\n\nPrestige Example\nReturning to our previous example, let‚Äôs predict the variable prestige from the variable education - another continuous variable in the Prestige dataset. As a linear model, I would write this as : prestige = education + error\nIn the same way I could expect rain sounds to be related to rain, a job that requires more years of education might be related to how prestigious the job is. This is not true of all careers (consider the ‚Äúhigh educated‚Äù poet ridiculed by society, perhaps, or the ‚Äúlow educated‚Äù firefighter revered by society3.\n3¬†And good to explore why we care about prestige in careers anyway? I blame capitalism and our society‚Äôs unwillingness to provide people with their basic needs, so a person‚Äôs value is determined by how much their material conditions, and whether they can meet (or exceed) those basic needs. Anyway, point is firefighters and poets are both cool and important for society.Let‚Äôs graph these two variables side by side.\n\n\nCode\npar(mfrow = c(1,2)) # splits my graphing window\nhist(presto$prestige, col = 'black', bor = 'white', main = \"\")\nhist(presto$education, col = 'black', bor = 'white', main = \"\")\n\n\n\n\n\n\n\n\n\nGreat. So both prestige and education vary. Not every job has the same prestige, and not every job requires the same years of education.\nThe question is whether these two variables covary. Are changes in prestige related to changes in education??\nPop Quiz: How can you see the relationship between prestige and education in the graphs above?\n::: {.callout-tip collapse = ‚Äútrue‚Äù} You cannot!!! I‚Äôm guessing that some of you are thinking that because both distributions are slightly positively skewed, this means that there is some covariation between these variables. However, this is an example of patterns in randomness - we can‚Äôt really tell whether there is covariation from these two separate graphs. Instead, we need a different kind of graph that explicitly draws a connection between these two separate variables. This is the scatterplot, which you will learn about below :) :::\n\n\nThe Scatterplot\nIn order to examine how these two variables covary (that is, how variation in education is related to variation in prestige), we need to use a scatterplot.\nA scatterplot graphs individual scores in terms of one variable on the Y axis (the vertical line) and the other variable on the X axis (the horizontal line).\nTo graph a scatterplot, you can use the plot() function, and tell R to predict one variable (in this case prestige) from another (education).\nTake a look at the graph below - what do you see?\n\n\nCode\nplot(prestige ~ education, data = presto)\n\n\n\n\n\n\n\n\n\nWatch the video below for an explanation of what‚Äôs going on in the graph.\n\nTo define the model (as I did in the video), we just need to use a few lines of code.\n\n\nCode\nmod &lt;- lm(prestige ~ education, data = presto) # this defines the model, and then saves it to an object (called mod)\ncoef(mod) # this shows me the values of the model\n\n\n(Intercept)   education \n -10.731982    5.360878 \n\n\nThe linear model is just another line that updates our predictions of one variable based on knowledge of another.\nThis line (in red, on the graph below) has the following equation :\n\\(\\huge y_i = a + b_1 * X_i + \\epsilon_i\\)\n\n\nCode\nplot(prestige ~ education, data = presto, # plots the model again\n     xlim = c(0, 16),  # changes my x-axis to range from 0 to 16\n     ylim = c(-20,90)) # changes my y-axis to range from -15 to 90\nabline(mod, col = 'red', lwd = 5) # this adds the line to my graph.\n\n\n\n\n\n\n\n\n\n\n\n\\(\\Large y_i\\) = the DV = each individual‚Äôs actual score on the dependent variable.\n\non the graph: the value of each dot on the y-axis\n\n\\(\\Large a\\) = the intercept = the starting place for our prediction. You can think of the intercept as ‚Äúthe predicted value of y when all x values are zero‚Äù.)\n\non the graph : the value of the line at X = 0\n\n\\(\\Large X_i\\) = the IV = the individual‚Äôs actual score on the independent variable (a different variable than the DV).\n\non the graph : the value of each dot on the x-axis\n\n\\(\\Large b_1\\) = the slope = an adjustment we make in our prediction of y, based on the individual‚Äôs x value.\n\non the graph: how much the line increases in y value when x-values increase by 1 unit.\n\n\\(\\Large \\epsilon_i\\) = residual error = the distance between our prediction and the individual‚Äôs actual y value.\n\non the graph: the distance between each individual data point and the line.\n\n\n\n\n\n\n\nCheck-In: Intercepts and Slopes\n\n\n\n\n\nTest your understanding of linear models! Use the Prestige dataset and R to predict the variable income (the DV) from the variable education (the IV). You can try to do this in R on your own computer, or use the output below. Use this model to answer the questions for this check-in in the link above.\n\n\n\nCall:\nlm(formula = income ~ education, data = presto)\n\nCoefficients:\n(Intercept)    education  \n    -2853.6        898.8",
    "crumbs": [
      "List of R Code",
      "Predicting People",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>The Linear Model</span>"
    ]
  },
  {
    "objectID": "chapters/6R_LinearModels.html#visualizing-residual-error-in-the-model",
    "href": "chapters/6R_LinearModels.html#visualizing-residual-error-in-the-model",
    "title": "The Linear Model",
    "section": "Visualizing Residual Error in the Model",
    "text": "Visualizing Residual Error in the Model\nThis is similar to what we did with the mean - we draw a line that is close to all the individual scores, and then calculate the sum of the squared errors. However, whereas the mean yields the same prediction for each individual (and the line is flat), the linear model yields a specific prediction for each individual‚Äôs y score, based on the value of x (and the line has a slope).\nBelow are two graphs - the one on the left uses the mean to make predictions of prestige, and the one on the right uses education to make predictions of prestige (our model).\nLook at the two graphs - can you tell which one has more (or less) residual error?\n\n\nCode\npar(mfrow = c(1,2))\nplot(presto$prestige)\nabline(h = mean(presto$prestige), lwd = 5, col = 'red')\nplot(prestige ~ education, data = presto, # plots the model again\n     xlim = c(0, 16),  # changes my x-axis to range from 0 to 16\n     ylim = c(-20,90)) # changes my y-axis to range from -15 to 90\nabline(mod, col = 'red', lwd = 5) # this adds the line to my graph.\n\n\n\n\n\n\n\n\n\nJust by looking at two graphs, it‚Äôs clear that there‚Äôs less residual error when we use education to make predictions of prestige (vs.¬†using the mean). In other words, the individual scores are further from the red line on the left graph than on the right graph.",
    "crumbs": [
      "List of R Code",
      "Predicting People",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>The Linear Model</span>"
    ]
  },
  {
    "objectID": "chapters/6R_LinearModels.html#calculating-residual-error-in-the-model",
    "href": "chapters/6R_LinearModels.html#calculating-residual-error-in-the-model",
    "title": "The Linear Model",
    "section": "Calculating Residual Error in the Model",
    "text": "Calculating Residual Error in the Model\nStill, we are going to want to calculate these differences to describe just how much better our predictions are. What makes this potentially more challenging is that when we use a model to make predictions for individual scores, we will predict a different value for each individual based on the result of our model. For example, a job that requires 6 years of education will have a different predicted value of prestige than a job that requires 7 years of education.\nFortunately, R does the hard work of making predictions for us, and even saves the residual errors as part of the model output.\nFor example, from our model where we predicted prestige from education:\n\n\nCode\nhead(mod$residuals) # these are the residuals (the errors from our model)\n\n\n gov.administrators    general.managers         accountants purchasing.officers \n           9.250875           14.107621            5.673573            6.310758 \n           chemists          physicists \n           5.855950            4.487854 \n\n\nCode\nhead(mod$residuals^2) # these are the squared residuals\n\n\n gov.administrators    general.managers         accountants purchasing.officers \n           85.57869           199.02497            32.18943            39.82567 \n           chemists          physicists \n           34.29215            20.14084 \n\n\nCode\nsum(mod$residuals^2) # these are the sum of the squared residuals when using the model to make predictions\n\n\n[1] 8286.99\n\n\nNotice that the sum of the squared errors for this model, where we use education to predict prestige, is less than the sum of the squared errors from the model where we used the mean to make predictions.\nIn fact, we can calculate exactly how large this difference in errors is between the two predictions.\n\n\nCode\nSSM &lt;- sum(mod$residuals^2) # saving the sum of the squared errors from the model\nSST &lt;- sum(residual^2) # saving the sum of the squared errors from the mean\nSST - SSM # the difference in the squared errors from the mean vs. the model\n\n\n[1] 21608.44\n\n\nThis tells me that the model where we use education to predict prestige reduces the squared error by 21608.44. That is a large number! But it‚Äôs hard to understand how large it is, because it‚Äôs missing context.\nTo provide this number context, we can describe how large this reduction in residual error is, relative to the original residual error that we had when we used the mean to make predictions.\n\n\nCode\n(SST - SSM)/SST\n\n\n[1] 0.7228007\n\n\nThis number (.72) means that using education to predict prestige explains 72% of the total variation in prestige (when you use the mean to make predictions). If this sounds like a large percentage, you would be right - there‚Äôs no ‚Äúrule‚Äù about what counts as a large or little percentage of variation explained.\nThis statistic is called \\(R^2\\) (‚Äúr-squared‚Äù), and is defined by the following equation.\n\\(\\Huge R^2 = \\frac{SS_{total} - SS_{model}}{SS_{total}}\\)\nWhat \\(R^2\\) does is contextualize our reduction in error, by describing how much less error we have in our model, compared to the error that we had when using just the mean to make predictions.\n\nAnother way to think of \\(R^2\\) is that it describes the percentage of variation in the DV that our model is able to predict or explain.\n\\(R^2\\) can range from 0 to 1. The closer to zero, the less our model improves upon predictions (because 0 = no difference between the error when using the mean to make predictions and the error when using the model to make predictions). The closer to one, the more our model improves upon predictions. An \\(R^2\\) of 1 would mean that you are making perfect predictions. If this happens, you have probably done something wrong.",
    "crumbs": [
      "List of R Code",
      "Predicting People",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>The Linear Model</span>"
    ]
  },
  {
    "objectID": "chapters/6R_LinearModels.html#video-example-age-and-narcissism",
    "href": "chapters/6R_LinearModels.html#video-example-age-and-narcissism",
    "title": "The Linear Model",
    "section": "Video Example : Age and Narcissism",
    "text": "Video Example : Age and Narcissism\nRemember our Narcissistic MBA students? They‚Äôre back, in Linear Model form!!!!\n\nlink to the R script I used for this video\n\n\n\n\n\n\n\n\nCheck-In : Understanding \\(R^2\\)\n\n\n\n\n\nHere‚Äôs a super quick check-in on interpreting \\(R^2\\) values!",
    "crumbs": [
      "List of R Code",
      "Predicting People",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>The Linear Model</span>"
    ]
  },
  {
    "objectID": "chapters/6R_LinearModels.html#why-z-score-in-a-model",
    "href": "chapters/6R_LinearModels.html#why-z-score-in-a-model",
    "title": "The Linear Model",
    "section": "Why Z-Score in a Model",
    "text": "Why Z-Score in a Model\nIn a linear model, the slope describes the relationship between the two variables in whatever units the DV and the IV were measured in. So in our prestige example, the slope of 5.36 means that for every 1-unit increase in years of education, our prediction of prestige goes up by 5.36 points. Is that a little change in prestige? A lot? It‚Äôs hard to know, since prestige is not tangible, but a human-created construct.\nWhich brings us back to z-scores. If we z-score both the DV and the IV in our linear model, the (arbitrary) units of measurement disappear, and both variables are described in terms of standard deviation. This allows us to better relate each variable to another. To z-score the variables in a model, you just use the scale() function inside the linear model.\nClick the tabs to switch between Raw Units and Z-Scored Units. What changes? What stays the same??\n\nLinear Model in Raw UnitsLinear Model in Z-Scored Units\n\n\nHere‚Äôs the graph, in the original units of measurement.\n\n\nCode\nplot(prestige ~ education, data = presto, \n     ylab = \"Prestige (Raw Units)\",\n     xlab = \"Education (Years)\") \nmod &lt;- lm(prestige ~ education, data = presto)\nabline(mod, lwd = 5, col = 'red')\n\n\n\n\n\n\n\n\n\nAnd here‚Äôs the result of the model.\n\n\nCode\nround(coef(mod), 2)\n\n\n(Intercept)   education \n     -10.73        5.36 \n\n\n\n\nHere‚Äôs the graph when the DV and IV are Z-Scored.\n\n\nCode\nplot(scale(prestige) ~ scale(education), data = presto, \n     ylab = \"Prestige (Units of Standard Deviation)\",\n     xlab = \"Education (Units of Standard Deviation)\") \nzmod &lt;- lm(scale(prestige) ~ scale(education), data = presto)\nabline(zmod, lwd = 5, col = 'red')\n\n\n\n\n\n\n\n\n\nAnd here‚Äôs the result of the model.\n\n\nCode\nround(coef(zmod), 2)\n\n\n     (Intercept) scale(education) \n            0.00             0.85 \n\n\n\n\n\n\nInterpretation of Z-Scores\nAs before, the data do not change - the only thing that changes are the units in which the DV and IV are measured (and thus the units of the intercept and slope).¬†\n\nThe intercept of a z-scored model will always be zero (or something very near zero). Remember that a z-score of zero means average, and the intercept is defined as ‚Äúthe predicted value of the DV when all IVs are zero.‚Äù This means that‚Ä¶.\n\n‚Ä¶the predicted value of the DV is zero when the IV is zero.\n‚Ä¶someone with the average IV (IV = z-score of zero) is predicted to have the average DV (DV = z-score of zero), or¬†\n\nThe slope of a z-scored model describes the relationship between the variables in units of standard deviation. When both the DV and IV share the same units of measurement, the slope becomes a lot more informative, since it tells you exactly how linked the two variables are. Knowing that for every 1-unit increase in years of education, the predicted prestige goes up by 5.36 (the slope) makes far less sense to me than knowing that for every standard deviation increase in years of education, the predicted prestige goes up by .85.\n\nThe maximum slope of a z-scored linear model (with one IV) is 1 (one). This would be a perfect positive relationship, where a one standard deviation increase in the IV is equal to a one standard deviation increase in the DV.¬†\nThe minimum slope of a z-scored linear model (with one IV) is -1 (negative one). This would be a perfect negative relationship, where a one standard deviation increase in the IV is equal to a one standard deviation decrease in the DV.¬†\nA slope of zero would mean that there‚Äôs no relationship between the two variables.\nWait a minute‚Ä¶that‚Äôs‚Ä¶.CORRELATION COEFFICIENT‚ÄôS MUSIC.\n\n\nYes, class, a correlation - the relationship between two variables, is just the standardized (z-scored) slope of a linear model (with one IV).\n\n\nCode\ncor(presto$prestige, presto$education) # the correlation\n\n\n[1] 0.8501769\n\n\nCode\ncoef(zmod)[2] # the slope of our z-scored model\n\n\nscale(education) \n       0.8501769 \n\n\nWow! We will chat more about this in the video below, and in lecture next week :) thanks for reading!\n\n\nVideo Example : Age and Narcissism (Z-Scored)\n{{&lt; https://youtu.be/2jYwIOTaQ6g &gt;}} The video above walks through z-scoring in another example, from the narcissism dataset.",
    "crumbs": [
      "List of R Code",
      "Predicting People",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>The Linear Model</span>"
    ]
  },
  {
    "objectID": "chapters/6R_LinearModels.html#check-out-understanding-z-scores-and-this-chapter-document",
    "href": "chapters/6R_LinearModels.html#check-out-understanding-z-scores-and-this-chapter-document",
    "title": "The Linear Model",
    "section": "CHECK-OUT : Understanding Z-Scores and this Chapter! Document",
    "text": "CHECK-OUT : Understanding Z-Scores and this Chapter! Document",
    "crumbs": [
      "List of R Code",
      "Predicting People",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>The Linear Model</span>"
    ]
  },
  {
    "objectID": "chapters/7R_CategoricalIV.html",
    "href": "chapters/7R_CategoricalIV.html",
    "title": "Linear Model : Categorical IV",
    "section": "",
    "text": "Chapter Overview\nThis week, we will learn how to define and interpret a linear model when the predictor is categorical.\nOften times, researchers want to group people into categories that represent broad and important differences. For example researchers or artists looking to understand social class might divide individuals into groups like capitalists and workers.\nThese categorical variables highlight ways that people differ - there are real and important differences between people who own the means of production, and people who do the work. However, categorical variables can also exaggerate the differences between groups (e.g., both capitalists and workers probably love their children in similar ways) and can minimize individual differences within groups (e.g., workers can differ in terms of their income, their race, their attitudes about capitalism, etc.)",
    "crumbs": [
      "List of R Code",
      "Predicting People",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Linear Model : Categorical IV</span>"
    ]
  },
  {
    "objectID": "chapters/7R_CategoricalIV.html#recap-working-with-categorical-factor-and-levels",
    "href": "chapters/7R_CategoricalIV.html#recap-working-with-categorical-factor-and-levels",
    "title": "Linear Model : Categorical IV",
    "section": "RECAP : Working With Categorical Factor and Levels",
    "text": "RECAP : Working With Categorical Factor and Levels\nIt‚Äôs been a while since we focused on categorical variables, so let‚Äôs review some terms as we work through an example.\nThe specific groups of a categorical variable are called levels, and the broad name for the groups is called a factor. For example, in the ‚ÄúMBA Business Student Data‚Äù (hormone_data.csv) researchers measure the variable sex as a categorical factor, with just two levels : female and male.\n\n\nCode\nmba &lt;- read.csv(\"~/Dropbox/!WHY STATS/Chapter Datasets/hormone_data.csv\", stringsAsFactors = T)\nmba$sex\n\n\n  [1] male   male   male   male   male   female male   male   male   male  \n [11] male   male   male   male   male   male   male   male   male   male  \n [21] male   male   male   male   male   male   male   male   male   male  \n [31] male   male   male   male   male   male   male   male   male   male  \n [41] male   male   male   male   male   male   male   male   male   female\n [51] female female female female female female female female female female\n [61] female female female female female male   male   male   male   male  \n [71] male   male   male   male   male   male   male   male   male   male  \n [81] male   male   male   male   male   male   male   male   male   male  \n [91] male   male   male   male   male   male   male   male   male   male  \n[101] male   male   male   male   female female female female female female\n[111] female female female female female female female female female female\n[121] female female\nLevels: female male\n\n\nI can use the plot() function to illustrate this variable, and summary() function to report the number of individuals in each group.\n\n\nCode\nplot(mba$sex)\n\n\n\n\n\n\nSex, Graphed and Summarized\n\n\n\nCode\nsummary(mba$sex)\n\n\nfemale   male \n    35     87",
    "crumbs": [
      "List of R Code",
      "Predicting People",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Linear Model : Categorical IV</span>"
    ]
  },
  {
    "objectID": "chapters/7R_CategoricalIV.html#a-conceptual-example",
    "href": "chapters/7R_CategoricalIV.html#a-conceptual-example",
    "title": "Linear Model : Categorical IV",
    "section": "A Conceptual Example",
    "text": "A Conceptual Example\nThe equation for a linear model with a categorical IV is identical to what we‚Äôve seen before :¬†\n\\(\\huge y_i = a + b_1 * X_i + \\epsilon_i\\)\nWe are making a prediction about some dependent variable (\\(y_i\\)) from a linear equation that has an intercept (\\(a\\), the starting place for our prediction), and then a slope (\\(b_1\\) an adjustment we make) based on information about another (independent) variable (\\(x_i\\)).\nFor this first example, let‚Äôs define a model to predict narcissism (how self-centered and egotistical a person says they are) from the person‚Äôs sex.\nBefore we jump to the linear model, let‚Äôs graph our dependent variable using the hist() function, since the variable is numeric. In the mba dataset, NPI (Narcissistic Personality Inventory) is what researchers defined for Narcissism\n\n\nCode\nhist(mba$NPI, col = 'black', bor = 'white', main = \"\", xlab = \"Narcissism (NPI) Score\", xlim = c(1,5))\n\n\n\n\n\n\nNarcissism Variable, Graphed\n\n\n\nThe data look good - I don‚Äôt see any outliers or problems in the data, and while it seems a little odd nobody said they were a 1 in terms of narcissism, the distribution is mostly normal and maybe everyone‚Äôs a little narcissistic?\nIn the graph below, I‚Äôm using the plot function to illustrate the individual narcissism scores.\nEach individual narcissism score is a dot defined by the value on the y-axis, and their index (position in the dataset) is located on the x-axis. So the individual in the top left corner has a narcissism score of around 4.45, and was one of the first people to provide data (the index is not super relevant).\nSee if you can guess where the mean is; illustrated as a horizontal line that goes closest to all the individual narcissism scores, such that the sum of the residual errors will be zero. Click the tab to see where the mean is actually is.\n\nWhere is the Mean???Here is the mean!\n\n\n\n\nCode\nplot(mba$NPI, pch = 19, ylab = \"Narcissism Score\", xlab = \"Index (Position in Dataset)\",\n     main = \"Plot of Individual Narcissism Scores\")\n\n\n\n\n\n\n\n\n\nCode\n# abline(h = mean(mba$NPI, na.rm = T), lwd = 5)\n\n\n\n\n\n\nCode\nplot(mba$NPI, pch = 19, ylab = \"Narcissism Score\", xlab = \"Index (Position in Dataset)\",\n     main = \"Plot of Individual Narcissism Scores\")\nabline(h = mean(mba$NPI, na.rm = T), lwd = 5)\n\n\n\n\n\n\n\n\n\n\n\n\nHow did you do? I was a little low on this one. Okay; let‚Äôs keep moving.\nThe graph below illustrates the same data, however this time I‚Äôve asked R to color the dots based on the variable sex. In this graph, red dots illustrate the narcissism score for males in the dataset, the black dots illustrate the narcissism score for females in the dataset. Narcissism scores are on the y-axis, and the x-axis again indicates the index of the individual.\nTake a moment and look at this graph - what do you observe?\n\n\nCode\nplot(mba$NPI, pch = 19, col = mba$sex, ylab = \"Narcissism Score\", xlab = \"Index (Position in Dataset)\",\n     main = \"Plot of Individual Narcissism Scores\\nGrouped By Sex (Red = Male, Black = Female)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat Professor Sees.\n\n\n\n\n\n\nI see that there are more red dots than black dots. This matches what I know about the variable ‚Äòsex‚Äô - there were more Male MBA students than Female MBA students in the data.\nI see that there are some clusters of where red and black dots occur. For some reason, the males and females are grouped - my guess is that these data were collected in two waves (maybe two classes of students) and were organized in each class by sex. This is not super relevant to the data, but\nI see that more of the red dots are higher on the y-axis than the black dots. This is not super easy to see, but there seems to be a trend there - the red dots are slightly higher on average than the black dots. But you don‚Äôt have to take my word for it, this is what the linear model does!\n\n\n\n\nOkay, time to play‚Ä¶.WHERE‚ÄôS‚Ä¶THAT‚Ä¶.LINE!!!! (crowd of students go wild). Think about where you would draw two horizontal lines in the graph above - one that is closest to all the red dots and one that is closest to all the black dots.\n\nWhere‚Äôs the line?The Black LineThe Red Line!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWow! How did you do??? Professor‚Äôs observations from the graph on the last page appear to be correct; the red dots tend to be higher than the black dots.\nI can see this on the graph because the red line (which is the average of the red dots) is higher than the black line (which is the average of the black dots).\nThis is the core idea of a linear model - we are making predictions of one variable (y = the DV = Narcissism) from another variable (x = the IV = sex). For people who are male (the red dots), we are going to predict a narcissism score of around‚Ä¶..3.3 (where the red line hits the y-axis). Not every male has this exact same narcissism score (life is complex!) - we can see the residual error of red dots above and below this line (we will get to calculate this soon; hooray!) - but this red line defines the trend.\nFor people who are female, we can predict a narcissism score of around 3 (where the black line hits the y-axis). Not every female has this exact same narcissism score, but the line defines the trend.\nAnd now, we can calculate the difference between these two groups - 0.3 - as the slope - the change we make in our predictions of narcissism depending on whether the person is male or female. Wow.",
    "crumbs": [
      "List of R Code",
      "Predicting People",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Linear Model : Categorical IV</span>"
    ]
  },
  {
    "objectID": "chapters/7R_CategoricalIV.html#connecting-a-linear-model-to-the-graph",
    "href": "chapters/7R_CategoricalIV.html#connecting-a-linear-model-to-the-graph",
    "title": "Linear Model : Categorical IV",
    "section": "Connecting a Linear Model to the Graph",
    "text": "Connecting a Linear Model to the Graph\nOkay, let‚Äôs dig into what‚Äôs really going on here with the model. To define these lines, I ran the same function we used last week - `lm()`.\n\n\nCode\nmod &lt;- lm(NPI ~ sex, data = mba) \n\n\n\nThis code defines an object (that I‚Äôm calling mod, but you can call it whatever you want) as a linear model (lm), predicting Narcissism (NPI) from the variable sex, using the mba dataset. Nothing is show, because we have just defined the model.\n\n\n\nCode\ncoef(mod)\n\n\n(Intercept)     sexmale \n  3.0138235   0.2628015 \n\n\n\nThe coef() function asks R to report the coefficients from this new object. R reports the intercept (3.01) and a slope (0.26). Note that R has assigned this slope to one of the groups (male) of the categorical variable.\n\nThe linear model that we have defined with this categorical independent is doing the same thing that our linear model with a numeric IV did - making predictions about the dependent from changes in the independent variable.¬†\nThe slope is the key statistic here, since it tells us how our predictions of the dependent variable should change as the independent variable changes. When the independent variable was numeric, it could theoretically take any value. We could, for example, calculate the narcissism of someone who was 20 years old, 21 years old, 20.5 years old, etc. Of course, it may not be appropriate to calculate the narcissism of someone who was -100 years old, or 1000 years old (since these are nonsensical numbers), or even someone who was 10 years old if all the original data were based on college-age students.¬†\nWhen the independent variable is categorical, it cannot take any value, since the data are constrained to be in a specific group. This means that we have to assign each group some numeric value - something called dummy coding.In our example, researchers have measured sex as a simple binary (Male or Female), the independent variable (sex) can only take two values - Male and Female. Male and Female are not values, so we will assign them values - 0 and 1.\nR defaults to alphabetical order - because F (for Female) comes earlier in the alphabet than M (for Male), when X = 0, we will be referring to the Females in the dataset, and when X = 1, we will be referring to the Males in the dataset.1\n1¬†It‚Äôs up to you which group will be assigned the value 0 and which group will be assigned the value 1, and later we will learn how to swap this around and why it might matter.\n\n\nIV Value\nCategorical Factor Level\n\n\n\n\nX = 0\nFemale\n\n\nX = 1\nMale\n\n\n\nLet‚Äôs look at the graph. The intercept is the starting place for our predictions when all X values (the IV) are equal to zero. In this case, an X value of zero means that the individual is NOT male. The only other option in these data if the individual is NOT male is to be female. So the intercept - 3.01 - is the predicted Narcissism for someone who is female.\n\n\nCode\ncoef(mod)\n\n\n(Intercept)     sexmale \n  3.0138235   0.2628015 \n\n\nCode\nplot(mba$NPI, pch = 19, col = mba$sex, ylab = \"Narcissism Score\", xlab = \"Index (Position in Dataset)\",\n     main = \"Plot of Individual Narcissism Scores\\nGrouped By Sex (Red = Male, Black = Female)\")\nabline(h = coef(mod)[1], lwd = 5, col = 'black')\nabline(h = coef(mod)[1] + coef(mod)[2], lwd = 5, col = 'red')\n\n\n\n\n\n\n\n\n\nThe slope is the adjustment we make to our prediction when X changes by 1, which means we refer to the males in the dataset. This means we add 0.26 to our starting place = 3.01 + .26 = 3.27 = where the horizontal red line is drawn = the predicted value of Narcissism for males in the dataset.",
    "crumbs": [
      "List of R Code",
      "Predicting People",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Linear Model : Categorical IV</span>"
    ]
  },
  {
    "objectID": "chapters/7R_CategoricalIV.html#a-simpler-way-to-illustrate-a-model-with-a-categorical-iv-the-plotmeans-function",
    "href": "chapters/7R_CategoricalIV.html#a-simpler-way-to-illustrate-a-model-with-a-categorical-iv-the-plotmeans-function",
    "title": "Linear Model : Categorical IV",
    "section": "A Simpler Way to Illustrate a Model with a Categorical IV: the plotmeans() function",
    "text": "A Simpler Way to Illustrate a Model with a Categorical IV: the plotmeans() function\nI only use the plot() function for conceptual understanding - there are better (and easier) ways to visualize this linear model in R.\nOne method I like is the plotmeans() function. This function comes from the gplots package. You will need to install this package once, then load it from the library each time you start R.2\n2¬†install.packages(‚Äúgplots‚Äù) to install the package, then library(gplots) to load.3¬†The warning message is telling me that the gplots and the stats libaries both have a function called lowess. R is ‚Äúmasking‚Äù this function from stats, which means if I refer to the lowess function , R will think I mean the one that comes from the gplots package. If I want to use the lowess from the stats library, I will need to manually tell R to do this using stats::lowess(). We won‚Äôt use the lowess function in this class.Below is what it looks like when I install the gplots package (remember you only need to do this once) and then load the gplots library.3\n\n\n\n\n\nOnce I get the gplots library working, I can use the plotmeans() function. This works similar to plot(), in that I define a DV, IV, and dataset. It can also take familiar arguments (like changing the title, axis labels, and axis limits). Below is the default graph you see when I run the most basic code :¬†\n\n\nCode\nlibrary(gplots)\n\n\n\nAttaching package: 'gplots'\n\n\nThe following object is masked from 'package:stats':\n\n    lowess\n\n\nCode\nplotmeans(NPI ~ sex, data = mba)\n\n\n\n\n\n\n\n\n\n\nNPI (Narcissism) scores are on the y-axis. R has limited the range of this variable in order to highlight the slope.¬†\nThe categorical variable sex is defined on the x-axis. I can see the two groups - female and male. Above each label, R has listed the sample size (n) - the number of individuals in each group (there are 34 females and 80 males who gave narcissism data).\nThe small dot for each group is the predicted value of Narcissism for that group. Note that the dot for females is 3.01 (the intercept) and the dot for males is 3.27 (which is .26 higher than female = the slope).\nThe bars above and below the dots are something called ‚Äústandard error bars‚Äù. We will learn about those in a few weeks; but you can basically think of them as an illustration of the ‚Äúmargin of error‚Äù we have when making predictions.\nThe line connecting the two dots is trying to illustrate the slope.However, it‚Äôs a little misleading, since it makes it seem like there are possible predicted values of Narcissism between female and male. While sex is a spectrum, these researchers did not measure sex in a numeric way, so there‚Äôs not data looking at estimates of narcissism for non-cisgendered people / folks on the spectrum, so this line is not appropriate. We can turn the line off with an argument.\n\nBelow is a graph I might run to ‚Äúclean-up‚Äù some of these issues; I‚Äôve removed the line illustrating the slope, expanded the range of the y-axis, and renamed the variables.¬†\n\n\nCode\nplotmeans(NPI ~ sex, data = mba, connect = F, ylab = \"Narcissism Score\", xlab = \"Sex\",\n          ylim = c(1,5))\n\n\nWarning in arrows(x, li, x, pmax(y - gap, li), col = barcol, lwd = lwd, :\nzero-length arrow is of indeterminate angle and so skipped\n\n\nWarning in arrows(x, ui, x, pmin(y + gap, ui), col = barcol, lwd = lwd, :\nzero-length arrow is of indeterminate angle and so skipped\n\n\n\n\n\n\n\n\n\nNotice that when I expand the y-axis range to include the full range of the scale, the difference in narcissism looks a lot smaller than it did when the graph was ‚Äúzoomed‚Äù in. This is a critical media literacy skill - researchers sometimes report ‚Äúzoomed in‚Äù graphs that make the effect look bigger than it really is.¬†\nIf only there was a way to actually define how large an effect is..using numbers! Oh‚Ä¶.do you hear that‚Ä¶..it‚Äôs \\(R^2\\)‚Äôs music!!!!!",
    "crumbs": [
      "List of R Code",
      "Predicting People",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Linear Model : Categorical IV</span>"
    ]
  },
  {
    "objectID": "chapters/7R_CategoricalIV.html#r-and-the-linear-model-with-a-categorical-iv",
    "href": "chapters/7R_CategoricalIV.html#r-and-the-linear-model-with-a-categorical-iv",
    "title": "Linear Model Pt 2: Categorical IVs",
    "section": "\\(R^\\) and the Linear Model with a Categorical IV",
    "text": "\\(R^\\) and the Linear Model with a Categorical IV\nThe principle behind \\(R^2\\) for a linear model with a categorical IV is the same - we are looking to see how much less residual error there is when we use the model to make predictions of our DV, compared to when we use the mean when making predictions of our DV.\nYou can try to visualize this decrease in the graphs below.\n\n\nCode\npar(mfrow = c(1,2))\nplot(mba$NPI, pch = 19, ylab = \"Narcissism Score\", xlab = \"Index (Position in Dataset)\",\n     main = \"Residual Errors Using the Mean\")\nabline(h = mean(mba$NPI, na.rm = T), lwd = 5)\nplot(mba$NPI, pch = 19, col = mba$sex, ylab = \"Narcissism Score\", xlab = \"Index (Position in Dataset)\",\n     main = \"Residual Errors Using the Model\")\nabline(h = coef(mod)[1], lwd = 5, col = 'black')\nabline(h = coef(mod)[1] + coef(mod)[2], lwd = 5, col = 'red')\n\n\n\n\n\n\n\n\n\n\nFor the mean (graph on left), the residual errors are the distance between each individual score (dot) and the prediction (black line).\nFor the model (graph on right), the residual errors are the distance between each red dot (males actual narcissism score) and the red line (predicted narcissism for males) AND the distance between each black dot (females actual narcissism score) and the black line (predicted narcissism for females).\n\nI can calculate these residuals, like we did in the last chapter for a linear model with a numeric IV.\n\n\nCode\nresidual &lt;- mba$NPI - mean(mba$NPI, na.rm = T)\ntotal.residual &lt;- sum(residual^2, na.rm = T) # 32.48\ntotal.residual\n\n\n[1] 32.47965\n\n\nCode\nmodel.residual &lt;- sum(mod$residuals^2)\nmodel.residual\n\n\n[1] 30.83179\n\n\nCode\ntotal.residual - model.residual\n\n\n[1] 1.647857\n\n\nIt‚Äôs a pretty small difference in residual errors, and plugging these values into our equation of \\(R^2\\) shows that our model really only reduces residual error by about 5% (compared to the mean).\n\n\nCode\n(total.residual - model.residual)/model.residual\n\n\n[1] 0.0534467\n\n\nCode\nsummary(mod)$r.squared # R^2 the easy way\n\n\n[1] 0.05073507\n\n\nSo yes, male business students say they are more narcissistic than female business students, but differences in sex only explain about 5% of the variation in narcissism. Life is complex. As always.",
    "crumbs": [
      "List of R Code",
      "Predicting People",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Linear Model Pt 2: Categorical IVs</span>"
    ]
  },
  {
    "objectID": "chapters/7R_CategoricalIV.html#re-leveling-a-2-level-factor-variable.",
    "href": "chapters/7R_CategoricalIV.html#re-leveling-a-2-level-factor-variable.",
    "title": "Linear Model : Categorical IV",
    "section": "Re-Leveling a 2-Level Factor Variable.",
    "text": "Re-Leveling a 2-Level Factor Variable.\nOkay, last thing. Remember how I write that the order of the levels (female = 0, male = 1) is arbitrary, and R defaults to alphabetical order? Sometimes you want to change the order around - in this example, make male = 0 and female = 1.4\n4¬†It‚Äôs not important to do releveling in this example. However, sometimes one group is a clear reference group that should be assigned as the intercept. For example, if one group is the ‚Äúdefault‚Äù experience, and you want to see how the other group changes that default. We‚Äôll chat more about this next week.This is called releveling. When you relevel, the data do not change - just the order of the data changes.\n\n\nCode\nrelevel(mba$sex, ref = \"male\")\n\n\n  [1] male   male   male   male   male   female male   male   male   male  \n [11] male   male   male   male   male   male   male   male   male   male  \n [21] male   male   male   male   male   male   male   male   male   male  \n [31] male   male   male   male   male   male   male   male   male   male  \n [41] male   male   male   male   male   male   male   male   male   female\n [51] female female female female female female female female female female\n [61] female female female female female male   male   male   male   male  \n [71] male   male   male   male   male   male   male   male   male   male  \n [81] male   male   male   male   male   male   male   male   male   male  \n [91] male   male   male   male   male   male   male   male   male   male  \n[101] male   male   male   male   female female female female female female\n[111] female female female female female female female female female female\n[121] female female\nLevels: male female\n\n\nNote that this time at the bottom of the output, R is listing male as the first level, and female as the second level. To save the releveling change, I‚Äôm going to define a new variable (sexR) that is part of the mba dataset. I can then use this new, releveled variable, in my linear model and create a graph.\n\n\nCode\n#|fig-column: margin\nmba$sexR &lt;- relevel(mba$sex, ref = \"male\")\nmodR &lt;- lm(NPI ~ sexR, data = mba)\ncoef(modR)\n\n\n(Intercept)  sexRfemale \n  3.2766250  -0.2628015 \n\n\nCode\nplotmeans(NPI ~ sexR, data = mba)\n\n\n\n\n\n\n\n\n\nCode\nsummary(modR)$r.squared\n\n\n[1] 0.05073507\n\n\nThis graph should look familiar; it‚Äôs the mirror image of what we saw before. The intercept is still the predicted value of narcissism when X = 0, but now X = 0 means the person is NOT female (and therefore is male). This predicted value of 3.27 is the same predicted value we saw before releveling.\nThe slope now is the adjustment in narcissism we make when we go from X = 0 to X = 1, or the difference in narcissism between females and males. This is just the flip of our previous slope, and describes that our prediction is that females will be .26 points less narcissistic than males.\nAnd if I ask R to calculate the R2 value, I get the same result because the model has not really changed; just the order of my levels.",
    "crumbs": [
      "List of R Code",
      "Predicting People",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Linear Model : Categorical IV</span>"
    ]
  },
  {
    "objectID": "chapters/7R_CategoricalIV.html#check-in-to-assess-your-understanding-here",
    "href": "chapters/7R_CategoricalIV.html#check-in-to-assess-your-understanding-here",
    "title": "Linear Model : Categorical IV",
    "section": "CHECK-IN TO ASSESS YOUR UNDERSTANDING HERE",
    "text": "CHECK-IN TO ASSESS YOUR UNDERSTANDING HERE",
    "crumbs": [
      "List of R Code",
      "Predicting People",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Linear Model : Categorical IV</span>"
    ]
  },
  {
    "objectID": "chapters/7R_CategoricalIV.html#another-example",
    "href": "chapters/7R_CategoricalIV.html#another-example",
    "title": "Linear Model : Categorical IV",
    "section": "Another Example",
    "text": "Another Example\nLet‚Äôs practice with another example : Professor wants to know whether there are sex differences in testosterone. Define a linear model to predict testosterone (DV = test) from sex (IV = sex). Graph the linear model, report the slope and R2 value from the linear model, and interpret what you learn.\nTry this on your own. There‚Äôs a key (and video key) below.\n\n\n\n\n\n\nTip\n\n\n\n\n\nDefining the Linear Model and Graphing the Relationship with plotmeans()\n\n\nCode\ntest.mod &lt;- lm(test ~ sex, data = mba)\ncoef(test.mod)\n\n\n(Intercept)     sexmale \n   41.39556    49.28841 \n\n\nGraphing the Relationship: Conceptual Example\n\n\nCode\nplot(mba$test, col = mba$sex)\nabline(h = coef(test.mod)[1], lwd = 5, col = \"black\")\nabline(h = coef(test.mod)[1] + coef(test.mod)[2], lwd = 5, col = \"red\")\n\n\n\n\n\n\n\n\n\nInterpreting R2\n\n\nCode\nsummary(test.mod)$r.squared\n\n\n[1] 0.3469868",
    "crumbs": [
      "List of R Code",
      "Predicting People",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Linear Model : Categorical IV</span>"
    ]
  },
  {
    "objectID": "chapters/8R_SamplingError.html",
    "href": "chapters/8R_SamplingError.html",
    "title": "Sampling Error and Bias",
    "section": "",
    "text": "Is This The Real Life? Is This Just Fantasy? (Inferential Statistics)",
    "crumbs": [
      "List of R Code",
      "Predicting People",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Sampling Error and Bias</span>"
    ]
  },
  {
    "objectID": "chapters/8R_SamplingError.html#is-this-the-real-life-is-this-just-fantasy-inferential-statistics",
    "href": "chapters/8R_SamplingError.html#is-this-the-real-life-is-this-just-fantasy-inferential-statistics",
    "title": "Sampling Error and Bias",
    "section": "",
    "text": "Samples and Populations\nSo far, the statistics that we‚Äôve done and described have been focused on sample data - the data that a reseracher collects and / or analyzes. For example, when we calculate the mean of a sample, we are reporting the statistic that is closest to all the scores in our sample of data. When we calculate a slope, we are describing how one variable is related to another variable in our sample of data.\nResearchers are rarely interested in only their sample, but instead want to learn about a broader population - that is, all the individuals who might be relevant to a researcher‚Äôs question.\nAs we will discuss, thinking about who would be the population for a research question, and then identifying who is in the sample, is a critical step for understanding and evaluating the interpretation of the data.\nThe sample is always clearly defined and reported in the ‚ÄúMethods‚Äù section, where researchers describe (in detail) the number (sample size = n) and demographic features of the people they studied.\nThe population is not often clearly defined or reported, but can be inferred by thinking about all the possible people who could be affected by, or relevant to, the researcher‚Äôs question.\nBelow are a few examples of reserach questions, populations, and the specific samples that researchers studied.\n\n\n\nResearcher Question\nPopulation\nSample (Example)\n\n\nWhat is the patient‚Äôs white blood cell count.\nAll the blood in a person‚Äôs body.\nThe vial of blood that a phlebotomist collects.\n\n\nIs money related to happiness?\nEveryone in the world who could be happy and have an income.\n‚ÄúParticipants were 33,391 employed adults living in the United States. The median age was 33, the median household income was $85,000/year (25th¬†percentile = $45,000; 75th¬†percentile = $137,500; mean = $106,548; SD = $95,393), 36% were male, and 37% were married.‚Äù\n\n\nYour final project research question.\nWho is in the population?\nWho will be in your sample?\n\n\n\n\n\nWhen the Sample Doesn‚Äôt Equal the Population\nResearchers rarely, if ever, have access to the full population of data, and thus must use the sample to make a guess (or inference) about the population. For example, doctors don‚Äôt drain a person of all their blood in order to learn about the person‚Äôs health, but trust that the sample will give them valid information about what‚Äôs going on in the whole body. That‚Äôs not always the case, however, and an important task for researchers is to consider the possibility that they are wrong.\nThe key question is then, how can we trust that the information we learn from the sample is at all related to the broader population? This is hard to do, and the focus of this chapter. In fact, there are two reasons why we would expect that the sample will not, in fact, equal the population.\nSampling Error describes when the sample is different from the population because of random reasons. A good sample will be random sample, meaning that each individual in the population has an equal chance of being selected for the sample. Even if this is the case, there‚Äôs always going to be some differences between the individuals in the study, and the individuals in the population. As you‚Äôll learn in the section below, researchers use statistics to try and estimate how much sampling error might influence their results, and try to design studies to limit the extent that sampling error can affect their results.\nSampling Bias describes when the sample is different from the population because of predictable (or non-random) reasons. A blood sample would be biased if we knew there was something systematically different about the blood that was drawn from the arm vs.¬†the hand vs.¬†the neck vs.¬†the part of your foot between your toes. Identifying sampling bias requires some critical thinking skills that we will practice in the section below, and our goal as researchers will be to identify possible sources of bias, and minimize their influence (or do new studies to test their influence.)\nLet‚Äôs start with sampling bias, because it‚Äôs a lot shorter and easier to understand :)",
    "crumbs": [
      "List of R Code",
      "Predicting People",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Sampling Error and Bias</span>"
    ]
  },
  {
    "objectID": "chapters/8R_SamplingError.html#sampling-bias",
    "href": "chapters/8R_SamplingError.html#sampling-bias",
    "title": "Sampling Error and Bias",
    "section": "Sampling Bias",
    "text": "Sampling Bias\n\nEvaluating Whether Sampling Bias Exists\nThere are three questions to consider when evaluating whether sampling bias is influencing the results.\n\nWhat is the research question, who is the population relevant to this question, and who is in the sample? This first question (‚Ä¶okay, technically three questions) asks you to define the key features of the study (as described above.)\nIs the sample representative of the population, or is there a systematic bias? The second question asks you to evaluate whether the people in the sample are predictably different from the people in the population. This will almost always be true - most psychological research questions are relevant to people across the world, and it‚Äôs very difficult to ensure that everyone in the world has an equal chance of participating in the study.\nIs this bias related to the dependent variable? (Will the bias influence the results?) The final question is the hardest, and most important, to consider.\n\n\n\nObama Polling Example : Sampling Bias\nBelow are data from 20121, when Barack Obama was running against Mitt Romney in the US Presidential Election. Look over the data, and think about the answers to the three questions to evaluate sampling bias.\n1¬†source : https://fivethirtyeight.com/features/obamas-lead-looks-stronger-in-polls-that-include-cellphones/\n\n\n\n\n\n\nWhat is the research question, who is the population relevant to this question, and who is in the sample?\n\n\n\n\n\n\nthe question : who are people going to vote for US President in 2012?\nthe population : all voters / likely voters in the US.\nthere are multiple samples : some polls only sample people without cell-phones by randomly calling people who have landline phones. other polls sample people with landlines or cell-phones.\n\n\n\n\n\n\n\n\n\n\nIs the sample representative of the population, or is there a systematic bias?\n\n\n\n\n\n\nno! the sample is not representative of the population. not all voters have landline phones (or cell-phones, but more folks have cell-phones these days). So while both samples are biased, the samples that only include landline phone owners are more biased than the samples that include both landlines and cell-phone owners.\n\n\n\n\n\n\n\n\n\n\nIs this bias related to the dependent variable? (Will the bias influence the results?)\n\n\n\n\n\n\nthe bias is related to the dependent variable / research question. landline phone owners will tend to be older, and older voters tend to vote more conservative.\nYou can see this bias in the data; Obama (the more liberal candidate) was predicted to win by less in the studies that only sampled landline phone owners.\n\n\n\n\n\n\nTypes of Sampling Bias\nThe sampling bais in this example would be labeled exclusion bias; below are a few other common types of biases.\n\nSelf-Selection Bias : People volunteer to be in a study, and there‚Äôs likely something different about someone who might choose to be in a study compared to someone who may not want to be in a study in ways that might influence the results.\nSurvivor Bias : Participants often drop out of a study, especially those where you are following the participant over a long period of time. Researchers therefore are only collecting data from a specific group of people who ‚Äúlasted‚Äù through the study. I experience this with my check-ins; the students who rate how the class is going are biased, because they are the ones who haven‚Äôt quit out of boredom or anxiety.\nWEIRD Samples. Read the short article ‚ÄúA WEIRD View of Human Nature‚Äù that describes how most psychologists get samples for their studies. Focus on the following key ideas: the definition of WEIRD; the frequency and influence of cross-cultural research on sampling biases.\n\n\n\nAnother Example\n\nWhat is the POPULATION for ‚ÄúRate My Professor‚Äù (a website where students rate the quality of a professor they have had)?\n\nAll the people in the world.\nAll the students in the world.\nAll the students who have taken a specific professor‚Äôs course.\nAll the students who wrote a review about the course.\nAll the students who read the reviews.\n\nWhat is the SAMPLE for ‚ÄúRate My Professor‚Äù (a website where students rate the quality of a professor they have had)?\n\nAll the people in the world.\nAll the students in the world.\nAll the students who have taken a specific professor‚Äôs course.\nAll the students who wrote a review about the professor‚Äôs course.\nAll the students who read the reviews.\n\nWhat does WEIRD stand for?\n\nWhite, Educated, Individualistic, Rich, Democratic\nWestern, Educated, Industrial, Rich, Democratic\nA statistical term that describes participants who are more than three standard deviations from the average of a variable.\nThis is a term that psychologists no longer use because it is dehumanizing\nNone of the above.\n\nAccording to the article on cross-cultural differences in psychology‚Ä¶\n\nEffects that are ‚Äúfundamental‚Äù do not differ across cultures.\nCertain areas of psychology, like visual perception, do not differ across cultures.\nMost psychological researchers put effort into examining cross-cultural differences.\nAll of the above.\nNone of the above.\n\nDr.¬†Researcher wants to study people‚Äôs attitudes about marijuana. According to the Professor‚Äôs video in the reading notes, which of the following would be an example of sampling bias?\n\nDr.¬†Researcher surveys people living in the Bay Area (where people tend to have more positive attitudes about marijuana than people living in other places).\nDr.¬†Researcher surveys people living in Lytle, Texas (where people tend to have more negative attitudes about marijuana than people living in other places).\nDr.¬†Researcher surveys people wearing white t-shirts (a factor that is unrelated to attitudes about marijuana).\nBoth A and B\nall of the above.\n\n\n¬†Piff, P. K., Dietze, P., Feinberg, M., Stancato, D. M., & Keltner, D. (2015). Awe, the small self, and prosocial behavior. Journal of personality and social psychology, 108(6), 883. LINK TO FULL ARTICLE",
    "crumbs": [
      "List of R Code",
      "Predicting People",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Sampling Error and Bias</span>"
    ]
  },
  {
    "objectID": "chapters/8R_SamplingError.html#sampling-error-conceptual",
    "href": "chapters/8R_SamplingError.html#sampling-error-conceptual",
    "title": "Sampling Error and Bias",
    "section": "Sampling Error (Conceptual)",
    "text": "Sampling Error (Conceptual)\n\nThinking Through a Population\nOkay, this may seem a little strange, but I‚Äôll need your full attention for this part of the chapter. Please place your left and right hand thumbs and index fingers together to make a triangle with your two hands. Spread your other fingers forward, so you are using your full hand to make this power triangle.\n\nOkay, now please place your thumbs in between your eyebrows, and stare at the triangle below. While you stare, say ‚Äúna-na-na-na-na-na-na-na-na-na‚Äù (repeating) - starting as quiet as you can, and getting as loud as you feel comfortable for no less than 5 seconds and no more than 15 seconds. I‚Äôll give you a moment to do this.\n\n\n\n\n\nWow. Did you feel that power? You did, right?? Yeah. Amazing.\nWhat we‚Äôve done - through the power of triangles (the strongest shape, both physically and psychologically) - is generate a dataset2 that we will consider to be a population of data.\n2¬†You can access the fake dataset here if you want to follow along in R, but this is not necessary and I think reading these notes should suffice.I‚Äôve named this dataset babby, and it has two variables for 40000 individuals - one IV and one DV.\n\nbabby &lt;- read.csv(\"~/Dropbox/Teaching Datasets/brainwavebabydata.csv\")\nhead(babby)\n\n           IV         DV\n1 -0.72974695  0.7393607\n2  1.55894120 -0.8977654\n3  1.87894858 -0.4780742\n4  0.36830861 -0.9365583\n5  0.02805939  1.5690341\n6  2.18060600 -2.1974925\n\nnrow(babby)\n\n[1] 40000\n\n\nIf we graph these variables, we see that they appear to each be normally distributed.¬†\n\npar(mfrow = c(1,2))\nhist(babby$IV, col = 'black', bor = 'white', main = \"Histogram of IV\", xlab = \"Independent Variable\")\nhist(babby$DV, col = 'black', bor = 'white', main = \"Histogram of DV\", xlab = \"Dependent Variable\")\n\n\n\n\n\n\n\n\nAnd if we plot the relationship between these two variables, we find that there is no relationship between the DV and the IV. Content warning - some students find the image below existentially terrifying. (What do you see in the graph below? What does this say about you and your personality?)\n\npar(mfrow = c(1,1))\nplot(DV ~ IV, data = babby, pch = 19, main = \"Relationship Between DV and IV\", xlab = \"IV\", ylab = \"DV\")\nmod &lt;- lm(DV ~ IV, data = babby)\nabline(mod, lwd = 5, col = 'red')\n\n\n\n\n\n\n\n\nR confirms that there is no relationship between these two variables - the slope for the IV rounds (to ten decimal places) to zero.\n\ncoef(mod)\n\n  (Intercept)            IV \n-8.035936e-18 -6.313305e-16 \n\nround(coef(mod), 10)\n\n(Intercept)          IV \n          0           0 \n\n\n\nSo to recap, we have a population of data (40,000 individuals) for whom the ‚Äútrue‚Äù relationship is zero.\n\n\n\n\nUsing R to Define a ‚ÄúPerfectly Random Sample‚Äù\nAs described above, psychologists will almost never have access to the true population of data. Instead, they take a ‚Äúsample‚Äù from the population. When every individual in the population has an equal chance of being selected for this sample, we can say the sample is a ‚Äúrandom sample‚Äù. When there are certain individuals who are more or less likely to be included in the sample, we say the sample has ‚Äúbias‚Äù.\nMost samples in psychology are biased; taking a random sample from the population as a psychological researcher is incredibly difficult, if not impossible. However, even if you were to take a perfectly random sample from the population, you should expect your sample to differ at least a little from the broader population.\nWe can illustrate this in R. R has a built-in function called sample(), which takes a random sample3 of whatever numbers you give it. For example, here‚Äôs a fun game - think of a number between 1 and 10. Then ask R to sample() a random number between 1 and 10 and see if you are vibing with R. I am thinking of the number FOUR.\n3¬†There are different ways that computers can take ‚Äúrandom samples‚Äù, each with different merits. The default method R uses is ‚Äú‚ÄúMersenne-Twister‚Äù From Matsumoto and Nishimura (1998). This stuff goes way beyond my pay grade; I imagine there are a few explanations on the internet. Let me know if you find a good one!\n\n4¬†Note that when I use the set.seed() function before the sample() function, I‚Äôm telling R to ‚Äúfix‚Äù the random number generator it uses. This gives me a consistent result time it runs the ‚Äúrandom‚Äù sample() function. And if you also run set.seed(42) before you run sample(1:10, 1) you should also get the same answer of 1. You can specify any number within set.seed(), and only want to use this function when you want others to get the same answer as you (like when sharing code). Most of the time, we don‚Äôt want to use this function because we want things to be ‚Äúrandom‚Äù. Let me know if this is confusing! I can try to clarify.In the code below, I‚Äôm asking R to define the numbers 1 through 10, and then draw ONE number from this set.4\n\nset.seed(42)\nsample(1:10, 1)\n\n[1] 1\n\n\nR chose the number 1. We are not vibing. But every time I run the sample() function again, I‚Äôll get a new random number every time.\n\nsample(1:10, 1)\n\n[1] 5\n\nsample(1:10, 1)\n\n[1] 1\n\nsample(1:10, 1)\n\n[1] 9\n\nsample(1:10, 1)\n\n[1] 10\n\nsample(1:10, 1)\n\n[1] 4\n\n\nOkay, go ahead and try this yourself - think of a number, and let us know on Discord how many times it took you to run the sample(1:10, 1) function before R chose your number. (Or not.)\nWe can use this function to ask R to take a ‚Äúperfect‚Äù random sample from our population. I‚Äôm going to adapt the code a bit to draw a random sample from the babby dataset.\n\nbabby[sample(1:nrow(babby), 10), ]\n\n               IV           DV\n8826   0.98527631 -0.518293954\n16740 -0.55196783  0.056675257\n7700   1.10483168  0.005026792\n36722  0.09618142  0.117929657\n9091  -0.07852571 -0.552495893\n33700 -1.00134328  1.441827817\n13610 -1.33371387 -1.064411326\n28559  0.18542185 -0.461931331\n22725  0.47526546 -0.617396883\n11224  0.73776812 -2.402637937\n\n\n\nbabby[ , ] # this code indexes the babby dataset. the dataset is two dimensional; I can instruct R to select specific rows before the comma, and columns after the comma.\nsample(1:nrow(babby), 10) # this code instructs R to select specific rows (since it comes before the comma), and tells R to take a random sample of 10 numbers, starting with the number 1 and going through however many rows babby has (in this case, 40,000).¬†¬†\nafter the comma, I have no code, which tells R to give me all the columns (in this case two - the IV and DV)\n\nAs you can see in the above output, R has drawn a random sample of 10 individuals from the list of 40,000. If I run this code again, R will select another 10 individuals from the list of 40,000.\n\nbabby[sample(1:nrow(babby), 10), ]\n\n               IV          DV\n33713  0.08619728  1.63778479\n37126 -0.42197038 -0.65012382\n12418 -0.38048978  0.61041238\n15765 -2.07429669 -0.89532671\n9207   0.54379840 -0.02403073\n31418 -1.52499812  1.27384500\n24609  0.81390193 -0.56375600\n103   -0.29130008 -1.44600496\n10349  0.17771080 -1.27641647\n36940 -0.46201091 -0.67224917\n\n\nIf I want to SAVE this sample (and use it later), I need to give it a name so R remembers what the sample is (and doesn‚Äôt keep creating new samples).¬†\n\nsampy &lt;- babby[sample(1:nrow(babby), 10), ]\nsampy\n\n              IV         DV\n5897   0.8485550 -0.2524885\n20003  0.6540785 -0.1779805\n16     0.4508883  0.2223145\n33756  1.2866107  0.6269824\n14232 -1.0166493  0.3983281\n25490  1.3144997 -0.7714008\n149    0.9310813 -1.3942383\n29671  0.3357943  0.4989499\n35218  0.5225263 -1.7209132\n23309  0.8328597  1.2089172\n\nsampy\n\n              IV         DV\n5897   0.8485550 -0.2524885\n20003  0.6540785 -0.1779805\n16     0.4508883  0.2223145\n33756  1.2866107  0.6269824\n14232 -1.0166493  0.3983281\n25490  1.3144997 -0.7714008\n149    0.9310813 -1.3942383\n29671  0.3357943  0.4989499\n35218  0.5225263 -1.7209132\n23309  0.8328597  1.2089172\n\n\n\n\nOur ‚ÄúPerfect‚Äù Sample Has ‚ÄúError‚Äù\nOkay, we finally have a perfectly random sample of the population - sampy. If we were researchers, we would use this sample to learn something about what the population is like and, like before, define a linear model to examine the relationship between the DV and IV.\n\nplot(DV ~ IV, data = sampy, pch = 19, main = \"Relationship Between DV and IV in SAMPY\", xlab = \"IV (from sampy)\", ylab = \"DV (from Sampy)\")\nmodS &lt;- lm(DV ~ IV, data = sampy)\nabline(modS, lwd = 5, col = 'green')\n\n\n\n\n\n\n\nround(coef(modS), 5)\n\n(Intercept)          IV \n    0.02525    -0.26201 \n\nsummary(modS)$r.squared\n\n[1] 0.03484331\n\n\nOur sample has error - it is not showing us the true nature of the population.\n\nIndeed, we could ask R to generate 10 random samples of 10 individuals, and see these errors repeat.\n\npar(mfrow = c(2,5))\nfor(i in c(1:20)){\n  sampy &lt;- babby[sample(1:nrow(babby), 10), ]\n  plot(DV ~ IV, data = sampy, pch = 19, main = \"Another Sampy\", xlab = \"IV\", ylab = \"DV\")\n  modS &lt;- lm(DV ~ IV, data = sampy)\n  abline(modS, lwd = 5, col = 'green')\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncoef(modS)\n\n(Intercept)          IV \n 0.69109563 -0.02107053 \n\n\nIn none of these samples does the slope equal exactly zero - which as we know is the ‚Äútrue‚Äù slope of our population. Some may be close, but none are exact - they all have some degree of error. Sampling error. (Whoa.)\n\n\n\nImproving the Quality of These Samples\n‚ÄúBut wait, Professor‚Ä¶‚Äù I can hear you scream in your mind (actively engaged while reading this text), ‚Äúthese are not very good samples! The sample size was only 10! We can get a better estimate of the population if we survey more people!‚Äù\nYes, my dear actively engaged student, you are correct. And yet, there will still be sampling error. We can increase the size of the random sample that R takes by changing the code to define sampy such that it is a random sample of 150 individuals from babby : sampy &lt;- babby[sample(1:nrow(babby), 150), ]\nHere are 10 more linear models from 10 random samples of babby, each with a sample size of 1505.¬†\n5¬†A sample size of 150 may still seem small, but it‚Äôs around the average in fields like social and personality psychology (Fraley et al., 2022) and well above the average in other fields like neuroscience (Button et al., 2013).What do you notice when looking at these different linear models?\n\n## Random Samples: n = 150\npar(mfrow = c(2,5))\nfor(i in c(1:20)){\n  sampy &lt;- babby[sample(1:nrow(babby), 150), ]\n  plot(DV ~ IV, data = sampy, pch = 19, main = \"Big Sampy\", xlab = \"IV\", ylab = \"DV\")\n  modS &lt;- lm(DV ~ IV, data = sampy)\n  abline(modS, lwd = 5, col = 'green')\n  #print(coef(modS))\n  #print(summary(modS)$r.squared)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI notice a few things :\n\nEach sample, as instructed, has 150 individuals in the dataset. I can see there are more dots in each scatterplot. Good to know that R did what I asked.\nNone of the lines appear to have a slope of exactly zero, like our population does. A few are close (the third linear model on the top row has a slope = 0.003 and an R2 = 0.000011 - close to zero but not exaclty zero! there is still some error!!!)\nI can see that there‚Äôs less variation in the steepness of the lines. Some slopes (like top row fourth column) appear steeper than others, but I see less extremes in the variation in these slopes when I have a sample size = 150 than when I have a sample size = 10. (I can see this because the green lines are more similar to each other when n = 150 than when n = 10.)\n\nThis last point is a critical idea : we can ‚Äúvisualize‚Äù sampling error by thinking about how much the slopes from multiple samples differ from each other.\n\nA small amount of sampling error would mean that each sample is very similar to the ‚Äútruth‚Äù of our population. If sampling error is small, we‚Äôd expect to see very little variation in our slopes across different samples.¬†\nA large amount of sampling error would mean that each sample is VERY DIFFERENT from the ‚Äútruth‚Äù of our population. If sampling error is large, we‚Äôd expect to see a lot of variation in our slopes across different samples.\n\nBelow, I illustrate sampling error by asking R to take a random sample from my population (where the ‚Äútrue‚Äù slope = 0), save the value of the slope from this sample, and then repeat this process 1000 times.\nThe graphs below show histograms of these 1000 slopes for a sample size of n = 10 (on the left) and n = 150 (on the right). I‚Äôll walk through the graphs below in more detail, but at first glance you can hopefully see that there‚Äôs less variation in our slopes when the sample size is 150 than when the sample size is 10.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe graph on the left reports the slopes of 1000 samples when the sample size = 10.\nThe graph on the right reports the slopes of 1000 samples when the sample size = 150.\n\n\n\nThe range of these random samples (n = 10) was between -1.56 and 1.46.\nThe standard deviation of these 1000 slopes is .37, which tells us that the ‚Äúaverage‚Äù random sample differs from the mean by about .37.\nThe mean of these 1000 slopes is zero (which is the ‚Äútrue‚Äù population mean!!!).\n\n\nThe range of these random samples (n = 150) was between -0.24 and 0.26.\nThe standard deviation of these 1000 slopes is .08, which tells us that the ‚Äúaverage‚Äù random sample differs from the mean by about .08.\nThe mean of these 1000 slopes is zero (which is the ‚Äútrue‚Äù population mean!!!)\n\n\n\n\n\n\nSPOOKY : Distribution of ‚ÄúRandom‚Äù Sample Estimates Will Alwasy Be Normally Distributed and Centered Over the Population Mean.\nWhen taking a random sample from the population, we always expect to see a ‚Äúnormal‚Äù / symmetrical distribution that is centered over the mean of the population. (This is called the Central Limit Theorem.)\nThe distribution of sample estimates (what happens to the slope if we ask R to take a random sample of the population) will always be normally distributed, since a) each estimate is determined by multiple individuals (‚ÄúThere are multiple explanations for why the variation occurs (life is complex!)‚Äù and b)‚Äúthese multiple explanations occur randomly‚Äù (since R is using very good randomization techniques.)¬†\nTLDR : IF we are drawing random samples from a population, we can expect that the distribution of estimates from multiple samples (the many slopes that we find) will be normally distributed. We care about this because if we can predict what random sampling will look like, it will allow us to estimate the amount of variation caused by random sampling error. And we can then build that estimate of sampling error into our model, and try to estimate how ‚Äúwrong‚Äù our sample might be. To be clear, all of this is made up - a replication is the best thing to do if you really want to know whether your sample is representative of the population. But replications can be expensive in terms of time and money, and the ‚Äúpublish or perish‚Äù beast must be fed.\nIn the next part, you‚Äôll learn about a very common way researchers try to estimate sampling error.",
    "crumbs": [
      "List of R Code",
      "Predicting People",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Sampling Error and Bias</span>"
    ]
  },
  {
    "objectID": "chapters/8R_SamplingError.html#estimating-sampling-error-with-null-hypothesis-significance-testing-nhst",
    "href": "chapters/8R_SamplingError.html#estimating-sampling-error-with-null-hypothesis-significance-testing-nhst",
    "title": "Sampling Error and Bias",
    "section": "Estimating Sampling Error With Null Hypothesis Significance Testing (NHST)",
    "text": "Estimating Sampling Error With Null Hypothesis Significance Testing (NHST)\nThe goal of NHST is to estimate how much sampling error might be influencing the results. Again - we are making things up here - without repeatedly taking samples from the data, we don‚Äôt actually know how much sampling error might be influencing the results. But in theory, it‚Äôs better to have an estimate of sampling error (even if it‚Äôs wrong) than just pretend that our sample is THE TRUTH‚Ñ¢.\n\nNHST : A Written Explanation\n\nNull Hypothesis Significance Testing : An Assumption that the Null Hypothesis is ‚ÄúTrue‚Äù\nNull Hypothesis Significance Testing (NHST) starts with the assumption that the null hypothesis is ‚Äútrue‚Äù. You can remember this important detail by noting the phrase starts with ‚ÄúNull Hypothesis‚Äù.\nIf the null hypothesis is ‚Äútrue‚Äù, then you would not expect to find a relationship between the two variables (since researchers‚Äô theory - the alternative hypothesis - is often predicting that some relationship between two variables exists).¬†\nIn other words, if the null hypothesis were true, you‚Äôd expect to find a slope of zero - no relationship.\nIf you DID NOT find a slope of zero in your study, then there are two possibilities :\n\nYour study found a real relationship between two variables - the null hypothesis is NOT TRUE.\nYour study experienced some sampling error - the null hypothesis IS TRUE, but you drew a random sample from the population that made it seem like there‚Äôs a relationship between these two variables.¬†\n\nIn order to evaluate whether the non-zero slope you found is due to 1) some real relationship between the two variables or 2) sampling error, you need to determine how much sampling error might be influencing your results.¬†\nMany replications would be the best way to do this. As discussed, replications are costly in terms of time and money (paying for participants, grad student salaries, fMRI machine time, etc.)\nSo researchers have developed methods of estimating sampling error. One method is described below.\n\n\nThe ‚ÄúStandard Error‚Äù : Estimating Sampling Error With NHST\nThe ‚ÄúStandard Error‚Äù is the name given to the NHST approach to estimating sampling error. The ‚Äúconceptual‚Äù equation6 to estimate standard error is as follows :\n6¬†¬†This conceptual equation won‚Äôt give you what R calculates, because R will use something called the pooled variance in its calculation of the standard deviation (since the null hypothesis assumes no relationship between the DV and IV, it‚Äôs better to calculate a weighted average of the DV and IV for your estimate of the standard deviation. There‚Äôs a long equation to calculate this that I used to students and have them do ‚Äúby hand‚Äù. However, over the years I realize that this does not help students learn, and that even the complicated equation is an oversimplification of a much more involved proof which you can learn about in more advanced classes if you want.\\[\n\\huge \\text{standard error (SE)} = \\frac{\\sigma}{\\sqrt{n}}\n\\]\nRemember that \\(\\sigma\\) is the standard deviation and \\(n\\) is the sample size. Based on this equation, you can determine that the standard error will increase as the standard deviation increases, or when the sample size decreases. Remember that standard error is an estimate of sampling error; you can think of the standard error like standard deviation. However, whereas standard deviation describes how much the average individual score differs from the mean, the standard error estimates how much the average individual samples might differ from the estimated slope (the slope you found in your study). Again, we are making this up - we don‚Äôt know what the sampling error actually will be, or whether the estimate from our sample is a good estimate of the population. But it‚Äôs all we have.\nSo we expect that we will have MORE sampling error when :¬†\n\n‚Ä¶.our standard deviation increases. A larger standard deviation means that individuals are very different from each other (individual scores differ a lot from the mean). If individuals differ a lot from each other, than which individuals are in our sample will matter more (and create more sampling error) than if everyone was the same. Imagine doing a study of robots who are all programmed to give the same answer - I could study one robot, and probably learn about ALL the robots.\n‚Ä¶.our sample size decreases. The more people we have in our study, the smaller we expect sampling error to be. We saw this when we took larger sample sizes of babby. Asking two students what they think of these notes will give me more error than if I ask half the class, or even better, the entire class.\n\nTo be clear - our estimate of sampling error is ENTIRELY MADE UP. We do not actually know how much sampling error there is in our study - we would need to do multiple replications in order to determine this, which would be costly in terms of time and money. So the standard error is a GUESS about sampling error, and a way to help us better understand the slope that we found.\n\nAs a pretend example. Let‚Äôs say I‚Äôm a researcher who found a slope of .50 with a sample size of 200 and a pooled standard deviation of 2.83. My estimate of sampling error would be .2, which would mean that if the null hypothesis were true, I‚Äôd draw a random sample that would (on average) have a slope of .2 or -.2 (even though the ‚Äútrue‚Äù slope was zero).\n\n\nThe ‚Äút-value‚Äù : Contextualizing the difference between your slope and the null (slope = zero).\nNow that we have our estimate of sampling error (the standard error), we want to compare this value to something. Is this sampling error a lot? A little? We can compare this sampling error to the slope that we found. This comparison is called the t-value, and the equation is below :¬†\n\\[\n\\huge t = \\frac{b}{SE}\n\\]\nAll the t-value does is evaluate how large the slope from your study (b = the slope) is, compared to the ‚Äúaverage‚Äù slope you might find due to random sampling error (se = the standard error).¬†\n\n\n\n\n\n\nPop Quiz. As a researcher hoping to show that your slope is ‚Äúreal‚Äù, would you want the t-value to be LARGE or SMALL?\n\n\n\n\n\nYou would want the t-value to be LARGE, since this means that your slope is a lot bigger than the ‚Äúaverage slope‚Äù you might find due to sampling error if the null hypothesis were true (the standard error).\n\n\n\nThe standard error is critical for evaluating the size of your slope. A researcher could find a very large slope - let‚Äôs say a correlation of r = .6. But if the researcher finds that the sampling error is also high (r = .6), then the slope they found could easily be found due to chance random sampling from a population where the true slope is equal to zero. In other words, the standard error helps researchers evaluate whether the slope they found is different enough from the average sampling error they might find if they were taking samples from a population where the slope was zero.\nAs a pretend example : let‚Äôs say that I‚Äôm a researcher who found a slope of .5, with a standard error of .2. My t-value would be .5 / .2 = 2.5 = my slope is 2.5 times as large as the average slope I might find due to random sampling if the null hypothesis were true.\n\n\nThe ‚Äúp-value‚Äù : estimating the probability of observing a t-value this large (or larger) if the Null Hypothesis Were ‚ÄúTrue‚Äù.\nThe Central Limit Theorem (see above) allows us to expect that our random samples from the population will be normally distributed. As a reminder, the normal distribution describes a range of possible scores that differ from the mean, and assigns a probability to these scores. For example, we can use the chart below to estimate that 50% of the scores will be above the mean, that 13.59 + 2.14 + .13% = 15.86% of scores will be above 1 standard deviation, that 64.26% of scores will be between -1 and +1 standard deviations of the mean, that .13% of scores will be below 4 standard deviations of the mean, and so on.\nNote that the graph below reports percentages. But it is also common to report these numbers as percentiles. So a 34.13% would be written as a percentile as .3413.\n\n\nThe p-value, then, describes the probability that you would observe a t-value as large (or larger) than the one that you observed if the null hypothesis were true. Practically, the p-value is the percentage of scores that fall further away from the mean from your t-value. A few examples, using the chart above¬† :\n\nA t-value of -3 would have a p-value of .0013, which means that there is a .13% chance that you would observe a t-value of -3 or more extreme if the null hypothesis were true.\nA t-value of -1 would mean that our slope is as large as the sampling error that we found (the negative sign just means that our slope is negative), and would have a p-value of .1586. This means that we would expect 15.86% of random samples to be as large (or larger) than the slope that we found.¬†\nA t-value of +2 means that our slope is twice as large as the sampling error that we found, and would have a p-value of .0227, which means that we would expect 2.27% of random samples to be as large (or larger) than the slope that we found.¬†\n\nWhen R reports the p-value, it‚Äôs actually calculating the p-value for both sides of the distribution. That is, if your slope had a t-value of 3-standard errors below the mean, R would report a p-value of .026 or 2.6%, since it‚Äôs calculating the probability of the curve that falls to the left of 3 standard deviations below the mean, and also the probability of the curve that falls to the right of 3 standard deviations above the mean.\nThis is called a two-tailed test, and the logic of doing this by default is :¬†\n\nIf we are really pretending to live in the world of the null hypothesis, then the slope we found in our sample is random. And if it‚Äôs random, we could have found a slope this extreme in either the positive OR negative direction. So when estimating the probability of this ‚Äúrandom‚Äù slope occurring, it‚Äôs good to¬†\nA two-tailed test is more conservative, since you are doubling the p-value which increases the probability you report of finding a random slope due to chance. It‚Äôs good practice to be more conservative when reporting statistics, since people are biased to find results in their favor.¬†\n\nSome researchers report a one-tailed test - in R, you would just divide the reported p-value in R in half. The logic in doing this is that researchers claim if they have a strong prediction that their result will be positive (or negative) in direction, they can ignore the other side of the distribution. This has never made sense to me, since NHST is about random sampling (and not the alternative hypothesis) and I‚Äôm always a little suspicious of doing things that make it easier to show your hypothesis was supported.\n\nBack to my pretend example : with a slope of .5, a standard error of .2, and a t-value of 2.5, R would report my p-value (two sided test) to be p = 0.0132. This means there is a 1.3% chance that I would observe a slope as different from zero as .5 (in either a positive or negative direction) due to random sampling error if the null hypothesis were true. This is a low probability! It is unlikely to happen due to random chance (if the null hypothesis were true). And I could say that it was a significant effect.\n\n\nA ‚ÄúSignificant‚Äù Effect\nOkay, time to bring this together. IF the null hypothesis is true, then we expect to find a slope of zero. But we also know that we may not find a slope of zero all the time because of random sampling error. So the question is - how do we know whether our non-zero slope that we found in OUR study is due to sampling error (from a population where the true slope is zero), OR is due to some REAL RELATIONSHIP between the two variables????¬†\nThe answer is that statisticians made up an arbitrary threshold of 5% (or a p-value of .05). If the probability of observing your slope, relative to your estimate of sampling error, is less than 5%, then it means it is UNLIKELY that you observed this slope because of taking some random sample from a population where the slope is zero (sampling error) and MORE LIKELY that you observed this slope because you took a random sample from a population where the slope is not zero.¬†\nIn other words, as a researcher looking to support your theory, you ‚Äúneed‚Äù to find that the p-value is small, because this is the probability that you observed your slope IF THE NULL HYPOTHESIS were ‚Äútrue‚Äù. When this happens, researchers say the effect is ‚Äústatistically significant‚Äù.¬†\nHowever - we need to be careful here! A ‚Äúsignificant effect‚Äù only means that our made-up estimate of sampling error is much smaller than the slope that we observed in our study, and that this gives us a made-up sense of confidence that we can reject the null hypothesis and say that the data supports our theory.\nA significant effect DOES NOT mean :¬†\n\nthe effect we found in our study is real. we don‚Äôt have access to the truth, and are still making a guess about what the population is like based on our limited sample. so just because an effect is significant doesn‚Äôt make it real.\nthe effect we found in our study is not due to chance. there is still a chance that we are making a mistake in our estimate of sampling error. and even a small p-value of .000001 means there is a chance that we observed this slope due to random sampling error7.\nthe effect we found in our study is important. a ‚Äúsignificant effect‚Äù doesn‚Äôt mean the effect is important or meaningful. That‚Äôs a question for your critical thinking brain : are these results important to society or me? How will this knowledge be used?\nthe study used valid methodology. a bad study designed with poorly defined and unreliable measures, a biased sample, or an improper experimental method that is ‚Äúsignificant‚Äù is still a bad study.¬†\n\n7¬†¬†This is called Type I error (a false positive) - when we incorrectly reject the null hypothesis. The probability of committing Type I error is defined by the p-value you set to reject the null hypothesis, typically .05 = 5%. Type II error is when you incorrectly reject the alternative hypothesis (false negative). we won‚Äôt cover how to calculate Type II error this semester (or the related concept of Statistical Power - which is an estimate of the probability you can correctly support your theory as a researcher), but they are important ideas. Here‚Äôs an okay video that seems to give a good overview of these types of error for those students who are interested, and here‚Äôs a video walking through how to do this in R. Let me know if you have questions / find better videos!So what does a significant effect mean again? A significant effect means : ‚ÄúWe are unlikely to observe the slope we found in our sample (or one more extreme) if we were to draw a random sample from a population under conditions of the null hypothesis.‚Äù\n\n\nThe 95% Confidence Interval\nTo help ensure that the reader understands the slope found in the study is not the absolute truth, but instead is an estimate that might change from sample to sample, psychologists will often report the slope that they found, along with a range of slopes based on their expected sampling error. If you‚Äôve ever seen a political poll reporting a ‚Äúmargin of error‚Äù, this is an estimate of sampling error.\nTypically, researchers report the 95% Confidence Interval - this defines 95% of the distribution of sample estimates they might expect if they were to experience sampling error around the slope that they found. The 95% confidence interval is an area around the estimate of the original slope, and is defined by the following equation.\n\\[\n\\huge \\text{95% Confidence Interval} = b\\pm 1.96 * SE_b\n\\]\nPositive and Negative 1.96 are the values of the normal distribution that contain 95% of the scores8. Multiplying this value by the estiamted standard error of the slope (which describes how much the slope might vary due to random sampling error) translates these boundaries into the same units as your slope. You can visualize the 95% Confidence Interval below.\n8¬†The exact range of the 95% confidence interval will not always be within ¬±1.96 standard errors of the slope - that‚Äôs the theoretical range based on a normal distribution. In practice, the range will depend on certain features of your sample and model (much like the t-distribution we discussed), but will approach 1.96 as your sample size increases; and if you want to be more conservative, you can round up to 2*SEb as long as your sample size is &gt; 30.\n\n\nSo, back to my pretend example. With a slope of .5, a standard error of .2, and a t-value of 2.5, and a p-value of 0.0132, my 95% confidence interval would be calculated as :¬†\n\n.5 + 1.96*.2 = .892 = this is the upper limit of my estimated 95% confidence interval\n.5 - 1.96*.2 = .108 = this is the lower limit of my estimated 95% confidence interval\n\nNote that a ‚Äúsignificant effect‚Äù should show the upper and lower limits in the same direction as your original slope. So if your slope is positive, the 95% confidence interval should contain all positive numbers. If the 95% confidence interval contains zero (or a mix of positive and negative numbers) than the effect is not considered statistically significant.\nI would then write this as : ‚ÄúI observed a significant and positive relationship between the DV and IV (b = .50, 95% CI = [.11, .89], t(200) = 2.5, p (two-tailed) = .0132).‚Äù¬†\nWHEW!!!\n\n\n\nCalculating NHST in R : Very Easy!\nY‚Äôall will like this - it is very easy to do all the NHST stuff in R. First, you start with a linear model. I‚Äôm going to draw from the MBA study we‚Äôve used before, and examine the relationship between Narcissism (NPI) and Age (age).\n\nmba &lt;- read.csv(\"~/Dropbox/!WHY STATS/Chapter Datasets/hormone_data.csv\", stringsAsFactors = T)\nage.mod &lt;- lm(NPI ~ age, data = mba)\npar(mfrow = c(1,1))\nplot(NPI ~ age, data = mba, pch = 19, xlab = \"Age\", ylab = \"Narcissism (NPI)\")\nabline(age.mod, lwd = 5, col = 'red')\n\n\n\n\n\n\n\n\nIf I look at the coefficients of the model :\n\ncoef(age.mod)\n\n(Intercept)         age \n 4.74855460 -0.05651782 \n\n\nSomeone with an age of 0 is predicted to have a Narcissism score of 4.7. These data don‚Äôt make sense, since our data were only collected on adults.\nFor every year someone ages, we predict their narcissism to go down by .0565. I can see this as negative slope I drew on the graph.\n\nTo do all the NHST stuff, all I need to run is the summary() function on the model that I defined; in this case I would run summary(age.mod) and get the following output :\n\nsummary(age.mod)\n\n\nCall:\nlm(formula = NPI ~ age, data = mba)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.25302 -0.34583  0.00417  0.27644  1.41394 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.74855    0.61722   7.693  7.8e-12 ***\nage         -0.05652    0.02217  -2.549   0.0122 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5219 on 106 degrees of freedom\n  (14 observations deleted due to missingness)\nMultiple R-squared:  0.05778,   Adjusted R-squared:  0.04889 \nF-statistic:   6.5 on 1 and 106 DF,  p-value: 0.01222\n\n\n\nformula : hey, that‚Äôs my model!\nresiduals : these describe some descriptive statistics for the residuals in my model.\ncoefficients : these will report the intercept and slope, along with the NHST statistics for these terms. You can ignore the intercept for now and just focus on the columns for the slope (age) :¬†\n\nage (estimate) : = - .057 = this is the estimate of the slope for age. There‚Äôs a negative relationship between these two variables.\nstd. error = .02217 = this is my estimate of the standard error (sampling error) for age. If the null hypothesis were true and the ‚Äútrue slope‚Äù was 0, I‚Äôd expect to find an average slope of .02 or -.02 just due to sampling error.\nt-value = -2.55 = my slope is 2.55 times larger than the standard error. I can confirm that -.057 / .02217 = 2.55. Note that when reporting the t-value, it‚Äôs also important to look up the degrees of freedom (which is the sample size - the number of things in your model.) R reports these degrees of freedom next to the residual standard error.\np-value = 0.0122 = if the null were true, I‚Äôd expect to sample a slope of -.057 (or +.057) or more extreme about 1.22% of the time. This is a low probability and less than .05 (or 5%) so I get my little star and can say that I reject the null hypothesis - the relationship between age and narcissism is unlikely to be due to random sampling error from a null distribution.\n95% Confidence Interval : this is not calculated, but I can do a rough estimate by taking my slope (-.057) and adding and subtracting 1.96 * .02, so the 95% CI = [-0.0962, -0.0178]. Note that this range does not include zero, so it‚Äôs another way of showing the effect is significant.\n\\(R^2\\): hey look, it‚Äôs our good friend R2! Go ahead and focus on the ‚Äúmultiple R-squared‚Äù = .05778.\n\n\nI would write this up as the following : ‚ÄúI observed a small but significant negative relationship between age and narcissism (b = -0.05, 95% CI = [-0.0962, -0.0178], R2 = .06, t(106) = -2.55, p = .01).\n\n\nNHST : Video Explanation\nFor the YouTube fans.",
    "crumbs": [
      "List of R Code",
      "Predicting People",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Sampling Error and Bias</span>"
    ]
  },
  {
    "objectID": "chapters/8R_SamplingError.html#check-out-statistical-significance",
    "href": "chapters/8R_SamplingError.html#check-out-statistical-significance",
    "title": "Sampling Error and Bias",
    "section": "Check-Out: Statistical Significance",
    "text": "Check-Out: Statistical Significance\nThis shit is confusing, so here are some questions on NHST to assess your understanding. We will review answers to these in our next lecture, so save your work so we can review!!!\nThanks for reading! Lemme know if you have questions on Discord :)",
    "crumbs": [
      "List of R Code",
      "Predicting People",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Sampling Error and Bias</span>"
    ]
  },
  {
    "objectID": "chapters/9R_ModelExtensions.html",
    "href": "chapters/9R_ModelExtensions.html",
    "title": "More Levels and Experiments",
    "section": "",
    "text": "Part 1 : The Linear Model with a Categorical IV‚Ä¶with THREE+ Levels\nIn Chapter 7, we showed how the linear model could be adapted to predict a numeric DV when the IV was a categorical variable with two levels. And guess what? When the IV has more than three levels, we can use the same linear model. Hooray! The only difference is now you have multiple levels that differ from the intercept (or ‚Äúreference group‚Äù).\nLet‚Äôs mix it up, and see if some good ol‚Äô fashioned YOUTUBE VIDEOS OF A DISEMBODIED VOICE can explain this.",
    "crumbs": [
      "List of R Code",
      "Predicting People",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>More Levels and Experiments</span>"
    ]
  },
  {
    "objectID": "chapters/9R_ModelExtensions.html#part-1-the-linear-model-with-a-categorical-ivwith-three-levels",
    "href": "chapters/9R_ModelExtensions.html#part-1-the-linear-model-with-a-categorical-ivwith-three-levels",
    "title": "More Levels and Experiments",
    "section": "",
    "text": "Video 1 : Defining the Model with the lm() function\n\nhere‚Äôs a link to R Script (as used in the video)\n\n\n\npar(mfrow = c(1,2))\nhist(presto$prestige)\nplot(presto$type)\n\n\n\n\n\n\n\nmod &lt;- lm(prestige ~ type, data = presto)\nmod\n\n\nCall:\nlm(formula = prestige ~ type, data = presto)\n\nCoefficients:\n(Intercept)     typeprof       typewc  \n     35.527       32.321        6.716  \n\n\n\n\nVideo 2 : Interpreting the Model\n\n\nThe Linear Model Equation:\n\ncoef(mod)\n\n(Intercept)    typeprof      typewc \n  35.527273   32.321114    6.716206 \n\n\n\n\nUsing the Equation and Dummy Coding to Generate Predicted Values\n\n\n\n\n\n\n\n\n\n\n\n\n(the intercept)\n35.5\nX1\n(typeprof)\n32.3\nX2\n(typewc)\n6.7\nCalculations\nPredicted Value\n\n\nbc\n1\n0\n0\n35.5 + 32.3*0 + 6.7*0\n= 35.5\n\n\nprof\n1\n1\n0\n35.5 + 32.3*1 + 6.7*0\n= 67.8\n\n\nwc\n1\n0\n1\n35.5 + 32.3*0 + 6.7*1\n= 42.2\n\n\n\n\n\nYes, Professor, A Picture is Worth‚Ä¶.1000 Words.\n\npar(mfrow = c(1,2))\nplot(presto$prestige, col = presto$type, pch = 19)\nabline(h = coef(mod)[1] + coef(mod)[2], col = 'red', lwd = 5)\nabline(h = coef(mod)[1] + coef(mod)[3], col = 'green', lwd = 5)\nabline(h = coef(mod)[1], col = 'black', lwd = 5) # blue collar\n\nplotmeans(prestige ~ type, data = presto, connect = FALSE)\n\n\n\n\n\n\n\n\n\n\n\nVideo 3 : Releveling the Variable\n\n\npresto$typeR &lt;- relevel(presto$type, ref = \"prof\")\nmod2 &lt;- lm(prestige ~ typeR, data = presto)\nplotmeans(prestige ~ typeR, data = presto, connect = FALSE)\n\n\n\n\n\n\n\ncoef(mod2)\n\n(Intercept)     typeRbc     typeRwc \n   67.84839   -32.32111   -25.60491 \n\n\n\n\nVideo 4 : \\(R^2\\) for a Categorical Model\n\n\npar(mfrow = c(1,2))\nplot(presto$prestige, main = \"The Mean as Our Prediction\")\nabline(h = mean(presto$prestige), )\n\nplot(presto$prestige, col = presto$type, pch = 19, main = \"The Model (Job Type) As Our Prediction\")\nabline(h = coef(mod)[1] + coef(mod)[2], col = 'red', lwd = 5)\nabline(h = coef(mod)[1] + coef(mod)[3], col = 'green', lwd = 5)\nabline(h = coef(mod)[1], col = 'black', lwd = 5) # blue collar",
    "crumbs": [
      "List of R Code",
      "Predicting People",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>More Levels and Experiments</span>"
    ]
  },
  {
    "objectID": "chapters/9R_ModelExtensions.html#check-in-to-assess-your-understanding-here-no-r-required",
    "href": "chapters/9R_ModelExtensions.html#check-in-to-assess-your-understanding-here-no-r-required",
    "title": "More Levels and Experiments",
    "section": "CHECK-IN TO ASSESS YOUR UNDERSTANDING HERE [no r required]",
    "text": "CHECK-IN TO ASSESS YOUR UNDERSTANDING HERE [no r required]",
    "crumbs": [
      "List of R Code",
      "Predicting People",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>More Levels and Experiments</span>"
    ]
  },
  {
    "objectID": "chapters/9R_ModelExtensions.html#part-2-experimental-methods",
    "href": "chapters/9R_ModelExtensions.html#part-2-experimental-methods",
    "title": "More Levels and Experiments",
    "section": "Part 2 : Experimental Methods",
    "text": "Part 2 : Experimental Methods\nLast chapter, we learned the ‚Äúfour reasons‚Äù why you might find a ‚Äúpattern in the data‚Äù (another way of saying why ‚Äúcorrelation does not equal causation‚Äù. However, scientists and psychologists often want (or need) to establish causation, and an experiment is the ‚Äúgold standard‚Äù approach for how to do this.\nScientists and psychologists often want (or need) to establish causation, and an experiment is the ‚Äúgold standard‚Äù approach for how to do this.\nI‚Äôm guessing y‚Äôall have learned about experiments in many other classes, so won‚Äôt spend pages and pages and pages going over it again here.1\n1¬†I‚Äôll just spend pages going over it. Hah hah.Here‚Äôs a supplemental chapter on experiments that reviews some key terms you should be familiar with : experimental manipulation; extraneous variables as ‚Äúnoise‚Äù; random assignment; placebo effects; external validity; construct validity; experimenter expectancy effects.\n\nThe Definition of Causality\n\nThe cause and effect are contiguous in space and time.\nThe cause must be prior to the effect. (no reverse causation)\nThere must be a constant union betwixt the cause and effect. (‚ÄúTis chiefly this quality, that constitutes the relation.‚Äù) (no random chance)\nThe same cause always produces the same effect, and the same effect never arises but from the same cause. (not ‚Äújust‚Äù some third variable) [^1]\n\n[^1] but remember, life is complex and there are often multiple causes of human behavior!\n\n\nThe Google Experiment\nWhen I worked at Google as an intern for one summer2, corporate PR told a story about their ‚Äúdata driven approach‚Äù, where no decision was left to mere chance. Even something as simple as the color font they used could be the focus of an analytic research question.\n2¬†well-fed; gratuitously paid; soul-drained. I was in their ‚ÄúPeople Operations‚Äù (/HR) department, helping them set up longitudinal surveys and do various other research projects on compensation, appreciation, and diversity that helped feed the corporate beast.\n\n\n\n\n\n\nGoogle Homepage : Before Experiments\nGoogle Homepage : After Experiments\n\n\n\n\n\n\n\nWatch the video below. You can read more about this study here.\n\n\n\n\n\n\nThen, see if you can identify the different parts of an experiment.\n\nlinear model : DV ~ IV (what was manipulated) + confound variables + error\nmanipulation : what were the treatment and control groups?\nrandom assignment : how were confound variables balanced across conditions?\ndouble-blind : did the study avoid demand characteristics & placebo effects?\ngeneralizability : did the study have external validity? what was the effect size (R2)?\nethics : should researchers do this type of study [predict & control]\n\n\n\n\n\n\n\nAnswers to Google Shade of Blue Experiment\n\n\n\n\n\n\nlinear model : number of ads that peopled clicked on ~ shade of blue people saw + age + location + income + education + media literacy + relevance of the ad + etc + error\nmanipulation : ‚Äúcontrol‚Äù group = shade of blue (existing shade of blue); treatment group = shade of blue.\nrandom assignment : participants were randomly assigned to see links in one shade of blue or another. because this was randomly assigned, all the other differences between people (those variables that might affect the DV, such as age or location) are ‚Äúbalanced out‚Äù.\ndouble-blind : the participant didn‚Äôt know that google was manipulating the shade of blue to get them to click on ads. the website (google) doesn‚Äôt know whats going on.\ngeneralizability : yes, high external validity! Google was changing its own website. an example of low external validity would be printing out a sheet with different shades of blue that users ‚Äúclick‚Äù on with their finger.\nethics : WOULD LOVE TO HEAR FROM Y‚ÄôALL : do you think it‚Äôs ethical for companies like Google to do this research???",
    "crumbs": [
      "List of R Code",
      "Predicting People",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>More Levels and Experiments</span>"
    ]
  },
  {
    "objectID": "chapters/10R_MultipleRegression.html",
    "href": "chapters/10R_MultipleRegression.html",
    "title": "Multiple Regression",
    "section": "",
    "text": "The Purpose of Multiple Regression\nKey Questions : What is redundant covariation and how does this relate to the idea of a 3rd variable?\n‚ÄúThe Whole is Greater Than the Sum of Its Parts‚Äù\nI like the above work by Max Ernst; like all evocative art it manages to transcend the individual brush strokes and colors to express a sense of terror. But if you were to isolate each line and color into its separate components - canvas; blue; brown; red - the sum of those different parts wouldn‚Äôt equal the work as a whole.\nThe idea behind a multiple regression is the same - by combining two separate independent variables into one integrated model, researchers can understand more about the phenomenon than they would understand if they just examined the results of two separate bivariate models.\nSpecifically, there are two main benefits to the multiple regression :\nThis first benefit to a multiple regression may sound familiar; we talked about it (briefly) in the beginning of the semester in the context of confound vs.¬†control variables.\nConfound vs.¬†Control Variables\nThe basic idea was that many variables explain human life, and it‚Äôs hard to know whether the relationships we find between one independent variable and the dependent variable are specific to that one independent variable, or whether there‚Äôs some other variable that might be influencing the relationship. We talked about this earlier in the semester when we talked about the idea behind a confound variable - a variable that is not part of your model, but would influence the results. The classic example is that ice cream sales are related to murder rates. Of course, ice cream does not really contribute to people getting murdered. There‚Äôs a confound variable - heat - that is related to both how much ice cream is sold (the more heat, the more people eat ice cream) and murder (the hotter it is, the more likely people are to kill each other‚Ä¶apparently.) And if you account for that confound variable by including it in your model, then you‚Äôd find that the relationship between ice cream and murder goes away‚Ä¶it‚Äôs explained by the relationship between heat and murder.\nRecap : The Principle of Covariation\nOur goal with multiple linear regression is the same goal we‚Äôve been working with so far this semester - to explain complexity. To achieve this goal, we‚Äôve been relying on something that I think of as the ‚Äúprinciple of covariation‚Äù. With a bivariate linear regression (what we‚Äôve been doing so far this semester), this involves explaining variation on one psychological dimension (the dependent variable) based on variation on another psychological dimension (the independent variable). In other words, making predictions about Y based on information in X.\nWe wrote out this bivariate model like so :\n\\[\\huge y_i = a + b_1*X_{1i} + {\\epsilon}_i\\]\nTo make sure we are all on the same page1, let‚Äôs recap what‚Äôs going on with this model. On the left-hand side, we are making predictions for some variable (y : the dependent variable) - specifically an individual‚Äôs score on that variable (yi : the tiny i represents some specific individual in the dataset). On the right-hand side of the equation, we have a starting place for our predictions (a : the intercept), and then a slope (b1) that describes how we adjust our predictions of Y based on the individual‚Äôs specific value of some other variable (X1i : the first independent variable). And since human life is complex, our predictions will not be perfect, but will have error (ei) which we define as the difference between the individual‚Äôs actual score (yi) and our predictions for that person‚Äôs score (a + b1X1i). Neat.\nSo far, we‚Äôve used the linear model to make predictions of continuous variables based on categorical variables. For example, in the Prestige dataset (download HERE) we predicted prestige from job type (a categorical factor with three levels - blue collar, professional, and white collar).\nlibrary(car)\n\nLoading required package: carData\n\nlibrary(gplots)\n\n\nAttaching package: 'gplots'\n\n\nThe following object is masked from 'package:stats':\n\n    lowess\n\nmod1 &lt;- lm(prestige ~ type, data = Prestige)\nplotmeans(prestige ~ type, data = Prestige, connect = F, ylab = \"Prestige\", xlab = \"Job Type\")\n\n\n\n\n\n\n\nsummary(mod1)\n\n\nCall:\nlm(formula = prestige ~ type, data = Prestige)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-18.2273  -7.1773  -0.0854   6.1174  25.2565 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   35.527      1.432  24.810  &lt; 2e-16 ***\ntypeprof      32.321      2.227  14.511  &lt; 2e-16 ***\ntypewc         6.716      2.444   2.748  0.00718 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.499 on 95 degrees of freedom\n  (4 observations deleted due to missingness)\nMultiple R-squared:  0.6976,    Adjusted R-squared:  0.6913 \nF-statistic: 109.6 on 2 and 95 DF,  p-value: &lt; 2.2e-16\nWe also used this linear model to make predictions of continuous variables (e.g., prestige) based on other continuous variables (e.g., education).\nmod2 &lt;- lm(prestige ~ education, data = Prestige)\nplot(prestige ~ education, data = Prestige, xlab = \"Years of Education\", ylab = \"Prestige\")\nabline(mod2, lwd = 5, col = 'red')\n\n\n\n\n\n\n\nsummary(mod2)\n\n\nCall:\nlm(formula = prestige ~ education, data = Prestige)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-26.0397  -6.5228   0.6611   6.7430  18.1636 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -10.732      3.677  -2.919  0.00434 ** \neducation      5.361      0.332  16.148  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.103 on 100 degrees of freedom\nMultiple R-squared:  0.7228,    Adjusted R-squared:   0.72 \nF-statistic: 260.8 on 1 and 100 DF,  p-value: &lt; 2.2e-16\nRedundant Covariation\nWe could go through life (and statistics) by considering simple relationships between two variables. These kinds of bivariate models are a good (and necessary) place to start our inquiry. But no psychological phenomenon is entirely explained by any one variable. Instead, human life is explained by multiple variables that are often related to each other and sometimes work together2 in order to influence behavior. We saw this in the example above, where prestige appears to be separately explained both by job type and education. But the situation gets more complicated, because as a quick test in R reveals, job type and education are also related to each other.\nmod3 &lt;- lm(education ~ type, data = Prestige)\nsummary(mod3)\n\n\nCall:\nlm(formula = education ~ type, data = Prestige)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.99419 -0.80932  0.08947  0.61392  2.57068 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   8.3593     0.1800  46.446  &lt; 2e-16 ***\ntypeprof      5.7249     0.2799  20.450  &lt; 2e-16 ***\ntypewc        2.6624     0.3072   8.667 1.16e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.194 on 95 degrees of freedom\n  (4 observations deleted due to missingness)\nMultiple R-squared:  0.8153,    Adjusted R-squared:  0.8114 \nF-statistic: 209.6 on 2 and 95 DF,  p-value: &lt; 2.2e-16\n\nplotmeans(education ~ type, data = Prestige, connect = F, xlab = \"Job Type\", ylab = \"Years of Education\")\nCompare this output (and graph) to the output (and graph) where we predicted prestige from job type. Spoiler alert - the effects are pretty similar. Blue collar workers are predicted to have the lowest level of education (8.35 years), professionals have 5.72 years of education more than blue collar workers (a significant difference), and white collar workers have 2.66 years of education more than blue collar workers (also a significant difference).\nThat job type, education, and prestige are all related to each other represents redundant covariation and raises an important possibility - the relationship between job type and prestige could really be due to the relationship between education and prestige. That is, since jobs with more education are rated as having more prestige, and blue collar workers have the least education (on average), the reason why blue collar jobs have the least amount of prestige could be better explained by differences in education. Similarly, professionals might be rated as having the most prestige because they have the most education. Of course, it‚Äôs also possible that education is related to prestige because of the type of job that workers have.\nWith separate bivariate models, it‚Äôs impossible to know whether these two independent variables are uniquely related to prestige or whether one variable is more related to prestige than another because we are only looking at one relationship at a time. The multivariate regression - one that predicts prestige simultaneously from both education and job type - will help sort out this issue.",
    "crumbs": [
      "List of R Code",
      "People Are Complex",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "chapters/10R_MultipleRegression.html#the-purpose-of-multiple-regression",
    "href": "chapters/10R_MultipleRegression.html#the-purpose-of-multiple-regression",
    "title": "Multiple Regression",
    "section": "",
    "text": "Examining the unique effects of variables : the first benefit of a multiple linear regression is that we can test whether the effect of one variable (IV1) on another (DV) is unique to that variable, or whether that relationship is also explained by some other variable (IV2). We‚Äôll talk more about this idea throughout this document.\nExamining the dependent effects of variables : another benefit of the multiple linear regression is that we can examine whether the relationship between one independent variable (IV1) and the dependent variable (DV) depends on another variable (IV2). This idea is called an interaction effect - we will talk about this in a separate lecture.\n\n\n\n\n\n\n\n\n\n1¬†since this is written. har har.\n\n\n\n\n\n2¬†we‚Äôll talk about this point (an interaction effect) in the next set of lecture notes.",
    "crumbs": [
      "List of R Code",
      "People Are Complex",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "chapters/10R_MultipleRegression.html#multivariate-linear-regression-a-more-complex-model",
    "href": "chapters/10R_MultipleRegression.html#multivariate-linear-regression-a-more-complex-model",
    "title": "Multiple Regression",
    "section": "Multivariate Linear Regression : A More Complex Model",
    "text": "Multivariate Linear Regression : A More Complex Model\nIn order to better understand the complexity of human life, we‚Äôll need to define a more complex model that explains some dependent variable as a function of multiple independent variables. This is called a multivariate (multiple variables) linear regression model, or a multiple regression for short. In a multiple linear regression, we are going to use‚Ä¶wait for it‚Ä¶multiple independent variables to make predictions about the dependent variable.¬†\nWe‚Äôll now write our model out like this :\n\\[\\huge y_i = a + b_1*X_{1i} + b_2*X_{2i} + ... + b_k*X_{ki} + {\\epsilon}_i\\]\nAt first, this model may look scary, but it‚Äôs actually quite familiar - we are making predictions about some specific individual‚Äôs score on some specific variable (y), but now are using information in multiple variables (X1 and X2) to update our predictions of y. The ‚Äôk‚Äôs indicate that you can build a model with as many predictors as you want (this will create complications that we‚Äôll talk about later‚Ä¶).\nBelow is a model where we predict prestige from both education and job type. Spend a few minutes looking at this output, and see if you can interpret it. (Don‚Äôt worry, we‚Äôll get to the NHST stuff later.)\n\nmod4 &lt;- lm(prestige ~ education + type, data = Prestige)\ncoef(mod4)\n\n(Intercept)   education    typeprof      typewc \n  -2.698159    4.572793    6.142444   -5.458495 \n\n\nAlright, hopefully you felt like a lot of the output from this multivariate linear regression model was somewhat familiar. If not, that‚Äôs okay‚Ä¶the good news is that much of this output can be interpreted in the SAME WAY that we interpreted the output from a bivariate relationship.¬†\nComponents of the Multivariate Linear Regression\nThis more complex model is comprised of the following components :¬†\n\nthe intercept : I say intercept, you say‚Ä¶.3 The intercept in this model describes the predicted prestige for someone with zero years of education, who is NOT a professional, and is NOT a white collar worker - in other words an uneducated blue collar worker. Because no one in the sample actually had less than six years of education, an education of zero does not make sense, which is why the prestige is below zero.\nthe slope of education : this describes the relationship between education and prestige - for every year of education, we add 4.6 prestige points. Critically, this is the relationship between education and prestige controlling for the effect of job type (the unique effect of education.)\nthe slope of professional : this coefficient describes the change in our predictions between the reference level (blue collar workers - defined by the intercept) and professionals, controlling for the effect of education (the unique effect of being a professional worker). When accounting for the effect of education in the model, professionals have 6.1 more prestige points than blue collar workers.\nthe slope of white collar : this coefficient describes the change in our predictions between the reference level (blue collar workers - defined by the intercept) and white collar workers, controlling for the effect of education. When accounting for the effect of education in the model, white collar workers are predicted to have 5.6 less prestige points than blue collar workers.\n\n3¬†THE PREDICTED VALUE OF Y WHEN ALL X VALUES ARE ZEROWe can use this model to make predictions for specific individuals : ≈∂ ~ -2.7 + 4.6(Xeducation) + 6.1(Xprof) - 5.5 (Xwc)\nPop Quiz : Use this model to calculate the predicted values of Y for‚Ä¶.\n\na blue collar worker with 10 years of education?\na white collar worker with 12 years of education?\na professional worker with 12 years of education?\n\nHere are the answers. No peeking!4\n4¬†43.3, 47, and 58.6.\nComparing Models : The Regression Table.\nBecause your multivariate model will contain two different independent variables, it‚Äôs likely that the scale of your two IVs will be different. To assist in comparing the change between models, or comparing the slopes within one multivariate model, it‚Äôs important to scale (standardize or z-score) your variables.\nWe did this before, when we used the scale function to transform each variable in the model.\nBut there‚Äôs a better way to do this, AND to export our analyses. All in one handy-dandy table.\nIf you are reading this far, go ahead and try a new function - export_summs() from the jtools library. You‚Äôll need to install this new package install.packages(\"jtools\"), and then load this to your library.\nWe can use the export_summs() function to export the summary of multiple models, and then ask R to transform both the response (DV) and IVs.\n\n# install.packages(\"jtools\")\nlibrary(jtools)\nexport_summs(mod1, mod2, mod4, transform.response = T, scale = T, center = T, binary.factors = T)\n\n\n\nModel 1Model 2Model 3\n\n(Intercept)-0.69 ***0.00¬†¬†¬†¬†-0.04¬†¬†¬†¬†\n\n(0.08)¬†¬†¬†(0.05)¬†¬†¬†(0.12)¬†¬†¬†\n\ntypeprof1.89 ***¬†¬†¬†¬†¬†¬†¬†0.36¬†¬†¬†¬†\n\n(0.13)¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†(0.25)¬†¬†¬†\n\ntypewc0.39 **¬†¬†¬†¬†¬†¬†¬†¬†-0.32 *¬†¬†\n\n(0.14)¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†(0.16)¬†¬†¬†\n\neducation¬†¬†¬†¬†¬†¬†¬†0.85 ***0.74 ***\n\n¬†¬†¬†¬†¬†¬†¬†(0.05)¬†¬†¬†(0.11)¬†¬†¬†\n\nN98¬†¬†¬†¬†¬†¬†¬†102¬†¬†¬†¬†¬†¬†¬†98¬†¬†¬†¬†¬†¬†¬†\n\nR20.70¬†¬†¬†¬†0.72¬†¬†¬†¬†0.80¬†¬†¬†¬†\n\nAll continuous variables are mean-centered and scaled by 1 standard deviation.  *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.\n\n\n\n\nWhat‚Äôs lovely about this kind of table is we can look across each row, and see how the coefficients for each variable change across our models. Remember - the reason we were doing a multivariate regression is to see how the bivariate relationships between the independent and dependent variables change when you account for redundant covariation.\n\n\nTypes of Changes Between the Multivariate and Bivariate Models\nThere are three different ways that the relationships can change from the bivariate to multivariate model - I‚Äôm simplifying these concepts a bit for the sake of this introductory class. Links to more information if you want to learn more about the precise definition and application of the terms.¬†\n\nIndependent Effects : The relationship between IV1 and the DV does not substantially change when IV2 is included in the model. This means that the relationship between IV1 and the DV is not explained by IV2. If your goal is to show that there is a significant relationship between IV1 and the DV, and that relationship does not go away when you include IV2 in your model, then this is what you are hoping to find.\nMediation Effects : The relationship between IV1 and the DV is weakened (which is called partial mediation) or goes away completely (which is called full mediation) when IV2 is included in the model. This means that the relationship between IV1 and the DV is dependent on the other variable. You can read more about this here.¬†\nSuppressor Effect : The relationship between IV1 and the DV is strengthened (or changes direction) when IV2 is included in the model. You can read more about this here.¬†\n\n\n\n\n\n\n\nACTIVITY : Look over the models. How do the slopes change from Model 1 to Model 3? From Model 2 to Model 3?\n\n\n\n\n\nThe difference (slope) between professionals and blue collar workers that we saw in Model 1 (√ü = 1.89) goes down and becomes non-significant (√ü = .36; this is called mediation). The the difference (slope) between white collar and blue collar workers in Model 1 (√ü = .39) changes in direction in Model 3 to become (√ü = -.32; this is called a suppressor effect).\nThe effect of education as seen in the bivariate model (Model 2 √ü = .85) remains about the same in Model 3 (√ü = .74; this is called an independent effect, since the effect of education is not influenced by the addition of job type in the model).\n\n\n\n\n\nReasons Why The Relationship Changes Between Models\nRemember, there are always two reasons why we‚Äôd see a change.\n\nchance : the change in the slope was just due to some kind of sampling error; the difference between the models is really just random chance.\nsome real effect. IV2 really does change the relationship between IV1 and the DV : that is, there is some real relationship between IV2, IV1, and the DV that changes the slopes in your model.\n\n\nThere are various ways to test for whether the change in slopes is large enough to be important / statistically significant. One method is is something called bootstrapping - I‚Äôll eventually put this in a supplemental chapter, but the basic idea is that you run a for-loop to resample the data, estimate the slope in a bivariate vs.¬†multivariate model (from a new dataset), and then save the difference in slopes to ‚Äúbucket‚Äù (or whatever you have named the array), and then repeat the process 1000 times. Another method to test for the difference is called the ‚ÄúSobel Test‚Äù; you can read about it here or here if you want. Or not!",
    "crumbs": [
      "List of R Code",
      "People Are Complex",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "chapters/10R_MultipleRegression.html#more-practice",
    "href": "chapters/10R_MultipleRegression.html#more-practice",
    "title": "Multiple Regression",
    "section": "More Practice",
    "text": "More Practice\nOkay, that was a lot! A few more resources to help and test your understanding.\n\nA MULTIPLE REGRESSION VIDEO :\nHey, do you like my disembodied voice explaining things on top of an R screenshot? Well good news, in this video I go through another example of multiple regression in R.\n\nHere‚Äôs the R script I used; the data come from the ‚Äúhormone‚Äù dataset, posted to bCourses.\n\n\n\n\nCheck-In : Here‚Äôs a Practice Quiz\nUse the check-in to practice and prepare for your quiz this week. See the answer key videos below, but try on your own first.\n\nProfessor Does the Check-In (Part 1)\n\n\n\nProfessor Does the Check-In (Part 2)\n\n\n\n\nCheck-Out\nhow‚Äôs it going and what should I focus on for our lecture next week.\nYEAH!",
    "crumbs": [
      "List of R Code",
      "People Are Complex",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Multiple Regression</span>"
    ]
  },
  {
    "objectID": "chapters/11R_InteractionFX.html",
    "href": "chapters/11R_InteractionFX.html",
    "title": "Interaction Effects",
    "section": "",
    "text": "Part 1 : Interaction Effects (‚ÄúIt Depends‚Äù / Life is Complex‚Ä¶)\nGoal : In this document, I‚Äôll do my best to walk y‚Äôall through the theory and computation required to understand and interpret interaction effects in R.",
    "crumbs": [
      "List of R Code",
      "People Are Complex",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Interaction Effects</span>"
    ]
  },
  {
    "objectID": "chapters/11R_InteractionFX.html#tldr-what-is-an-interaction-effect-and-why-should-i-care-professor",
    "href": "chapters/11R_InteractionFX.html#tldr-what-is-an-interaction-effect-and-why-should-i-care-professor",
    "title": "Interaction Effects",
    "section": "TLDR : What is an Interaction Effect and Why Should I Care, Professor?",
    "text": "TLDR : What is an Interaction Effect and Why Should I Care, Professor?\nBefore I make things confusing, let me try to clearly explain what an interaction effect is, and why we care about it :)\nLife is complex - one variable (a DV) can be explained by many other variables (IVs). And not only do each of these IVs uniquely influence the DV (multiple regression), but these IVs can influence each other, and the way that they each influence the DV (multiple regression with an interaction effect).¬†\nAn interaction effect describes the idea that the effect of an IV on the DV might depend on some other IV. Below are a few conceptual examples :¬†\n\nThe effect of studying on grades depends on how much sleep you‚Äôve gotten.\n\nFor someone with a lot of sleep, studying is very effective.\nFor someone with a little sleep, studying is not very effective.\n\nThe effect of an SSRI on well-being depends on whether the person is also engaged in cognitive behavioral therapy. For someone in talk-therapy, taking an anti-depressant is more effective than for someone who is not in talk-therapy.¬†\nThe effect of seeing a bowl full of food on your hunger depends on whether the food is something you enjoy or not.\n\nScientists looking to understand variables thus look to interaction effects in order to get a better understanding of the phenomenon. And sometimes, it‚Äôs only by looking at interaction effects that you can find patterns in the data. For example, for a while there was a mystery about why there was widespread bee colony death - turns out that scientists needed to look for an interaction effect between variables (including climate change) to find potential answers.\nPost some examples in our discord thread if you want :) - great if you have an example of an interaction effect to test in your final project (but this is not required!)",
    "crumbs": [
      "List of R Code",
      "People Are Complex",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Interaction Effects</span>"
    ]
  },
  {
    "objectID": "chapters/11R_InteractionFX.html#recap-main-effects-the-independent-effect-of-one-iv-on-the-dv",
    "href": "chapters/11R_InteractionFX.html#recap-main-effects-the-independent-effect-of-one-iv-on-the-dv",
    "title": "Interaction Effects",
    "section": "RECAP : Main Effects (the independent effect of one IV on the DV)",
    "text": "RECAP : Main Effects (the independent effect of one IV on the DV)\n\nMain Effects in Bivariate Regression\nSo far in this class, we have worked with what are called simple ‚Äúmain effects‚Äù - the relationship between one variable and another. We first looked at this in a bivariate (two variable) model, with one DV and one IV.\nFor example, here‚Äôs some fake data where I‚Äôm looking at two linear models predicting safety - whether people are safer on bikes (or not) and whether people are safer with a helmet (or not).\n\nWe see that there appears to be a difference - people in these fake data are more safe when on a bike (b = 5) than when not on a bike, and there appears to be no difference in terms of their safety when wearing a helmet or not.\n\n\nMain Effects in Multiple Regression\nIn Chapter 10, we saw that one of the benefits of adding complexity to our linear models is that we could ‚Äúexamine the unique effects of variables‚Äù; that ‚Äúwe can test whether the effect of one variable (IV1) on another (DV) is unique to that variable, or whether that relationship is also explained by some other variable (IV2).‚Äù That is, I could test whether the relationship between bike riding and safety changes if you control for any effects of wearing a helmet (or test for whether the non-effect of wearing a helmet on safety might be changed if you account for any effects of riding a bike on safety.)\nTo do this, I‚Äôd enter both variables into my linear model, and look to see how the slopes changes from their respective bivariate models.\n\nLooking at the fake data, I find the exact same slopes for each independent variable even when controlling for redundant covariation with a multiple regression - folks who ride a bike are more safe (b = 5) and there‚Äôs no effect of wearing a helmet on safety (b = 0). I‚Äôd say these are independent main effects - since I‚Äôm looking at each variable in isolation.",
    "crumbs": [
      "List of R Code",
      "People Are Complex",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Interaction Effects</span>"
    ]
  },
  {
    "objectID": "chapters/11R_InteractionFX.html#interaction-effects-when-the-effect-of-one-variable-depends-on-another",
    "href": "chapters/11R_InteractionFX.html#interaction-effects-when-the-effect-of-one-variable-depends-on-another",
    "title": "Interaction Effects",
    "section": "Interaction Effects : When the Effect of One Variable Depends on Another",
    "text": "Interaction Effects : When the Effect of One Variable Depends on Another\n\nThe Interaction Effect in an Equation\nWhen testing for an interaction effect, we are looking to see whether the two variables work together to make different predictions of the dependent variable. Because we are looking at how one IV interacts with / depends on the effect of an other IV, we need to connect these variables in some way, and we do this by multiplying the two variables together in our linear model :¬†\n\nThe interaction term is represented in b3 - this is an adjustment (a slope) that we will make to our other slopes. Below, we‚Äôll look at this with our bike riding example.\n\n\nExample : The Effect of Wearing a Helmet on Safety Depends on Whether You are Riding a Bicycle or Not\nI‚Äôd examine whether the effect of wearing a helmet might DEPEND on whether or not the person is on a bike. I‚Äôd expect that wearing a helmet would be more important for safety if the person is riding a bike than if the person is walking around.\nTo test this, I would again add both variables to my linear model in a multiple regression. In addition to adding the variables together, I would also add an interaction term - the product (multiplication) of the two variables :\n\nI go over the terms from this model much more in Part 2 of this document. But the result is a linear model that would result in the graph below, which reveals an interaction effect : the effect of wearing a helmet on safety depends on whether you are reiding a bike or not.\n\n\n\n\nLet‚Äôs go over a few examples.\n\n\n\n\nThe Effect of Using Implementation Intentions on Task Completion Depends on the Type of Goal\nWhat is an implementation intention? I‚Äôm glad you asked!\n\nHow to interpret the interaction effect of an implementation intentions? In six minutes.1\n1¬†note : somewhat tragic story, shortly after I started working on this implementation intention, I got an e-mail from someone advertising a paper that was a more clearly written version of the paper that I wrote that got rejected. Would have been more heartbreaking had it a) not been almost a decade since I got rejected (to be honest it was almost a relief to be free of that burden of a project that I had long stopped caring about) and b) the newer paper was really good. can‚Äôt find it right now but if anyone is reading this let me know and I‚Äôll try to find a copy of the paper!\n\n\n\nThe Effect of Resume Quality on Callback Percentage Depends on the Applicant‚Äôs Perceived Race\nIn the video below, I walk through data from Bertrand & Mullainathan‚Äôs (2004) excellent paper demonstrating white privilege with real data.\n\n\n\n\nThe Effect of Time on Hurt Feelings Depends on Whether You Take a Pain Pill or Not\nStill in the mood for another example? Don‚Äôt worry about hurting my feelings I‚Äôm loaded up on TYLENOL.2\n2¬†Data from : DeWall, C. N., MacDonald, G., Webster, G. D., Masten, C. L., Baumeister, R. F., Powell, C., ‚Ä¶ & Eisenberger, N. I. (2010). Acetaminophen reduces social pain: Behavioral and neural evidence. Psychological science, 21(7), 931-937. [Link and here‚Äôs a good critique of the research]\n\n\n\n\nGosh Professor, I Just Love These Examples, Can We Have Some More?\nNo, student. I am all done providing examples. But you can generate your own examples.",
    "crumbs": [
      "List of R Code",
      "People Are Complex",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Interaction Effects</span>"
    ]
  },
  {
    "objectID": "chapters/11R_InteractionFX.html#interaction-effects-vs.-main-effects",
    "href": "chapters/11R_InteractionFX.html#interaction-effects-vs.-main-effects",
    "title": "Interaction Effects",
    "section": "Interaction Effects (vs.¬†Main Effects)",
    "text": "Interaction Effects (vs.¬†Main Effects)\n\nMain Effects\nSo far in this class, we have focused on the slopes in regression models, since these terms describe the relationship between two variables (DV ~ IV1); researchers use this relationship to make their predictions about the DV (e.g., if there is a relationship between presence of rain clouds and rain, then I will predict it will rain when I see a rain cloud outside). Another word for the slope is an ‚Äúeffect‚Äù, and the type of effect we have been working with so far in this semester is often called a main effect.\n\n\nInteraction Effects\nAn interaction effect allows you to test whether the relationship between one independent variable and the dependent variable *depends on* another independent variable. (In other words, whether the main effect is moderated, or changed, by some other variable). Interaction effects are a special form of multiple regression, where you have two (or more) independent variables in your model. Whereas before we added a second (or third) variable to our model, an interaction effect examines the result of the product (or multiplication) of two independent variables.¬†\n\n\nThe Model\nThe model for an interaction effect should look familiar!\n\n\nyi = the individual‚Äôs actual score that we are trying to predict (the DV). Remember that the i symbol represents an individual; each individual has a specific value for the DV.\na = the intercept (a constant value that is always part of your model), and is the starting place for our predictions. (‚ÄúThe predicted value of y when all X values are zero!‚Äù, they said.)\nX1 = the individual‚Äôs specific value for the first independent variable (X1).\nb1 = the slope for X1 = the adjustment we make in our prediction of y, based on the individual‚Äôs value of X1.\nX2 = the individual‚Äôs specific value for the second independent varible (X2)\nb2 = the slope for X2 = the adjustment we make in our prediction of y, based on the individual‚Äôs value of X2.\nb3 = the slope for our interaction term = the change that we make in our predictions of y based on the values of BOTH X1 and X2\nei = the individual‚Äôs specific error (residuals) = the distance between the predicted values of y and the individual‚Äôs actual score for y.\n\n\nConsider the graph from our bike riding example.\n\nWe can use the output of a model to get the same estimated effects of safety for each group.\n\nWe can plug these estimates into our model :\nY ~ 10 - 5(bikeY) - 10(helmetY) + 20(bikeY * helmetY)\n\nbikeYes and helmetYes are both dummy coded variables that take the values 0 (NOT on a bike and NOT wearing a helmet) and 1 (on a bike and on a helmet).\n\nTry the calculations on your own. Plug these values into the equation in order to get the four combinations of bike and helmet.¬†\nExample : wearing a helmet (helmetY = 1) and not on a bike (bikeY = 0).\nY ~ 10 - 5(bikeY) - 10(helmetY) + 20(bikeY * helmetY)\nY ~ 10 - 5(0) - 10(1) + 20(0*1)\nY ~ 10 - 0 - 10 + 0¬†\nY ~ 0\n\n\n\n\n\n\nExample Model\n\n\n\n\n\n\n\n\n\nHelmet - Yes\nHelmet - No\n\n\nBike - Yes\n15\n5\n\n\nBike - No\n0\n10\n\n\n\nThese values should yield the same values you get from the graph - the model just defines changes to predictions of Y depending on information in X1 and X2.\nNote : the principle of sampling error still applies to these estimates - you could find the estimate (slope) in your model because there is some real interaction effect (the alternative hypothesis), or because of chance (the null hypothesis). You can use bootstrapping to estimate the variation due to sampling error, and use that distribution to estimate the likelihood of finding a non-zero estimate.",
    "crumbs": [
      "List of R Code",
      "People Are Complex",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Interaction Effects</span>"
    ]
  },
  {
    "objectID": "chapters/11R_InteractionFX.html#another-example-using-data-in-r",
    "href": "chapters/11R_InteractionFX.html#another-example-using-data-in-r",
    "title": "Interaction Effects",
    "section": "Another Example Using Data in R",
    "text": "Another Example Using Data in R\nInteraction effects are notoriously tricky, so below I will walk you through a few examples of how to model, interpret, and graph interaction effects. For this problem, I‚Äôm using the Prestige dataset, from the ‚Äòcar‚Äô library.¬† I‚Äôm skipping over the bivariate regression models (DV ~ IV1 , DV ~ IV2) and the multiple regression model (DV ~ IV1 + IV2) since we‚Äôve worked with these before. However, you MUST do all these steps first before you do an interaction effect in order to be able to fully interpret the model!\n\nThe Model : One Continuous DV, One Continuous IV, and One Categorical IV\nFirst,¬† we define our model, and examine the coefficients of this model. I‚Äôve z-scored all my terms, so everything is in units of standard deviation.\n\nI walk you through this model below :\n\n\n\n\n\n\n\n\n\nTerm\nName\nValue\nInterpretation\n\n\n1\nIntercept\n0.00\nThis is the starting place for our line - the predicted value of prestige when all X values are zero. Since my variables are z-scored, zero means ‚Äúaverage‚Äù, so again we see that someone with average education and is NOT a white collar or professional worker (a blue collar worker) has an average level of prestige.\n\n\n2\nz.education\n.76\nThis is the relationship between education and prestige for blue collar workers - in other words, for people who are NOT white collar or professional workers (blue collar workers).\n\n\n3\ntypeprof\n.24\nThis is the adjustment we make to our prediction of prestige for someone who IS a professional worker (compared to blue collar workers). In other words, a professional worker has .24 standard deviations more prestige than a blue collar worker, assuming both have similar levels of education.\n\n\n4\ntypewc\n-.19\nThis is the adjustment we make to our prediction of prestige for someone who IS a white collar worker (compared to blue collar workers), controlling for years of education. In other words, a white collar worker is predicted to have .19 standard deviations less¬† prestige than a blue collar worker, assuming both have similar levels of education.¬†\n\n\n5\nz.education:typeprof\n-.16\nThis is the adjustment we make to the slope for education if someone comes from a professional job. In other words, the relationship between education and prestige among professional workers is .16 standard deviations less than among blue collar workers. So the relationship between education and prestige among professional workers would be moderated (changed) : b = .76 - .16 =¬† .50. This difference in the slope is the interaction effect - the relationship between education and prestige depends on the type of job a person has.\n\n\n6\nz.education:typewc\n1.02\nAgain, this is the adjustment we make to the slope for education if someone comes from a white collar job. In other words, the relationship between education and prestige among white collar workers is STRONGER (by .26 standard deviations)¬† THAN the relationship between education and blue collar workers. This means the relationship between education and prestige among professional workers is moderated (changed) : b = .76 + .26 =¬† 1.02. This difference in the slope is another example of the interaction effect - the relationship between education and prestige depends on the type of job a person has.\n\n\n\nTLDR : the interaction effects are terms 5 and 6, which describe how the relationship between education and prestige for blue collar workers is changed (moderated) for individuals who come from professional jobs (change in slope of education = -.16) and white collar jobs (change in slope of education = .26).\n\n\nThe Graphs.\nInteraction effects might make more sense if you graph them. Below, I‚Äôve saved my model coefficients as an object cf, and refer to each coefficient by number. So cf[1] = the intercept; cf[5] = z.education:typeprof, etc.\n\n\nHere‚Äôs the final graph, reprinted.\n\nIf you look at the graph, you‚Äôll notice that the black line seems to best describe the relationship for the black dots (blue collar workers), the red line seems to describe the relationship for the red dots (professional workers), and the green line best for the green dots (white collar workers. And there are small differences in where the lines start (the main effects of job type) and their slope (the interaction effects of how job type changes the relationship between education and prestige).\nTo best interpret the main effects, you need to run the multivariate model with NO interaction effect term, as described in the previous lecture.\n\n\nStatistical Inference\nTo recap, the model just describes the relationships between our variables for the sample, and researchers are typically interested in using what we‚Äôve learned about the sample to make a prediction about the broader population. We know that our sample will not represent the population (sampling error), but are instead estimates that might vary due to chance. So we have to conduct some inferential statistics to estimate the amount of sampling error in our model.\nSo, like for bivariate models, we will have to estimate sampling error (using bootstrapping or the summary() function to calculate standard error) to guess at the chance that the interaction effects that change the relationship between education and prestige was found due to chance.\nOne important thing about interaction effects - because an interaction effect involves both two independent variables working together to make predictions of the DV, you need to make sure that both independent variables are on the same scale, or your standard error estimates will be off. To do this, make sure to z-score your variables, or use the standardize() function [part of the ‚Äòarm‚Äô package] on your model (as I‚Äôve already done - just reminding y‚Äôall.\nHere‚Äôs the output of my summary of a z-scored model. Let‚Äôs focus on just one interaction effect : the change in the relationship between education and prestige for professional workers (vs.¬†blue collar workers).\n\nOur estimate is that positive relationship between education and prestige for blue collar workers (Œ≤ = .76) is adjusted for professional workers (the adjustment is Œ≤ = -.16, so the slope for people who do are professional workers would be Œ≤ = .76 - .16 = Œ≤ = .60). The standard error for the interaction effect (.23) suggests that IF the null hypothesis were ‚Äútrue‚Äù, the average variation in interaction effects we might find due to chance would be .23. The t-value (-.68) suggests that the interaction effect we found is only ‚Öî of the average interaction effect we would expect to find on average due to sampling error if the null hypothesis were true. The p-value (p = .5) suggests that the probability of finding this interaction effect if the null hypothesis is true is about 50%, which is very likely, and suggests that it‚Äôs most likely we would have found this interaction effect due to chance.\nOf course, we could be wrong in our determination that this is a non-significant finding - there is a 50% chance that we would not observe this slope (or one larger) due to chance if the null hypothesis were true. But this, along with the fickle nature of interaction effects, I think this interaction effect, and the one for white collar jobs, is likely bogus. And in any case, if I really cared about this effect and wanted to write about this in a paper, I would replicate this effect in other datasets (starting with Berkeley students and then generalizing to other samples) no matter the significance level.\nCool. I think that‚Äôs all for now.",
    "crumbs": [
      "List of R Code",
      "People Are Complex",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Interaction Effects</span>"
    ]
  },
  {
    "objectID": "chapters/12R_LogisticRegression.html",
    "href": "chapters/12R_LogisticRegression.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "Part 1 : The Theory of Logistic Regression\nIn this chapter, I‚Äôve tried to organize a narrative about logistic regression - why we do this, how to do it in R, and how to interpret the results. This is not a topic that I will cover in the introduction to statistics class, but thought I would include it here because a) some students are interested in defining logistic regression models for their final project; b) this is a nice way to see how the ‚Äúgeneral‚Äù linear model can be ‚Äúgeneralized‚Äù to other topics, and c) sometimes I teach on this. Feel free to reach out with questions if something is unclear on the course Discord.",
    "crumbs": [
      "List of R Code",
      "People Are Complex",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "chapters/12R_LogisticRegression.html#a-non-logistic-example-to-start",
    "href": "chapters/12R_LogisticRegression.html#a-non-logistic-example-to-start",
    "title": "Logistic Regression",
    "section": "A Non-Logistic Example to Start",
    "text": "A Non-Logistic Example to Start\nLet‚Äôs look at the hormone dataset again. Previously, we saw that sex was related to testosterone.\n\n\nCode\nlibrary(gplots)\n\n\n\nAttaching package: 'gplots'\n\n\nThe following object is masked from 'package:stats':\n\n    lowess\n\n\nCode\nh &lt;- read.csv(\"~/Dropbox/!GRADSTATS/Datasets/hormone_dataset.csv\")\nh$sexF &lt;- as.factor(h$sex)\nlevels(h$sexF) &lt;- c(\"Male\", \"Female\")\nh$sexF &lt;- relevel(h$sexF, ref = \"Female\")\n\nmod &lt;- lm(test_mean ~ sexF, data = h)\nplotmeans(test_mean ~ sexF, data = h, connect = F)\n\n\n\n\n\n\n\n\n\nCode\nsummary(mod)\n\n\n\nCall:\nlm(formula = test_mean ~ sexF, data = h)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-60.927 -17.886  -3.404  13.801 138.723 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   41.581      5.894   7.055 3.77e-10 ***\nsexFMale      50.586      7.045   7.181 2.11e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 30.63 on 88 degrees of freedom\n  (32 observations deleted due to missingness)\nMultiple R-squared:  0.3695,    Adjusted R-squared:  0.3623 \nF-statistic: 51.56 on 1 and 88 DF,  p-value: 2.111e-10\n\n\nAs practice, take a look at the output - what do you observe? Think about a) the pattern in the data; b) the interpretation of the significance and effect size; c) the ‚Äúwho cares‚Äù about these results.\n\n\n\n\n\n\nWhat Professor Observes (Think On Ur Own First?)\n\n\n\n\n\nI notice the following:\nA. The pattern shows that males have higher (b = 50.586) testosterone than females on average.\nB. The effect is fairly large - I didn‚Äôt (and don‚Äôt want) to calculate cohen‚Äôs D, but the \\(R^2\\) value is very high - biological sex explains 36% of the variation in testosterone. This is ‚Äúhighly significant‚Äù, meaning that if the null were true (if there were no differences in testosterone between males and females) the probability that we would observe a difference as large as 50.586 is very, very small (p &lt; .00000000001).\nC. Who cares about this? Gosh, there seem to be a lot of bad takes on hormones and sex out there these days, and I don‚Äôt really want to add to that chorus, but since I chose these data‚Ä¶I‚Äôm not really sure what to make out of hormone data. I‚Äôm not a hormone researcher, think it‚Äôs odd that our society is SO FOCUSED on quantifying the hormones of individuals, and think that all of the energy focused on hormone levels and women in sports could be better spent following and watching female athletes and supporting them in that way, and (while we‚Äôre at it) making sure that we create inclusive spaces where all people can belong? Seems easy. IDK. Feel free to lemme know if you disagree / I‚Äôm missing something / I have some learning to do. Okay, back to the show.\n\n\n\n\nFlipping the Model Around\nSo far, we‚Äôve predicted a numeric / continuous variable with our good friend the linear model. But researchers often want to make predictions of categorical variables.\nWe could treat sex as a numeric variable; in fact, the original variable was coded as numeric (1 = male; 2 = female). So why not include this ‚Äúnumeric‚Äù variable as a DV in our linear model? Seems easy; what could go wrong!??\n\n\nCode\nmod2 &lt;- lm(sex ~ test_mean, data = h)\nplot(sex ~ test_mean, data = h)\nabline(mod2)\n\n\n\n\n\n\n\n\n\nCode\nsummary(mod2)\n\n\n\nCall:\nlm(formula = sex ~ test_mean, data = h)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.6341 -0.2735 -0.1131  0.3306  0.9166 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.862302   0.087390  21.310  &lt; 2e-16 ***\ntest_mean   -0.007303   0.001017  -7.181 2.11e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.368 on 88 degrees of freedom\n  (32 observations deleted due to missingness)\nMultiple R-squared:  0.3695,    Adjusted R-squared:  0.3623 \nF-statistic: 51.56 on 1 and 88 DF,  p-value: 2.111e-10\n\n\n\n\n\n\n\n\nWhat Professor Thinks Went Wrong (Think On Ur Own First?)\n\n\n\n\n\n\nThe predicted values of the DV are continuous, and can range between 1 and 2. However, we think of (and measured) sex as a binary variable (male OR female) and while a continuous approach might better match the complex biological reality, that wasn‚Äôt the way the data were measured in this study.\nThe predicted values of gender can go beyond the range of our DV. For example, someone with a testosterone of 150 would be predicted to have a gender of .766. If 1 = male and 2 = female, does this make the person super-male? Sub male? No, the value makes no sense and is wrong.\nOur DV is not normally distributed, and the linear model depends on certain assumptions (specifically normality and linearity.) We are violating those assumptions here.",
    "crumbs": [
      "List of R Code",
      "People Are Complex",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "chapters/12R_LogisticRegression.html#logistic-regression",
    "href": "chapters/12R_LogisticRegression.html#logistic-regression",
    "title": "Logistic Regression",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nAs a solution to our problem, we can transform our linear model to one that conforms to a non-normal (or non-Gaussian) distribution.\nRather than predict a specific ‚Äúvalue‚Äù of male or female, we will estimate the probability of being male vs.¬†female. The predicted values should necessarily fall between 0 (estimated probability of being female = 0%) and 1 (estimated probability of being female = 100%).\nWe do this using a ‚Äúlink‚Äù function that ‚Äúlinks‚Äù the mean (i.e., expectaion) of your outcome variable(s), Y, to our linear predictor. Different types of outcome variables each have a different ‚Äúcanonical‚Äù link function, as summarized in the table below.\n\n\n\nDistribution\nLink Function\nExample Use Case\n\n\nNormal\nIdentity\nDV is continuous response.\n\n\nBinomial\nLogit\nDV is binary response\n\n\nPoisson\nLog\nDV is a fixed count response.\n\n\nGamma\nReciprocal\nContinuous, but highly skewed, distributions.\n\n\n\nYou don‚Äôt need to memorize these - the key idea is that sometimes you want (or need) to adjust the parameters of the linear model in order to better fit the type of data that you are working with.\nBy adapting our linear model, we have extended the general linear model to something called the generalized linear model (glm).",
    "crumbs": [
      "List of R Code",
      "People Are Complex",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "chapters/12R_LogisticRegression.html#interpreting-the-intercept",
    "href": "chapters/12R_LogisticRegression.html#interpreting-the-intercept",
    "title": "Logistic Regression",
    "section": "Interpreting the Intercept",
    "text": "Interpreting the Intercept\nThere are two methods of interpreting the intercept (that I know of.)\n\nExponentiate the Intercept\nWe can exponentiate the intercept to transform the estimate into the odds of the DV occuring when the X value is zero.\nOdds are defined as \\(\\text{odds} = \\frac{p}{(1-p)}\\).\n\n\nCode\nround(exp(coef(glmod)[1]), 2)\n\n\n(Intercept) \n     123.93 \n\n\nSo the intercept in this case describes the probability of being female, divided by the probability of not being female, for someone with zero testosterone. In other words, there‚Äôs a much, much higher odd of being female for someone with zero testosterone.\n\n\nInverse Logit Function\nIf we take the inverse of the logit function of the intercept, the intercept is the probability of Y, when all X values are zero. This is often the clearest way to interpret the intercept (if that statistic is relevant) in my opinion.\n\\(\\huge \\text{inverse logit = } \\frac{1}{1 + e^{-x}} \\text{ or } \\frac{e^{x}}{1 + e^{x}}\\)\n\n\nCode\nexp(coef(glmod)[1]) / (1 + exp(coef(glmod)[1]))\n\n\n(Intercept) \n  0.9919957 \n\n\nSo, there‚Äôs a 99.19% chance that a participant with zero testosterone would be female.",
    "crumbs": [
      "List of R Code",
      "People Are Complex",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "chapters/12R_LogisticRegression.html#interpreting-the-slope.",
    "href": "chapters/12R_LogisticRegression.html#interpreting-the-slope.",
    "title": "Logistic Regression",
    "section": "Interpreting the Slope.",
    "text": "Interpreting the Slope.\nWhen we exponentiate the slope, we convert the estimate into an odds ratio. The odds ratio describes how the odds change between two different outcomes.\nNote that an odds ratio of 1 would mean that there is no change in the odds of Y as X changes (similar to a slope of zero in a general linear model).\nOne nice feature of odds ratios is that they are are scalable - you can keep doubling the odds, and not go beyond a probability of 1.\n\n\nCode\nexp(coef(glmod)[2])\n\n\ntest_mean \n0.9121065 \n\n\nThe odds ratio is .91, which is .09 less than an odds ratio of 1. This means that each unit increase in testosterone decreases the probability of being female by .09 or 9%. (The change in probability is in reference to an odds ratio of 1.)",
    "crumbs": [
      "List of R Code",
      "People Are Complex",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "chapters/12R_LogisticRegression.html#gelman-hills-divide-by-four-rule.",
    "href": "chapters/12R_LogisticRegression.html#gelman-hills-divide-by-four-rule.",
    "title": "Logistic Regression",
    "section": "Gelman & Hill‚Äôs ‚ÄúDivide by Four‚Äù Rule.",
    "text": "Gelman & Hill‚Äôs ‚ÄúDivide by Four‚Äù Rule.\nMany people find odds and odd ratios confusing. The estimable Gelman & Hill (2007) agree, and define a ‚Äúdivide by four‚Äù rule. Where you take the regression coefficient, divide it by four, and that number gives you the upper bound of the predictive difference in Y that corresponds to a 1-unit increase in X.\n\n\nCode\ncoef(glmod)/4\n\n\n(Intercept)   test_mean \n 1.20493609 -0.02299963 \n\n\nSo, this method would suggest that a unit increase in testosterone would decrease the probability of being female by no more than 2%.",
    "crumbs": [
      "List of R Code",
      "People Are Complex",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "chapters/12R_LogisticRegression.html#that-inferential-statistics-stuff",
    "href": "chapters/12R_LogisticRegression.html#that-inferential-statistics-stuff",
    "title": "Logistic Regression",
    "section": "That Inferential Statistics Stuff",
    "text": "That Inferential Statistics Stuff\nWe can extract inferential statistics using the summary() function, as before.\n\n\nCode\nsummary(glmod)\n\n\n\nCall:\nglm(formula = sexR ~ test_mean, family = \"binomial\", data = h)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  4.81974    1.19821   4.022 5.76e-05 ***\ntest_mean   -0.09200    0.02081  -4.420 9.86e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 109.96  on 89  degrees of freedom\nResidual deviance:  56.09  on 88  degrees of freedom\n  (32 observations deleted due to missingness)\nAIC: 60.09\n\nNumber of Fisher Scoring iterations: 7\n\n\nMuch is the same as before:\n\nWe have estimates of the slope. Note that these are not-yet exponentiated.\nWe have estimates of standard errors and p-values with stars that immediately show us whether our results are statistically significant or not.\n\nA few things are different:\n\nThere‚Äôs a dispersion parameter. This is part of the link function, and describes how variance in our outcome variable depends on the mean. This is always set to 1 in a logistic regression; there are other forms of regression (‚Äúquasi-logistic‚Äù and ‚Äúquasi-poisson‚Äù for example where the dispersion parameter can be increased to account for greater variablility in the outcome.)\nR no longer reports an \\(R^2\\) value. The \\(R^2\\) statistic is not appropriate for generalized linear models, since we are not calculating errors in the same way (i.e., we are not adding up . There are various methods of calculating what‚Äôs called a ‚Äúpseudo \\(R^2\\)‚Äù, which estimates this statistic, and often is reported via other functions (see below).\nInstead, R reports two deviance statistics - null and residual. Deviance is a measure of error - we ‚Äúwant‚Äù deviance to be low, and expect it to decrease by at least one for every new predictor we add to our model. The null deviance is the error when we have no predictor in our model (and are just using a constant term - the baseline probability of the outcome variable - to make predictions). The residual deviance is the deviance for this model; the fact that there‚Äôs a decrease of 53.87 exceeds our expected decrease of 1, and tells me the model has improved our predictions.\nR reports the Akaike Information Criterion (AIC). We will talk about this more next week, when we talk about model fit and comparing models, but the TLDR is this is a way to evaluate how ‚Äúgood‚Äù a model is at making predictions, with a lower AIC describing a model that better fits the data. The scale of this estimate - like the deviance statistics - is dependent on the data and sample size, so AIC is meant to compare one model to another from the same dataset.\n\nThere are a few different packages that make reporting the effects of a logistic regression easier. One example is the summ() function from the jtools package (which I think we looked at before to make nice multiple regression tables.) The function works the same, but we will add an argument to tell the function to exponentiate the coefficients to aid in the interpretation, and an argument to add confidence intervals. (See the documentation for the function for other arguments to add.)\n\n\nCode\nlibrary(jtools)\nexport_summs(glmod, exp = TRUE, error_format = \"[{conf.low}, {conf.high}]\")\n\n\n\n\nModel 1\n\n(Intercept)123.93 ***\n\n[11.84, 1297.49]¬†¬†¬†\n\ntest_mean0.91 ***\n\n[0.88, 0.95]¬†¬†¬†\n\nN90¬†¬†¬†¬†¬†¬†¬†\n\nAIC60.09¬†¬†¬†¬†\n\nBIC65.09¬†¬†¬†¬†\n\nPseudo R20.64¬†¬†¬†¬†\n\n *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05.\n\n\n\n\nNote that the default output for this function reports another statistic - the Bayesian Information Criterion (which is another way to evaluate the model) as well as some Pseudo \\(R^2\\) statistics (there are different methods of calculating this; not sure which one the authors defaulted to.)\n\nWould You Like To Learn More?\nYou don‚Äôt have to take my word for it. Below are a few readings that will help support your understanding of generalized linear models. Let me know if you find other useful resources!\n\nGelman & Hill (2007). Chapter 5 is focused on logistic regression; they work through a few examples, talk about interaction effects and scaling / centering variables; making specific preditions‚Ä¶.very thorough. I think you can easily find this online, but let me know if you want a .pdf.\nA textbook chapter on generalized models. I really like this researcher‚Äôs approach to linear models, and while this textbook chapter is a little more spare than some of the other chapters, it presents a nice overview of why and how we use generalized linear models.\nA stats blog works through the output of a generalized linear model in R; focuses on a poisson distribution, but many of the principles are the same (and good to see a different example of a similar concept.)",
    "crumbs": [
      "List of R Code",
      "People Are Complex",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "chapters/13R_Conclusion2Introduction.html",
    "href": "chapters/13R_Conclusion2Introduction.html",
    "title": "Farewell! (A Conclusion)",
    "section": "",
    "text": "Hi! In this final chapter, I‚Äôll summarize all the major themes from the course, integrate all of our critical statistics, R, and research methods skills into one transcendent ‚Ä¶.\n‚Ä¶oh, what‚Äôs that? You are tired of reading and I am tired of writing and isn‚Äôt a 13th chapter supposed to be unlucky anyway? 1\n1¬†And I only promised 12 chapters as part of the sabbatical project that gave me time and space to work on this textbook and I do not violate my contracts as an employee.Okay then, I will plan to save all this transcendence for our last class at the end of the semester when everyone is feeling braindead and ready for THE HOPEFUL ROT of either summer or winter break.\nWill be great to see you there.\nThanks!\n&lt;3",
    "crumbs": [
      "List of R Code",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Farewell! (A Conclusion)</span>"
    ]
  },
  {
    "objectID": "Rcode.html",
    "href": "Rcode.html",
    "title": "Appendix A ‚Äî Rcode",
    "section": "",
    "text": "A.1 R Code : Creating Variables in R\nHere is a list of the R code we use in this class.",
    "crumbs": [
      "List of R Code",
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Rcode</span>"
    ]
  },
  {
    "objectID": "Rcode.html#numeric-variables-in-r",
    "href": "Rcode.html#numeric-variables-in-r",
    "title": "Appendix A ‚Äî Rcode",
    "section": "A.2 Numeric Variables in R",
    "text": "A.2 Numeric Variables in R\n\n\n\n\n\n\n\nCode\nDescription\n\n\n\n\nvariable &lt;- c(#, #, #, #, etc.)\ntired &lt;- c(1,2,3,4)\nvariable = an object that you will define in R\n&lt;- = ‚Äúassign‚Äù; tells R to save whatever comes on the right to whatever object is on the left.\nc = combine : tells R to combine whatever happens in the parentheses\n() = parentheses to group related terms\n# = what you store in the variable; each item should be separated by a comma and space.\n\n\nhist(dat$variable)\nFor continuous variables : draws a histogram.",
    "crumbs": [
      "List of R Code",
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Rcode</span>"
    ]
  },
  {
    "objectID": "Rcode.html#string-variables",
    "href": "Rcode.html#string-variables",
    "title": "Appendix A ‚Äî Rcode",
    "section": "A.3 String Variables",
    "text": "A.3 String Variables\n\n\n\n\n\n\n\nvariable &lt;- c(‚Äúname1‚Äù, ‚Äúname2‚Äù, ‚Äúname1‚Äù, etc.)\nemotion &lt;- c(‚Äúsad‚Äù, ‚Äúhappy‚Äù, ‚Äúsad‚Äù)\nvariable = an object that you will define in R\n&lt;- = ‚Äúassign‚Äù; tells R to save whatever comes on the right to whatever object is on the left.\nc = combine : tells R to combine whatever happens in the parentheses\n() = parentheses to group related terms\n# = what you store in the variable; each item should be separated by a comma and space.\n\n\nas.factor(variable)\nas.factor(emotion)\nas.factor() # converts a string variable into a categorical factor\n\n\nvariable &lt;- as.factor(variable)\n# ‚Äúsaves‚Äù this conversion as the original variable\n\n\nplot(dat$variable)\nFor categorical variables : draws a barplot. For continuous variables :¬† illustrates values of the variable (y-axis) as a function of their index (x-axis).",
    "crumbs": [
      "List of R Code",
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Rcode</span>"
    ]
  },
  {
    "objectID": "Rcode.html#r-commands-for-importing-and-navigating-data",
    "href": "Rcode.html#r-commands-for-importing-and-navigating-data",
    "title": "Appendix A ‚Äî Rcode",
    "section": "A.4 R Commands for Importing and Navigating Data",
    "text": "A.4 R Commands for Importing and Navigating Data\n\n\n\n\n\n\n\nR Command\nWhat it Does\n\n\ndat &lt;- read.csv(‚Äúpath/file.csv‚Äù, stringsAsFactors = T)\nloads the data file into R (or use the ‚Äúpoint & click method‚Äù); sets string variables to be categorical factor variables.\n\n\nhead(dat)\nlooks at the first 6 rows of the data file\n\n\ntail(dat)\nlooks at the last 6 rows of the data file\n\n\nnrow(dat)\ndisplays the number of rows (each row = an individual)\n\n\nncol(dat)\ndisplays the number of columns (each column = a variable)\n\n\nnames(dat)\ndisplays the names of the object (column names = names of variables)\n\n\ndat$variable\ndisplays the variable from a dataset\n\n\ndat$variable[i]\ndisplays the individual row [i] from the variable\n\n\ndat[i, j]\ndisplays an individual row [i] and column [j] from the dataset",
    "crumbs": [
      "List of R Code",
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Rcode</span>"
    ]
  },
  {
    "objectID": "Rcode.html#r-commands-for-visualizing-data",
    "href": "Rcode.html#r-commands-for-visualizing-data",
    "title": "Appendix A ‚Äî Rcode",
    "section": "A.5 R Commands for Visualizing Data",
    "text": "A.5 R Commands for Visualizing Data\n\n\n\n\n\n\n\nR Command\nWhat it Does\n\n\nsummary(dat)\nReports descriptive statistics for all variables in the dataset.\n\n\nsummary(dat$variable)\nReports descriptive statistics for a categorical variable (frequency / number of individuals in each level) or continuous variable (mean, range, etc.)\n\n\nas.numeric(dat$variable)\nMakes the variable numeric (for continuous graphs)\n\n\nas.factor(dat$variable)\nMakes the variable a categorical factor (for categorical graphs)\n\n\ndat$variable &lt;- as.factor(dat$variable)\nAssigns the as.factor output to the original variable. (In other words, this saves your new categorical factor variable by overwriting the old one.)\n\n\nplot(dat$variable)\nFor categorical variables : draws a barplot. For continuous variables :¬† illustrates values of the variable (y-axis) as a function of their index (x-axis).\n\n\nhist(dat$variable)\nFor continuous variables : draws a histogram.\n\n\npar(mfrow = c(i, j))\nSplits your graphics window into i rows and j columns.",
    "crumbs": [
      "List of R Code",
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Rcode</span>"
    ]
  },
  {
    "objectID": "Rcode.html#r-code-descriptive-statistics",
    "href": "Rcode.html#r-code-descriptive-statistics",
    "title": "Appendix A ‚Äî Rcode",
    "section": "A.6 R Code : Descriptive Statistics",
    "text": "A.6 R Code : Descriptive Statistics\nBelow is a list of code we‚Äôll use to calculate descriptive statistics in R.\n\n\n\n\n\n\n\nR Command\nWhat It Does\n\n\nsummary(dat)\nReports descriptive statistics for all variables in the dataset.\n\n\nsummary(dat$variable)\nReports descriptive statistics for a continuous variable.\nReports frequency for a categorical variable.\n\n\nmean(dat$variable, na.rm = T)\nReports the mean (average) of a variable; you must include the na.rm = T argument if there is missing data (otherwise R will return NA as the result).\n\n\nmedian(dat$variable, na.rm = T)\nReports the median (middle point) of a variable.\n\n\nrange(dat$variable, na.rm = T)\nReports the lower limit and upper limit of the variable.\n\n\nsd(dat$variable, na.rm = T)\nReports the standard deviation of the variable.\n\n\nhist(dat$variable)\nabline(v =mean(dat$variable))\nDraws a line on a plot or histogram at specified values (e.g., this draws a vertical line at the mean of dat$variable. You can replace v with h to draw a horizontal line. We will use abline() later in the semester in a different way.\n\n\npar(mfrow = c(i, j))\nSplits your graphics window into i rows and j columns (replace i and j with numbers)",
    "crumbs": [
      "List of R Code",
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Rcode</span>"
    ]
  },
  {
    "objectID": "chapters/7R_CategoricalIV.html#r2-and-the-linear-model-with-a-categorical-iv",
    "href": "chapters/7R_CategoricalIV.html#r2-and-the-linear-model-with-a-categorical-iv",
    "title": "Linear Model : Categorical IV",
    "section": "\\(R^2\\) and the Linear Model with a Categorical IV",
    "text": "\\(R^2\\) and the Linear Model with a Categorical IV\nThe principle behind \\(R^2\\) for a linear model with a categorical IV is the same - we are looking to see how much less residual error there is when we use the model to make predictions of our DV, compared to when we use the mean when making predictions of our DV.\nYou can try to visualize this decrease in the graphs below.\n\n\nCode\npar(mfrow = c(1,2))\nplot(mba$NPI, pch = 19, ylab = \"Narcissism Score\", xlab = \"Index (Position in Dataset)\",\n     main = \"Residual Errors Using the Mean\")\nabline(h = mean(mba$NPI, na.rm = T), lwd = 5)\nplot(mba$NPI, pch = 19, col = mba$sex, ylab = \"Narcissism Score\", xlab = \"Index (Position in Dataset)\",\n     main = \"Residual Errors Using the Model\")\nabline(h = coef(mod)[1], lwd = 5, col = 'black')\nabline(h = coef(mod)[1] + coef(mod)[2], lwd = 5, col = 'red')\n\n\n\n\n\n\n\n\n\n\nFor the mean (graph on left), the residual errors are the distance between each individual score (dot) and the prediction (black line).\nFor the model (graph on right), the residual errors are the distance between each red dot (males actual narcissism score) and the red line (predicted narcissism for males) AND the distance between each black dot (females actual narcissism score) and the black line (predicted narcissism for females).\n\nI can calculate these residuals, like we did in the last chapter for a linear model with a numeric IV.\n\n\nCode\nresidual &lt;- mba$NPI - mean(mba$NPI, na.rm = T)\ntotal.residual &lt;- sum(residual^2, na.rm = T) # 32.48\ntotal.residual\n\n\n[1] 32.47965\n\n\nCode\nmodel.residual &lt;- sum(mod$residuals^2)\nmodel.residual\n\n\n[1] 30.83179\n\n\nCode\ntotal.residual - model.residual\n\n\n[1] 1.647857\n\n\nIt‚Äôs a pretty small difference in residual errors, and plugging these values into our equation of \\(R^2\\) shows that our model really only reduces residual error by about 5% (compared to the mean).\n\n\nCode\n(total.residual - model.residual)/model.residual\n\n\n[1] 0.0534467\n\n\nCode\nsummary(mod)$r.squared # R^2 the easy way\n\n\n[1] 0.05073507\n\n\nSo yes, male business students say they are more narcissistic than female business students, but differences in sex only explain about 5% of the variation in narcissism. Life is complex. As always.",
    "crumbs": [
      "List of R Code",
      "Predicting People",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Linear Model : Categorical IV</span>"
    ]
  },
  {
    "objectID": "index.html#why-are-we-learning-r-and-who-names-a-program-after-a-letter",
    "href": "index.html#why-are-we-learning-r-and-who-names-a-program-after-a-letter",
    "title": "Why Statistics?",
    "section": "Why are we learning R? And who names a program after a letter?",
    "text": "Why are we learning R? And who names a program after a letter?\n\n\n\n\n\nfun fact : R was named because it was inspired by another program called S, and R comes after S in the English alphabet.\n\n\nThis semester, we will also learn how to use the computer programming language R to work with data, conduct analyses, and make graphs. R can be intimidating to work with at first, and is more confusing than it needs to be sometimes, but I promise you will learn! In fact, that‚Äôs the point of this class.\nIt‚Äôs totally okay (and expected) for you to feel frustrated at times; this is part of the learning process. So please embrace the ‚ÄúI HAVE NO IDEA WHAT I‚ÄôM DOING‚Äù dog meme energy as you embark on your R journey.\n\n\n\n\n\nlook how happy the doggy looks while learning\n\n\n\nInstalling R\nUse this link to Download and Install the Programs R and RStudio Desktop\nNote : You must download both R and RStudio Desktop (these are two separate programs). Make sure to download the most recent version of R and RStudio to avoid issues in the future.\n\nR is the powerful, free, and friendly computer program that we will use to analyze data in this class. This website is not super friendly - choose the operating system you have (Windows, MacOS, or Linux) and then download the ‚Äúlatest release‚Äù on the next page. If you have a chromebook or iPad / tablet, you will need to use posit.cloud.\nRStudio is an Integrated Development Environment (IDE) - basically a ‚Äúhome‚Äù for R to live in, with rooms and this program is not 100% necessary, but makes it a little easier to navigate R. You will need to install R first in order for RStudio to work. Mac Users : make sure you drag the program from the virtual disc into your Applications Folder to fully install the program.\n\nHaving trouble getting these programs to work?\n\nHere‚Äôs one YouTube video someone made to show you how to download and install.\nTry posit.cloud. This is a web-based version of RStudio, and has a free option but limits your hours of work each month. There‚Äôs a paid option for $5/month that you can use if you sign up with your student e-mail address; former students also pointed out that you can always create a new ‚Äúfree‚Äù account if you run over the 15-hour limit.\nAsk for help! The professor, other students, or a tutor / your TA can help get everything working properly.\n\n\n\nNavigating R\nWatch the two videos below for a quick introduction to R - the program we will be using to analyze data.\n\nVIDEO : Navigating R\n\n\nwhat R looks like when you open it\nbasic math in the¬†console\nindexing and output\n\n\n\nVIDEO : Navigating RStudio\n\n\nwhat RStudio/Posit looks like; navigating the program\nbasic math in the console\nthe¬†source file¬†(makes life easier and saves your work!)",
    "crumbs": [
      "List of R Code",
      "Hello (An Introduction)!"
    ]
  },
  {
    "objectID": "chapters/1R_WhyStats.html#part-2-variables-in-r",
    "href": "chapters/1R_WhyStats.html#part-2-variables-in-r",
    "title": "Why Statistics?",
    "section": "Part 2 : Variables in R",
    "text": "Part 2 : Variables in R",
    "crumbs": [
      "List of R Code",
      "Describing People",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Why Statistics?</span>"
    ]
  },
  {
    "objectID": "chapters/1R_WhyStats.html#defining-variables-in-r",
    "href": "chapters/1R_WhyStats.html#defining-variables-in-r",
    "title": "Why Statistics?",
    "section": "Defining Variables in R",
    "text": "Defining Variables in R\nBelow are some videos, and R code, that review how to define a variable in R. Practice creating your own script in RStudio to follow along; there‚Äôs value in typing this out yourself to get that muscle memory in üí™ü§ò. Let us know if you get stuck on the class discord.\n\nVideo : Defining Numeric Variables in R\n\n\nobjects - assign function for numerical data\nc() : combining data together.\nlength() : the number of objects\n\n\n\nVideo : Graphing Numeric Variables in R\n\n\nhist() : a graph\nchanging arguments : xlab, ylab, main\n\n\n\nVideo : Defining Categorical Variables in R\n\n\ncategorical data (‚Äústring‚Äù)\nas.factor() : to convert a string to a categorical variable\nlevels() : to see the levels of your categorical variable.\n\n\n\nVideo : Graphing categorical variables in R\n\n\nplot()\nchanging arguments : col, bor, main; xlab; ylab",
    "crumbs": [
      "List of R Code",
      "Describing People",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Why Statistics?</span>"
    ]
  },
  {
    "objectID": "chapters/1R_WhyStats.html#part-2-defining-variables-in-r",
    "href": "chapters/1R_WhyStats.html#part-2-defining-variables-in-r",
    "title": "Why Statistics?",
    "section": "Part 2 : Defining Variables in R",
    "text": "Part 2 : Defining Variables in R\nBelow are some videos, and R code, that review how to define a variable in R. Practice creating your own script in RStudio to follow along; there‚Äôs value in typing this out yourself to get that muscle memory in üí™ü§ò.\nLet us know if you get stuck on the class discord.\n\nVideo : Defining Numeric Variables in R\n\n\nobjects - assign function for numerical data\nc() : combining data together.\nlength() : the number of objects\n\n\n\nVideo : Graphing Numeric Variables in R\n\n\nhist() : a graph\nchanging arguments : xlab, ylab, main\n\n\n\nVideo : Defining Categorical Variables in R\n\n\ncategorical data (‚Äústring‚Äù)\nas.factor() : to convert a string to a categorical variable\nlevels() : to see the levels of your categorical variable.\n\n\n\nVideo : Graphing categorical variables in R\n\n\nplot()\nchanging arguments : col, bor, main; xlab; ylab",
    "crumbs": [
      "List of R Code",
      "Describing People",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Why Statistics?</span>"
    ]
  },
  {
    "objectID": "chapters/5R_GoodScience.html#hard-data-in-a-soft-science",
    "href": "chapters/5R_GoodScience.html#hard-data-in-a-soft-science",
    "title": "‚ÄúNormal‚Äù Data",
    "section": "Hard Data in a Soft Science",
    "text": "Hard Data in a Soft Science\nOne of the challenges psychologists face in their attempts to be a REAL SCIENCE ‚Ñ¢ is that their data is particularly hard to collect. Unlike physical variables like temperature or mass, psychological variables are often internal to people, and defined by mental states that are difficult to observe. As we saw in lecture, even a ‚Äúsimple‚Äù expressed behavior like an interruption can be very difficult to measure with high degrees of reliability and validity that we would hope. For a more complex variable such as depression, the task might seem impossible. Indeed, a large part of psychological research is engaging in debate and scholarship about how to best operationalize variables of interest (e.g., how should we define or measure depression?)¬†\nA full discussion of the different types of methods psychologists use to collect data is beyond the scope of this author. However, below I‚Äôve tried to outline a few different approaches psychologists take, commenting on their benefits and limitations so you can begin to critically think about whether these methods are, in fact, getting at ‚ÄúTHE TRUTH‚Äù of what people (or non-human animals) are like.\n\nSelf-Reports\nOne of the simplest ways to collect data on an individual is just to ask them what they are like, and have the person report on themselves (a self-report). There are two different approaches to getting self-reports - survey methods and qualitative interviews.\n\nQualitative Interviews\nOne way to get individuals to tell you what they are like is through a structured interview where researchers ask open-ended questions. One such example of this is the McAdams Life Narrative12. In this structured interview, a trained research assistant asks a set of broad questions to participants over the course of 1 to 3 hours. The research assistant is advised to ‚Äúfeel free to skip some of these questions if they seem redundant or irrelevant, and should follow up with other questions as needed ‚Äú but also to ‚Äúnot adopt an advisory or judgmental role, but should instead serve as an empathic and encouraging guide and an affirming sounding board.‚Äù\n12¬†McAdams talks about his work in this popular press interview and writes about it in this scientific journal review article.13¬†Here‚Äôs a link to the full narrative instructions if you want to do the whole thing; it‚Äôs a great way to know someone.Below is an excerpt from the first part of the interview - if you are comfortable, please share your chapters on the Chapter 4 Discord thread!13\n\n\n\nSurvey Methods\nQualitative interviews are not very common in psychological research, because they take a lot of time to conduct, and then more time to convert people‚Äôs open-ended responses into data (a form of behavioral coding, described in more detail below).\nInstead, the majority of self-reports come from surveys. Read about these below.\n\n\n\nDefinition\nA questionnaire where individuals answer specific questions about themselves on a structured rating scale.\n\n\nExample\n‚ÄúOn a scale from 1 (Strongly Disagree) to 5 (Strongly Agree), how satisfied with your life are you right now?‚Äù\n\n\nBenefits\n\nEasy to collect : It only takes a few minutes for people to answer a survey, and the data come in a clean and organized format that requires little effort to analyze. In Part 2 below, you‚Äôll learn how to clean and organize the results of a likert scale.\nSelf-Knowledge Validity : People know things about themselves, often this knowledge is based in ‚Äúreality‚Äù, and sometimes a self-report is the only way to get this knowledge. For example, only you know what was your happiest moment in life, and you probably have an accurate awareness about how anxious you are about the final project in this class.\n\n\n\nLimitations\n\nSelf-Enhancement / Self-Diminishment Bias : People are often motivated to either enhance or diminish their accomplishments when asked. For example, no student has ever come up to me at the end of the semester and told me, ‚ÄúProfessor, just so you know - I cheated on the exam.‚Äù Even though they may know they cheated, they don‚Äôt want to admit that because our society has norms or guidelines about when cheating is appropriate14. Other times, a person may not want to highlight their accomplishments (self-diminishment) because they don‚Äôt want to seem like they are bragging.\nSelf-Insight Bias : Sometimes, people also really don‚Äôt know what they are like. A person may not know, for example, whether they snore when they sleep (a behavior), how anxious they really are in a situation (an affect), or their patterns of unconscious bias (a cognition).\n\n\n\n\n14¬†Indeed, our society decides what forms of cheating are acceptable.Self-reports have a bad reputation in psychology, particularly because of the ability for people to engage in self-enhancement/diminishment or self-insight bias. However, they are a very powerful, and very commonly used method of assessment, even for studies where researchers are able to observe other types of data.¬†\nFor example, despite all the behavioral data that powerful technology companies like Facebook/Instagram/Meta, TikTok, Twitter/X, etc. collect, you‚Äôve probably seen them ask you to answer some survey. They care about you15, and know that asking you questions about yourself is an important way to show that level of care.\n15¬†‚Ä¶and your clicking on advertisements; hey, it‚Äôs hard to distinguish the two really‚Ä¶Below is one example from some survey when I used to be on Facebook. If they changed their graphic design since I was last on, it‚Äôs probably because of some survey feedback they received.\n\n\n\n\n\n\n\n\nObservations\nOften, self-reports are insufficient to capture what a person is like, or researchers are studying individuals who cannot give self-reports, such as infants, people with disabilities, or non-human animals.16\n16¬†If I was a billionaire, I‚Äôd fund a team of psychologists to train monkeys to answer surveys. this is maybe why I am not a billionaire. that and the whole ‚Äúintergenerational wealth‚Äù thing / chosen teaching career / lack of a desire to crush others and extract as much wealth from them‚Ä¶hard to know which factor is at play. Life is complex! Let me know if you are a billionaire and want to fund some other ideas / subscribe to my newsletter.\n\nRead about some common forms of observation methods below.\n\n\n\nDefinition\nObservational methods refer to ways in which another individual generates data on the target person of interest.\n\n\nExamples\nInformant Reports. Informant reports are a special form of surveys, where researchers ask friends, family, or strangers to answer survey questions about another person. This is technically observational data, since the people answering the surveys are basing their judgments on their observations of the individual.\n |\nBehavioral Data. When the variables of interest are physical, then researchers can use measurement tools to directly observe the behavior. For example, researchers wanting to measure stress might measure cortisol by taking samples of saliva from the cheek; researchers wanting to understand the brain look to voxel activation with fMRI, or cortical neuron activation with EEG.\n |\nBehavioral Coding. Sometimes, it‚Äôs easier to have research assistants observe the physical behaviors of interest. For example, y‚Äôall served as behavioral coders when you counted the number of interruptions (a behavior!) Other times, research assistants will observe real-life interactions and observe variables such as time spent talking, distance between participants, or provide ratings of how much emotion or anxiety the person seemed to be expressing (using a rating scale). The ‚Äústrange situation‚Äù task (where a parent leaves the room and researchers observe what a child does) is another example.17\n\n\n\nBenefits\n\n‚ÄúExternal‚Äù. Researchers like observational methods because they, by definition, require some outward expression of the underlying psychological processes. Professor could go on a rant here and won‚Äôt, but psychology has been increasingly fixated on behavior since Watson kicked off the behaviorism movement in the 1920s, and these data are often prioritized, since our field loves predicting people‚Äôs actual behavior (so they can exert power over it).\nLess influenced by self-report biases. Observers are considered to be less biased than individuals, who may be particularly prone to self-enhancement, self-diminishment. And observers may be able to fill in the gaps left behind by self-insight bias. For example, a person‚Äôs close friends may know more about the individual‚Äôs reputation than they themselves do.\nMore reliable (multiple sources). While there is only one self to provide a self-report, there can be multiple observers. Indeed, observational methods almost always leverage this power and require multiple swabs of spit to get a reliable estimate of cortisol, or have multiple research assistants rate the same behavior to check and make sure they are each getting a similar answer.\n\n\n\nLimitations\n\nTime-Intensive. It can be incredibly costly to collect observational data. Yale charges over $600/hour for use of their fMRI machine (Berkeley doesn‚Äôt list their prices), and conducting a study to measure a naturally occurring behavior could take years between designing the study, recruiting and training the research assistants to observe the behavior, taking the time to collect the data, and then doing the behavioral coding necessary to convert the observations into numbers. Whew. Much easier to just give someone a survey and have them complete it.\nThe Hawthorne Effect. The act of observation can often change behavior. Couples may be less likely to fight if they know they are being monitored by a psychologist, and even light changes its behavior depending on the way it is observed. Researchers can take steps to address this change in behavior - and sometimes the power of the phenomenon is so large that it doesn‚Äôt matter if a researcher is there. In couples studies, for example, researchers can get couples to fight by having each person write out a list of things that cause conflict in the relationship, and then having the people trade lists and talk about it. Often however, individuals habituate to the presence of observation - do you think about the fact that your online interactions are being harvested by tech companies every day, or just kinda get used to it?\nNot all aspects of psychology are observable. A person‚Äôs subjective experience matters, and sometimes a self-report is the only way you can get this data. If you look like you are smiling and have the neurotransmitter levels that suggest you are happy, but feel miserable, are you happy?\n\n\n\n\n17¬†You can compare this child‚Äôs reaction to this child‚Äôs reaction. What do you observe? How would you quantify these observations and turn them into cold, hard data?",
    "crumbs": [
      "List of R Code",
      "Describing People",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>\"Normal\" Data</span>"
    ]
  }
]