describe(d$corp.power)[1]
describe(d$corp.power)
describe(d$corp.power)[c(1,2,3,4,6,7)]
describe(d$corp.power)[c(1,2,3,4,8,9)]
hist(d$corp.power, col = "black", bor = "white")
describe(d$corp.power)[c(1,2,3,4,8,9)]
hist(d$corp.power, col = "black", bor = "white",
main = "Perceptions of Corporate Power Over Participants' Data")
hist(d$corp.power, col = "black", bor = "white",
main = "",
xlab = "Perceptions of Corporate Power Over Participants' Data")
describe(d$corp.power)[c(1,2,3,4,8,9)]
describe(d$corp.power)[c(1,2,3,4,8,9)]
c <- read.csv("~/Dropbox/!WHY STATS/Professor Datasets/Conspiracy Beliefs/data.csv", stringsAsFactors = T)
head(c)
cons.df <- c[,1:15]
alpha(cons.df)
c <- read.csv("~/Dropbox/!WHY STATS/Professor Datasets/Personality and Random Numbers/personality_random_number_data.csv", stringsAsFactors = T)
head(c)
cons.df <- c[,22:32]
head(cons.df)
head(cons.df)
alpha(cons.df)
s
cons.df <- c[,22:31] # the extraversion items
head(cons.df)
alpha(cons.df) # some are negatively keyed...
alpha(cons.df, check.keys = T) # some are negatively keyed...
d <- read.csv("~/Dropbox/!WHY STATS/Professor Datasets/Personality and Random Numbers/personality_random_number_data.csv", stringsAsFactors = T)
head(d)
cons.df <- d[,22:31] # the extraversion items
head(cons.df)
alpha(cons.df) # some are negatively keyed...
alpha(cons.df, check.keys = T) # some are negatively keyed...
cons.df
consd.dfPOS <- cons.df[,c(2,4,6,8,10)]
consd.dfPOS
consd.dfPOS <- cons.df[,c(1,3,5,7,9)]
consd.dfNEG <- cons.df[,c(2,4,6,8,10)]
consPOS <- cons.df[,c(1,3,5,7,9)]
consNEG <- cons.df[,c(2,4,6,8,10)]
consCLEAN <- cbind(consPOS, consNEG)
head(consCLEAN)
alpha(consCLEAN)
cons.df <- d[,22:31] # the extraversion items
head(cons.df)
alpha(cons.df) # some are negatively keyed...
alpha(cons.df, check.keys = T) # some are negatively keyed...
range(consNEG)
hist(d$O4)
d[is.na(),]
d[is.na(d),]
d[is.na(d)]
d[!is.na(d),]
d <- read.csv("~/Dropbox/!WHY STATS/Professor Datasets/Personality and Random Numbers/personality_random_number_data.csv",
stringsAsFactors = T, na.strings = "0")
head(d)
d$E1
summary(d)
##
dz <- read.csv("~/Dropbox/!WHY STATS/Professor Datasets/Personality and Random Numbers/personality_random_number_data.csv", stringsAsFactors = T)
dz
summary(dz)
dz[!is.na(dz),]
dz[is.na(dz),]
dz[is.na(dz)]
dz == 0
d == 0 # evaluates whether the entry in the dataframe is zero.
d[d == 0]
d[d == 0, ]
## Loading the Data
d <- read.csv("~/Dropbox/!WHY STATS/Professor Datasets/Personality and Random Numbers/personality_random_number_data.csv", stringsAsFactors = T)
d == 0 # evaluates whether each entry in the dataframe is zero.
d[d == 0, ]
d[d == 0]
d[d == 0] <- NA # replaces with zero
summary(d)
### ANOTHER WAY TO DO THIS IS WHEN LOADING THE DATA, YOU CAN SPECIFY WHAT NA.STRINGS SHOULD BE EQUAL TO.
d <- read.csv("~/Dropbox/!WHY STATS/Professor Datasets/Personality and Random Numbers/personality_random_number_data.csv",
stringsAsFactors = T,
na.strings = "0") # r will look for 0 and replace with NA.
head(d)
summary(d)
cons.df <- d[,22:31] # using the names and the codebook, these are the 10 extraversion items
head(cons.df)
cons.df <- d[,22:31] # using the names and the codebook, these are the 10 extraversion items
head(cons.df)
alpha(cons.df) # r is warning me that some are negatively keyed...
alpha(cons.df, check.keys = T) # seeing that some are negatively keyed...
## good to look at the codebook and confirm that these are negatively keyed (opposite of extraversion.)
consPOS <- cons.df[,c(1,3,5,7,9)] # defining the positively keyed items
consNEG <- cons.df[,c(2,4,6,8,10)] # defining the negatively keyed items
range(consNEG)
range(consNEG, na.rm = T)
consNEG <- 6-consNEG # reverse scoring my negatively keyed items
consCLEAN <- cbind(consPOS, consNEG)
head(consCLEAN)
alpha(consCLEAN)
d <- read.csv("~/Dropbox/!WHY STATS/Professor Datasets/Personality and Random Numbers/personality_random_number_data.csv",
stringsAsFactors = T,
na.strings = "0") # r will look for 0 and replace with NA.
summary(d) # same thing as before.
#### WORKING WITH A LIKERT SCALE : PROFESSOR EXAMPLE
###### STEP 1: ORGANIZING THE ITEMS AND CHECKING THE ALPHA RELIABILITY
extra.df <- d[,22:31] # using the names and the codebook, these are the 10 extraversion items
head(extra.df)
alpha(extra.df) # r is warning me that some are negatively keyed...
alpha(extra.df, check.keys = T) # seeing that some are negatively keyed...
## good to look at the codebook and confirm that these are negatively keyed (opposite of extraversion.)
extraPOS <- extra.df[,c(1,3,5,7,9)] # defining the positively keyed items
extraNEG <- extra.df[,c(2,4,6,8,10)] # defining the negatively keyed items
range(extraNEG, na.rm = T) # a 1-5 scale...good! means I did my data cleaning correctly.
extraNEG <- 6-extraNEG # reverse scoring my negatively keyed items
extraCLEAN <- cbind(extraPOS, extraNEG)
head(extraCLEAN)
alpha(extraCLEAN) # high alpha; hooray.
?rowMeans
d$EXTRA <- rowMeans(extra.df, na.rm = FALSE)
d$EXTRA
d$EXTRA <- rowMeans(extra.df, na.rm = TRUE)
hist(d$EXTRA)
names(d)
RANDO.df <- d[,2:20]
head(RANDO.df)
RANDO.df <- d[,2:21]
head(RANDO.df)
alpha(RANDO.df)
alpha(scale(RANDO.df))
scale(RANDO.df)
alpha(scale(RANDO.df))
d$RANDZ <- rowMeans(sacle(RANDO.df), na.rm = T)
d$RANDZ <- rowMeans(scale(RANDO.df), na.rm = T)
hist(d$RANDZ)
plot(EXTAR ~ RANDZ, data = d)
plot(EXTRA ~ RANDZ, data = d)
abline(lm(EXTRA ~ RANDZ, data = d))
summary(lm(EXTRA ~ RANDZ, data = d))
hist(r$WDuratDays)
hist(r$TotalBDeaths)
hist(r$TotalBDeaths, breaks = 20)
hist(r$TotalBDeaths, breaks = 40)
hist(r$TotalBDeaths, breaks = 60)
hist(r$TotalBDeaths, breaks = 60, xlim = c(0,40000))
hist(r$TotalBDeaths, breaks = 100, xlim = c(0,40000))
hist(r$TotalBDeaths, breaks = 200, xlim = c(0,40000))
hist(r$TotalBDeaths, breaks = 400, xlim = c(0,50000))
nrow(r)
hist(r$WDuratDays)
names(r)
## Q. What percentage of scores are actually below this score in the distribution?
?ecdf()
#| include: false
d <- read.csv("~/Dropbox/!GRADSTATS/Datasets/CLASS DATASETS - 102 - FA25/Onboarding Data/honor_onboard_FA25.csv",
stringsAsFactors = T)
m <- array()
for(i in c(1:1000)){
nd <- d[sample(1:nrow(d), nrow(d), replace = T),] # a new sample
m[i] <- mean(nd$self.skills, na.rm = T)
}
mean(d$self.skills, na.rm = T)
mean(m) # similar!
sum(m > 2.5) # all of them (100% greater than the midpoint of the scale.)
sd(m) # sampling error!
hist(m, xlim = c(1,5)) # our distribution of sampling estimates
abline(v = c(mean(d$self.skills),
mean(d$self.skills) + 1.96 * sd(m),
mean(d$self.skills) - 1.96 * sd(m)),
lwd = c(5,2,2), # two line widths
lty = c(1,2,2)) # two line types
d <- read.csv("~/Dropbox/!GRADSTATS/Datasets/CLASS DATASETS - 102 - FA25/Onboarding Data/honor_onboard_FA25.csv", stringsAsFactors = T)
names(d)
d$can.forloop
summary(d$can.forloop)
m <- array()
for(i in c(1:1000)){
nd <- d[sample(1:nrow(d), nrow(d), replace = T),] # a new sample
m[i] <- mean(nd$self.skills, na.rm = T)
}
mean(d$self.skills, na.rm = T)
mean(m) # similar!
sum(m > 2.5) # all of them (100% greater than the midpoint of the scale.)
sd(m) # sampling error!
hist(m, xlim = c(1,5)) # our distribution of sampling estimates
abline(v = c(mean(d$self.skills),
mean(d$self.skills) + 1.96 * sd(m),
mean(d$self.skills) - 1.96 * sd(m)),
lwd = c(5,2,2), # two line widths
lty = c(1,2,2)) # two line types
plot(d$self.skills,
ylab = "Self-Perception of Skills",
xlab = "Index")
abline(h = mean(d$self.skills, na.rm = T), lwd = 0)
plot(d$self.skills,
ylab = "Self-Perception of Skills",
xlab = "Index")
abline(h = mean(d$self.skills, na.rm = T), lwd = 5)
hist(d$self.skills,
ylab = "Frequency",
xlab = "Self-Perception of Skills")
abline(v = mean(d$self.skills, na.rm = T), lwd = 5)
mod1 <- lm(self.skills ~ class.skills, data = d)
plot(jitter(self.skills) ~ class.skills, data = d, main = "Jittered Data") # jittered
abline(mod1, lwd = 5, col = 'red')
coef(mod1)
plot(jitter(self.skills) ~ class.skills, data = d, main = "Jittered Data") # jittered
abline(mod1, lwd = 5, col = 'red')
plot(jitter(self.skills) ~ class.skills, data = d, main = "Jittered Data") # jittered
abline(mod1, lwd = 5, col = 'red')
names(d)
plot(jitter(self.skills) ~ learn.r, data = d,
ylab = "Self-Perception of Skills",
xlab = "Confidence in Learning R")
abline(lm(self.skills ~ learn.r, data = d), lwd = 0)
plot(jitter(self.skills) ~ learn.r, data = d,
ylab = "Self-Perception of Skills", ylim = c(1,5),
xlab = "Confidence in Learning R", xlim = c(1,5))
plot(jitter(self.skills) ~ learn.r, data = d,
ylab = "Self-Perception of Skills", ylim = c(1,5),
xlab = "Confidence in Learning R", xlim = c(1,5))
abline(lm(self.skills ~ learn.r, data = d), lwd = 0)
plot(jitter(self.skills) ~ learn.r, data = d,
ylab = "Self-Perception of Skills", ylim = c(1,5),
xlab = "Confidence in Learning R", xlim = c(1,5))
abline(lm(self.skills ~ learn.r, data = d), lwd = 0)
plot(jitter(self.skills) ~ learn.r, data = d,
ylab = "Self-Perception of Skills", ylim = c(1,5),
xlab = "Confidence in Learning R", xlim = c(1,5))
abline(lm(self.skills ~ learn.r, data = d), lwd = 5)
mod1 <- lm(self.skills ~ learn.r, data = d)
plot(jitter(self.skills) ~ learn.r, data = d, main = "Jittered Data") # jittered
abline(mod1, lwd = 5, col = 'red')
coef(mod1)
# intercept = 2.65 = the predicted value of Y when ALL X values are ZERO.
# slope = .15 = relationship between learn.r and our DV (self.skills)
### as learn.r increase by ONE, then self.skills will increase by .38
### these units are in the original unit of measurement (1-5 likert scale.)
plot(jitter(self.skills) ~ learn.r, # dv is jittered
data = d,
main = "Jittered Data",
xlim = c(1,5), ylim = c(1,5))
plot(jitter(self.skills) ~ learn.r, # dv is jittered
data = d,
main = "Jittered Data",
xlim = c(1,5), ylim = c(1,5))
abline(mod1, lwd = 5, col = 'red')
coef(mod1)
1.46 + .46
plot(jitter(self.skills) ~ learn.r, # dv is jittered
data = d,
main = "Jittered Data",
xlim = c(1,5), ylim = c(1,5))
abline(mod1, lwd = 5, col = 'red')
3 - 1.92
mod1$residuals # R does the residual calculation for us. what will happen if we add this up?
sum(mod1$residuals) # they add to....
SSE <- sum(mod1$residuals^2) # so I square them
SSE # the total squared error when I use my model to make predictions.
## Visualizing Our Errors. (distance between actual scores and the line).
par(mfrow = c(1,2))
plot(d$self.skills,
ylab = "Self-Perception of Skills",
xlab = "Index", main = "Mean as Model \n(SST = Total Sum of Squared Errors)")
abline(h = mean(d$self.skills, na.rm = T), lwd = 5)
plot(jitter(self.skills) ~ learn.r, data = d, main = "Linear Model \n(SSE = Sum of Squared Errors When Model Making Predictions)",
xlim = c(1,5)) # jittered
abline(mod1, lwd = 5, col = 'red')
SST <- sum((d$self.skills - mean(d$self.skills))^2) # defining the total error
SST - SSE # a difference in errors when using the mean vs. our model
(SST - SSE)/SST # the relative difference in errors = R^2 (R-squared.)
summary(mod1)$r.squared # R does this for us. But good to do "by hand" to understand.
1.46 + .46 * 2
#| include: false
d <- read.csv("~/Dropbox/!GRADSTATS/Datasets/CLASS DATASETS - 102 - FA25/Onboarding Data/honor_onboard_FA25.csv",
stringsAsFactors = T)
plot(d$self.skills,
ylab = "Self-Perception of Skills",
xlab = "Index")
abline(h = mean(d$self.skills, na.rm = T), lwd = 0)
plot(d$self.skills,
ylab = "Self-Perception of Skills",
xlab = "Index")
abline(h = mean(d$self.skills, na.rm = T), lwd = 5)
hist(d$self.skills,
ylab = "Frequency",
xlab = "Self-Perception of Skills")
abline(v = mean(d$self.skills, na.rm = T), lwd = 5)
## quantifying errors (residuals)
residuals <- d$self.skills - mean(d$self.skills, na.rm = T)
SST <- sum(residuals^2)
SST
SST/length(residuals) # average of squared residuals (variance)
sqrt(SST/length(residuals)) # average of residuals, unsquared (standard deviation)
sd(d$self.skills) # slightly higher
sqrt(SST/(length(residuals)-1)) # the 'real' equation; n-1 to inflate our estimate / adjust for small samples.
m <- array()
for(i in c(1:1000)){
nd <- d[sample(1:nrow(d), nrow(d), replace = T),] # a new sample
m[i] <- mean(nd$self.skills, na.rm = T)
}
mean(d$self.skills, na.rm = T)
mean(m) # similar!
sum(m > 2.5) # all of them (100% greater than the midpoint of the scale.)
sd(m) # sampling error!
hist(m, xlim = c(1,5)) # our distribution of sampling estimates
abline(v = c(mean(d$self.skills),
mean(d$self.skills) + 1.96 * sd(m),
mean(d$self.skills) - 1.96 * sd(m)),
lwd = c(5,2,2), # two line widths
lty = c(1,2,2)) # two line types
lm(self.skills ~ 1, data = d) # predicting self.skills from a constant (1), using the datset = d
mod0 <- lm(self.skills ~ 1, data = d) # saving this as a model object
coef(mod0) # looking at the coefficients
mod0$residuals # finding the residuals
plot(jitter(self.skills) ~ learn.r, data = d,
ylab = "Self-Perception of Skills", ylim = c(1,5),
xlab = "Confidence in Learning R", xlim = c(1,5))
abline(lm(self.skills ~ learn.r, data = d), lwd = 0)
plot(jitter(self.skills) ~ learn.r, data = d,
ylab = "Self-Perception of Skills", ylim = c(1,5),
xlab = "Confidence in Learning R", xlim = c(1,5))
abline(lm(self.skills ~ learn.r, data = d), lwd = 5)
mod1 <- lm(self.skills ~ learn.r, data = d)
plot(jitter(self.skills) ~ learn.r, # dv is jittered
data = d,
main = "Jittered Data",
xlim = c(1,5), ylim = c(1,5))
abline(mod1, lwd = 5, col = 'red')
coef(mod1)
# intercept = 1.46 = the predicted value of Y when ALL X values are ZERO.
# slope = .46 = relationship between learn.r and our DV (self.skills)
### as learn.r increase by ONE, then self.skills will increase by .46
### these units are in the original unit of measurement (1-5 likert scale.)
mod1$residuals # R does the residual calculation for us. what will happen if we add this up?
sum(mod1$residuals) # they add to....
SSE <- sum(mod1$residuals^2) # so I square them
SSE # the total squared error when I use my model to make predictions.
## Visualizing Our Errors. (distance between actual scores and the line).
par(mfrow = c(1,2))
plot(d$self.skills,
ylab = "Self-Perception of Skills",
xlab = "Index", main = "Mean as Model \n(SST = Total Sum of Squared Errors)")
abline(h = mean(d$self.skills, na.rm = T), lwd = 5)
plot(jitter(self.skills) ~ learn.r, data = d, main = "Linear Model \n(SSE = Sum of Squared Errors When Model Making Predictions)",
xlim = c(1,5)) # jittered
abline(mod1, lwd = 5, col = 'red')
SST <- sum((d$self.skills - mean(d$self.skills))^2) # defining the total error
SST - SSE # a difference in errors when using the mean vs. our model
(SST - SSE)/SST # the relative difference in errors = R^2 (R-squared.)
summary(mod1)$r.squared # R does this for us. But good to do "by hand" to understand.
names(d) # what other (numeric, for now) variable might predict self.skills?
#| include: false
d <- read.csv("~/Dropbox/!GRADSTATS/Datasets/CLASS DATASETS - 102 - FA25/Onboarding Data/honor_onboard_FA25.csv",
stringsAsFactors = T)
plot(d$self.skills,
ylab = "Self-Perception of Skills",
xlab = "Index")
abline(h = mean(d$self.skills, na.rm = T), lwd = 0)
plot(d$self.skills,
ylab = "Self-Perception of Skills",
xlab = "Index")
abline(h = mean(d$self.skills, na.rm = T), lwd = 5)
hist(d$self.skills,
ylab = "Frequency",
xlab = "Self-Perception of Skills")
abline(v = mean(d$self.skills, na.rm = T), lwd = 5)
## quantifying errors (residuals)
residuals <- d$self.skills - mean(d$self.skills, na.rm = T)
SST <- sum(residuals^2)
SST
SST/length(residuals) # average of squared residuals (variance)
sqrt(SST/length(residuals)) # average of residuals, unsquared (standard deviation)
sd(d$self.skills) # slightly higher
sqrt(SST/(length(residuals)-1)) # the 'real' equation; n-1 to inflate our estimate / adjust for small samples.
m <- array()
for(i in c(1:1000)){
nd <- d[sample(1:nrow(d), nrow(d), replace = T),] # a new sample
m[i] <- mean(nd$self.skills, na.rm = T)
}
mean(d$self.skills, na.rm = T)
mean(m) # similar!
sum(m > 2.5) # all of them (100% greater than the midpoint of the scale.)
sd(m) # sampling error!
hist(m, xlim = c(1,5)) # our distribution of sampling estimates
abline(v = c(mean(d$self.skills),
mean(d$self.skills) + 1.96 * sd(m),
mean(d$self.skills) - 1.96 * sd(m)),
lwd = c(5,2,2), # two line widths
lty = c(1,2,2)) # two line types
lm(self.skills ~ 1, data = d) # predicting self.skills from a constant (1), using the datset = d
mod0 <- lm(self.skills ~ 1, data = d) # saving this as a model object
coef(mod0) # looking at the coefficients
mod0$residuals # finding the residuals
plot(jitter(self.skills) ~ learn.r, data = d,
ylab = "Self-Perception of Skills", ylim = c(1,5),
xlab = "Confidence in Learning R", xlim = c(1,5))
abline(lm(self.skills ~ learn.r, data = d), lwd = 0)
plot(jitter(self.skills) ~ learn.r, data = d,
ylab = "Self-Perception of Skills", ylim = c(1,5),
xlab = "Confidence in Learning R", xlim = c(1,5))
abline(lm(self.skills ~ learn.r, data = d), lwd = 5)
mod1 <- lm(self.skills ~ learn.r, data = d)
plot(jitter(self.skills) ~ learn.r, # dv is jittered
data = d,
main = "Jittered Data",
xlim = c(1,5), ylim = c(1,5))
abline(mod1, lwd = 5, col = 'red')
coef(mod1)
# intercept = 1.46 = the predicted value of Y when ALL X values are ZERO.
# slope = .46 = relationship between learn.r and our DV (self.skills)
### as learn.r increase by ONE, then self.skills will increase by .46
### these units are in the original unit of measurement (1-5 likert scale.)
mod1$residuals # R does the residual calculation for us. what will happen if we add this up?
sum(mod1$residuals) # they add to....
SSE <- sum(mod1$residuals^2) # so I square them
SSE # the total squared error when I use my model to make predictions.
## Visualizing Our Errors. (distance between actual scores and the line).
par(mfrow = c(1,2))
plot(d$self.skills,
ylab = "Self-Perception of Skills",
xlab = "Index", main = "Mean as Model \n(SST = Total Sum of Squared Errors)")
abline(h = mean(d$self.skills, na.rm = T), lwd = 5)
plot(jitter(self.skills) ~ learn.r, data = d, main = "Linear Model \n(SSE = Sum of Squared Errors When Model Making Predictions)",
xlim = c(1,5)) # jittered
abline(mod1, lwd = 5, col = 'red')
SST <- sum((d$self.skills - mean(d$self.skills))^2) # defining the total error
SST - SSE # a difference in errors when using the mean vs. our model
(SST - SSE)/SST # the relative difference in errors = R^2 (R-squared.)
summary(mod1)$r.squared # R does this for us. But good to do "by hand" to understand.
names(d) # what other (numeric, for now) variable might predict self.skills?
fakey <- rnorm(10000000, mean = 100, sd = 30)
truthbucket <- array()
for(i in c(1:1000)){
lilfakey <- fakey[sample(1:length(fakey), 10)] # ten random individuals from fakey.
truthbucket[i] <- mean(lilfakey)
}
length(truthbucket)
hist(truthbucket)
mean(truthbucket)
fakey <- rnorm(10000000, mean = 100, sd = 30)
hist(fakey)
abline(v = mean(fakey), lwd = 5)
mean(fakey)
truthbucket <- array()
for(i in c(1:1000)){
lilfakey <- fakey[sample(1:length(fakey), 10)] # ten random individuals from fakey.
truthbucket[i] <- mean(lilfakey)
}
length(truthbucket)
hist(truthbucket)
abline(v = mean(truthbucket), lwd = 5, col = 'red')
mean(truthbucket)
sd(truthbucket)
d <- read.csv("~/Dropbox/!WHY STATS/Chapter Datasets/Personality and Random Numbers/personality_random_number_data.csv", stringsAsFactors = T)
names(d)
head(d)
## R1	What is a number between 1 and 100?
d$R1
hist(d$R1)
## Look at the variable E1
hist(d$E1)
d$E1
mean(d$E1)
sd(d$E1)
summary(d$E1)
abline(v = mean(d$E1, lwd = 5))
abline(v = mean(d$E1), lwd = 5)
abline(v = mean(d$E1) + sd(d$E1), lwd = 5, lty = "dashed")
abline(v = mean(d$E1) - sd(d$E1), lwd = 5, lty = "dashed")
d == 0
d[d == 0]
d[d == 0] <- NA # replaces with zero
summary(d)
d[d == 0] # finds these zeros in the dataset.
hist(d$E1)
mean(d$E1)
mean(d$E1, na.rm = T)
d <- read.csv("~/Dropbox/!WHY STATS/Chapter Datasets/Personality and Random Numbers/personality_random_number_data.csv", na.strings="0", stringsAsFactors=TRUE)
View(d)
hist(d$E2)
hist(d$E2, main = "I don't talk a lot") #
hist(6-d$E2, main = "I [don't] don't talk a lot") # now extraversion
hist(d$E3)
hist(d$E4)
hist(d$E4)
hist(d$E5)
# STEP 1: ORGANIZING THE ITEMS INTO A DATA.FRAME ...
names(d)
d[,22:31]
names(d)[22:31]
extra.df <- d[,22:31] # using the codebook to index the 10 extraversion items
head(extra.df) # checking to make sure I did this correctly
## ...AND CHECKING THE ALPHA RELIABILITY
alpha(extra.df) # r is warning me that some are negatively keyed...
library(psych)
library(psych) # DLC...extra tools to install and load from a library
alpha(extra.df) # r is warning me that some are negatively keyed...
alpha(extra.df, check.keys = T) # seeing that some are negatively keyed...
## look at the codebook and confirm these are negatively keyed
extraPOS <- extra.df[,c(1,3,5,7,9)] # defining the positively keyed items
extraNEG <- extra.df[,c(2,4,6,8,10)] # defining the negatively keyed items
extraNEG <- 6-extraNEG # reverse scoring my negatively keyed items
extraCLEAN <- cbind(extraPOS, extraNEG)
head(extraCLEAN)
alpha(extraCLEAN) # high alpha; hooray.
head(extraCLEAN)
d$EXTRA <- rowMeans(extra.df,     # 10 items into one variable.
na.rm = TRUE) # still calculate if there’s missing data
d$EXTRA
d$EXTRA <- rowMeans(extraCLEAN,     # 10 items into one variable.
na.rm = TRUE) # still calculate if there’s missing data
d$EXTRA
hist(d$EXTRA)                     # What do you learn / observe?
